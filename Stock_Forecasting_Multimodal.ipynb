{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soumabha4444/MLBA_Project_57C/blob/main/Stock_Forecasting_Multimodal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spdTxIH1TPbt",
        "outputId": "6a623ed7-dc1c-4770-bab2-6823e0d5553a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mlba-stock-forecasting\n"
          ]
        }
      ],
      "source": [
        "# create folders exactly as in the repo\n",
        "!mkdir -p mlba-stock-forecasting/{src,data/raw,data/processed,outputs,results}\n",
        "%cd mlba-stock-forecasting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr3BtVhxWZEr",
        "outputId": "d1914732-efdc-44e3-fd4b-318f8da213a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.12/dist-packages (0.2.66)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.32.4)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.0.12)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.5.0)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.12/dist-packages (from yfinance) (3.18.3)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.13.5)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.13.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (5.29.5)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.8)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (2025.10.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (2.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.23)\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance numpy pandas scikit-learn torch tqdm scipy matplotlib seaborn joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQW7r44X01t1",
        "outputId": "0d3ae35d-ed4a-4977-e63e-8ac48f26b321"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uYAuTEWWggC",
        "outputId": "82263ab3-e6fd-4576-99f7-adf20f4ebbf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/data.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/data.py\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import os\n",
        "\n",
        "def download_price(ticker, start, end, out_csv):\n",
        "    df = yf.download(ticker, start=start, end=end, progress=False)\n",
        "    df = df.reset_index().rename(columns={'Date':'date'})\n",
        "    df['ticker'] = ticker\n",
        "    os.makedirs(os.path.dirname(out_csv), exist_ok=True)\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(f\"Saved prices to {out_csv}\")\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    download_price(\"AAPL\", \"2008-01-01\", \"2016-10-31\", \"data/raw/aapl.csv\")\n",
        "\n",
        "# This downloads real AAPL stock data and saves it as data/raw/aapl.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYTYBTecXiB3",
        "outputId": "e4c3d8cf-9325-4594-d4de-c909212f8e7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/features.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/features.py\n",
        "import argparse\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def create_features(df, lags=(1,2,3,5,10,20), roll_mean_win=5, roll_std_win=20):\n",
        "    # normalize date column\n",
        "    if 'Date' in df.columns and 'date' not in df.columns:\n",
        "        df = df.rename(columns={'Date':'date'})\n",
        "    if 'date' not in df.columns:\n",
        "        raise ValueError(\"Input dataframe must contain a 'date' or 'Date' column.\")\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "\n",
        "    # standardize some common column names (map spaces -> underscores)\n",
        "    to_check = ['Open','High','Low','Close','Adj Close','Adj_Close','Volume']\n",
        "    for col in to_check:\n",
        "        if col in df.columns:\n",
        "            std_name = col.replace(' ', '_')\n",
        "            df[std_name] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # prefer adjusted close if Close missing\n",
        "    if 'Close' not in df.columns and 'Adj_Close' in df.columns:\n",
        "        df['Close'] = df['Adj_Close']\n",
        "\n",
        "    # accept lowercase variant\n",
        "    if 'close' in df.columns and 'Close' not in df.columns:\n",
        "        df['Close'] = pd.to_numeric(df['close'], errors='coerce')\n",
        "\n",
        "    # sort and drop rows missing Close\n",
        "    df = df.sort_values('date').copy().reset_index(drop=True)\n",
        "    df = df.dropna(subset=['Close']).reset_index(drop=True)\n",
        "\n",
        "    # create price-based target: next-day close (kept for reference)\n",
        "    df['close_next'] = df['Close'].shift(-1)\n",
        "\n",
        "    # next-day log return target (preferred for modeling)\n",
        "    # ret_next = log(close_next / Close)\n",
        "    df['ret_next'] = np.log(df['close_next'] / df['Close'])\n",
        "\n",
        "    # log return (today)\n",
        "    df['ret'] = np.log(df['Close']).diff()\n",
        "\n",
        "    # lag features\n",
        "    for l in lags:\n",
        "        df[f'ret_lag_{l}'] = df['ret'].shift(l)\n",
        "\n",
        "    # rolling stats\n",
        "    df[f'ret_roll_{roll_mean_win}_mean'] = df['ret'].rolling(roll_mean_win).mean()\n",
        "    df[f'ret_roll_{roll_std_win}_std'] = df['ret'].rolling(roll_std_win).std()\n",
        "\n",
        "    # list of columns required for modelling\n",
        "    cols_needed = ['ret_next'] + [f'ret_lag_{l}' for l in lags] + [f'ret_roll_{roll_mean_win}_mean', f'ret_roll_{roll_std_win}_std']\n",
        "\n",
        "    df = df.dropna(subset=cols_needed).reset_index(drop=True)\n",
        "\n",
        "    if df.empty:\n",
        "        raise ValueError(\"No rows remain after feature creation. Check input data length and column names.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def save_sample(df, out_csv, n=200):\n",
        "    os.makedirs(os.path.dirname(out_csv), exist_ok=True)\n",
        "    df.head(n).to_csv(out_csv, index=False)\n",
        "    print(\"Saved sample slice to\", out_csv)\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--input\", dest=\"infile\", required=False, default=\"data/raw/aapl.csv\",\n",
        "                        help=\"Input raw CSV file (has Date/Close/Adj Close etc.)\")\n",
        "    parser.add_argument(\"--out\", dest=\"outfile\", required=False, default=\"data/processed/aapl_features.parquet\")\n",
        "    parser.add_argument(\"--sample_out\", dest=\"sample_out\", default=\"data/sample_slice.csv\")\n",
        "    parser.add_argument(\"--sample_n\", type=int, default=200)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if not os.path.exists(args.infile):\n",
        "        raise SystemExit(f\"Input file not found: {args.infile}\")\n",
        "    df = pd.read_csv(args.infile)\n",
        "    df_feats = create_features(df)\n",
        "    os.makedirs(os.path.dirname(args.outfile), exist_ok=True)\n",
        "    df_feats.to_parquet(args.outfile, index=False)\n",
        "    save_sample(df_feats, args.sample_out, n=args.sample_n)\n",
        "    print(f\"Saved features to {args.outfile} with {len(df_feats)} rows\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yko0UqXNXzUZ",
        "outputId": "0e513aac-ad91-4016-ee8b-a841588e123d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/train_baseline.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/train_baseline.py\n",
        "import argparse, os, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import math\n",
        "from scipy import stats\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    y_true = np.array(y_true); y_pred = np.array(y_pred)\n",
        "    return float(math.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
        "\n",
        "def directional_accuracy_returns(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Directional accuracy for returns: sign(pred) == sign(true)\n",
        "    Returns percentage in [0,100].\n",
        "    \"\"\"\n",
        "    y_true = np.array(y_true, dtype=float)\n",
        "    y_pred = np.array(y_pred, dtype=float)\n",
        "    n = min(len(y_true), len(y_pred))\n",
        "    if n == 0:\n",
        "        return 0.0\n",
        "    signs_pred = np.sign(y_pred[:n])\n",
        "    signs_true = np.sign(y_true[:n])\n",
        "    acc = np.mean(signs_pred == signs_true)\n",
        "    return float(100.0 * acc)\n",
        "\n",
        "def rolling_origin(df, initial_train_days=800, test_days=60, step_days=60):\n",
        "    dates = sorted(df['date'].unique())\n",
        "    folds=[]\n",
        "    start_idx=0\n",
        "    while True:\n",
        "        train_end = start_idx + initial_train_days - 1\n",
        "        test_start = train_end + 1\n",
        "        test_end = test_start + test_days - 1\n",
        "        if test_end >= len(dates): break\n",
        "        folds.append((dates[0], dates[train_end], dates[test_start], dates[test_end]))\n",
        "        start_idx += step_days\n",
        "    return folds\n",
        "\n",
        "def run_fold(df, train_end_date, test_start, test_end, features):\n",
        "    train_df = df[df['date']<=train_end_date]\n",
        "    test_df = df[(df['date']>=test_start) & (df['date']<=test_end)].copy().reset_index(drop=True)\n",
        "\n",
        "    X_train = train_df[features].values\n",
        "    y_train = train_df['ret_next'].values   # now predicting next-day log return\n",
        "    X_test = test_df[features].values\n",
        "    y_test = test_df['ret_next'].values\n",
        "\n",
        "    # Keep current_close for reference (not used for DA on returns)\n",
        "    current_close = test_df['Close'].values if 'Close' in test_df.columns else np.full_like(y_test, np.nan, dtype=float)\n",
        "\n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "    X_train = scaler.transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    model = Ridge(alpha=1.0)\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "\n",
        "    # compute metrics for this fold (on returns)\n",
        "    mae_val = float(mean_absolute_error(y_test, preds))\n",
        "    rmse_val = float(rmse(y_test, preds))\n",
        "    da_val = directional_accuracy_returns(y_test, preds)\n",
        "\n",
        "    # convert arrays to plain python lists (floats / None) for JSON\n",
        "    def as_pylist(arr, cast=float):\n",
        "        out = []\n",
        "        for x in list(arr):\n",
        "            if pd.isna(x) or (isinstance(x, float) and (np.isinf(x) or np.isnan(x))):\n",
        "                out.append(None)\n",
        "            else:\n",
        "                out.append(cast(x))\n",
        "        return out\n",
        "\n",
        "    return {\n",
        "        \"y_true\": as_pylist(y_test, float),\n",
        "        \"y_pred\": as_pylist(preds, float),\n",
        "        \"current_close\": as_pylist(current_close, float),\n",
        "        \"metrics\": {\n",
        "            \"MAE\": mae_val,\n",
        "            \"RMSE\": rmse_val,\n",
        "            \"DA_percent\": da_val\n",
        "        }\n",
        "    }\n",
        "\n",
        "def t_ci(x, alpha=0.95):\n",
        "    x = np.array(x, dtype=float); n=len(x)\n",
        "    if n == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    mean = float(np.mean(x))\n",
        "    if n == 1:\n",
        "        return mean, mean, mean\n",
        "    se = float(np.std(x, ddof=1)/math.sqrt(n))\n",
        "    t = stats.t.ppf((1+alpha)/2., n-1)\n",
        "    return mean, mean - t*se, mean + t*se\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--features\", required=False, default=\"data/processed/aapl_features.parquet\")\n",
        "    parser.add_argument(\"--out\", required=False, default=\"outputs/baseline_preds.json\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    df = pd.read_parquet(args.features)\n",
        "    df = df.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "    # pick feature columns (lags & rolling stats)\n",
        "    features = [c for c in df.columns if c.startswith('ret_lag_') or c.startswith('ret_roll_')]\n",
        "\n",
        "    folds = rolling_origin(df)\n",
        "    os.makedirs(os.path.dirname(args.out), exist_ok=True)\n",
        "    all_preds=[]\n",
        "\n",
        "    for idx,(train_start, train_end, test_start, test_end) in enumerate(folds):\n",
        "        print(f\"Fold {idx}: train_end={train_end}, test_start={test_start}, test_end={test_end}\")\n",
        "        rec = run_fold(df, train_end, test_start, test_end, features)\n",
        "        # stringify timestamps\n",
        "        def to_str(d):\n",
        "            try:\n",
        "                return d.isoformat()\n",
        "            except Exception:\n",
        "                return str(d)\n",
        "        out_rec = {\n",
        "            \"fold\": int(idx),\n",
        "            \"model\": \"ridge_baseline\",\n",
        "            \"train_start\": to_str(train_start),\n",
        "            \"train_end\": to_str(train_end),\n",
        "            \"test_start\": to_str(test_start),\n",
        "            \"test_end\": to_str(test_end),\n",
        "            \"y_true\": rec[\"y_true\"],\n",
        "            \"y_pred\": rec[\"y_pred\"],\n",
        "            \"current_close\": rec[\"current_close\"],\n",
        "            \"metrics\": rec[\"metrics\"]\n",
        "        }\n",
        "        all_preds.append(out_rec)\n",
        "\n",
        "    # compute summary across folds\n",
        "    maes = [r[\"metrics\"][\"MAE\"] for r in all_preds]\n",
        "    rmses = [r[\"metrics\"][\"RMSE\"] for r in all_preds]\n",
        "    das = [r[\"metrics\"][\"DA_percent\"] for r in all_preds]\n",
        "\n",
        "    summary = {\n",
        "        \"MAE\": { \"mean\": t_ci(maes)[0], \"lo\": t_ci(maes)[1], \"hi\": t_ci(maes)[2] },\n",
        "        \"RMSE\": { \"mean\": t_ci(rmses)[0], \"lo\": t_ci(rmses)[1], \"hi\": t_ci(rmses)[2] },\n",
        "        \"DA_percent\": { \"mean\": t_ci(das)[0], \"lo\": t_ci(das)[1], \"hi\": t_ci(das)[2] }\n",
        "    }\n",
        "\n",
        "    out_blob = {\n",
        "        \"per_fold\": all_preds,\n",
        "        \"summary\": summary\n",
        "    }\n",
        "\n",
        "    with open(args.out,\"w\") as f:\n",
        "        json.dump(out_blob,f, indent=2)\n",
        "    print(\"Saved preds and metrics to\", args.out)\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAfz_SWFX45p",
        "outputId": "f0ecf291-e9a2-44f9-89c6-407a6124a7ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/eval.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/eval.py\n",
        "import json, argparse, os, math, sys\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from scipy import stats\n",
        "\n",
        "def t_confidence_interval(x, confidence=0.95):\n",
        "    x = np.array(x, dtype=float)\n",
        "    n = len(x)\n",
        "    if n == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    mean = float(np.mean(x))\n",
        "    if n == 1:\n",
        "        return mean, mean, mean\n",
        "    se = float(np.std(x, ddof=1) / math.sqrt(n))\n",
        "    t = stats.t.ppf((1 + confidence) / 2.0, n - 1)\n",
        "    return mean, mean - t * se, mean + t * se\n",
        "\n",
        "def directional_accuracy_filtered(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute directional accuracy (%) for returns.\n",
        "    Filters out non-finite entries and aligns arrays.\n",
        "    \"\"\"\n",
        "    # convert to numpy arrays and attempt to coerce to float\n",
        "    y_t = np.array([_safe_float(x) for x in y_true], dtype=float)\n",
        "    y_p = np.array([_safe_float(x) for x in y_pred], dtype=float)\n",
        "\n",
        "    # mask finite entries in both\n",
        "    mask = np.isfinite(y_t) & np.isfinite(y_p)\n",
        "    if mask.sum() == 0:\n",
        "        return 0.0\n",
        "    y_t = y_t[mask]\n",
        "    y_p = y_p[mask]\n",
        "    signs_true = np.sign(y_t)\n",
        "    signs_pred = np.sign(y_p)\n",
        "    acc = np.mean(signs_true == signs_pred)\n",
        "    return float(100.0 * acc)\n",
        "\n",
        "def _safe_float(x):\n",
        "    try:\n",
        "        v = float(x)\n",
        "        if math.isfinite(v):\n",
        "            return v\n",
        "    except Exception:\n",
        "        pass\n",
        "    return float('nan')\n",
        "\n",
        "def load_preds(path):\n",
        "    blob = json.load(open(path))\n",
        "    if isinstance(blob, dict) and \"per_fold\" in blob:\n",
        "        records = blob[\"per_fold\"]\n",
        "    elif isinstance(blob, list):\n",
        "        records = blob\n",
        "    else:\n",
        "        raise ValueError(\"Unrecognized preds_json structure. Expect list or dict with 'per_fold'.\")\n",
        "    return records\n",
        "\n",
        "def t_confidence_interval_list(x):\n",
        "    return t_confidence_interval(x)\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--preds_json\", required=True)\n",
        "    parser.add_argument(\"--out\", required=True)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    records = load_preds(args.preds_json)\n",
        "\n",
        "    maes, rmses, das = [], [], []\n",
        "    for rec in records:\n",
        "        y_true = rec.get(\"y_true\", [])\n",
        "        y_pred = rec.get(\"y_pred\", [])\n",
        "        # compute per-fold metrics, making sure to coerce/filter invalid entries\n",
        "        # MAE/RMSE: only use aligned finite pairs\n",
        "        y_t = np.array([_safe_float(x) for x in y_true], dtype=float)\n",
        "        y_p = np.array([_safe_float(x) for x in y_pred], dtype=float)\n",
        "        mask = np.isfinite(y_t) & np.isfinite(y_p)\n",
        "        if mask.sum() == 0:\n",
        "            continue\n",
        "        y_t_f = y_t[mask]\n",
        "        y_p_f = y_p[mask]\n",
        "        maes.append(float(mean_absolute_error(y_t_f, y_p_f)))\n",
        "        rmses.append(float(math.sqrt(mean_squared_error(y_t_f, y_p_f))))\n",
        "        das.append(directional_accuracy_filtered(y_t_f, y_p_f))\n",
        "\n",
        "    mae_mean, mae_lo, mae_hi = t_confidence_interval_list(maes)\n",
        "    rmse_mean, rmse_lo, rmse_hi = t_confidence_interval_list(rmses)\n",
        "    da_mean, da_lo, da_hi = t_confidence_interval_list(das)\n",
        "\n",
        "    out = {\n",
        "        \"MAE_mean\": mae_mean, \"MAE_lo\": mae_lo, \"MAE_hi\": mae_hi,\n",
        "        \"RMSE_mean\": rmse_mean, \"RMSE_lo\": rmse_lo, \"RMSE_hi\": rmse_hi,\n",
        "        \"DA_mean\": da_mean, \"DA_lo\": da_lo, \"DA_hi\": da_hi\n",
        "    }\n",
        "\n",
        "    os.makedirs(os.path.dirname(args.out), exist_ok=True)\n",
        "    with open(args.out, \"w\") as f:\n",
        "        json.dump(out, f, indent=2)\n",
        "    print(\"Saved metrics to\", args.out)\n",
        "    print(json.dumps(out, indent=2))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "Ge-e0YHOYANH",
        "outputId": "48e48ac7-d885-48aa-f2da-8108e80e760b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mlba-stock-forecasting/src/data.py:8: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=start, end=end, progress=False)\n",
            "Saved prices to data/raw/aapl.csv\n",
            "COLUMNS: ['date', 'Close', 'High', 'Low', 'Open', 'Volume', 'ticker']\n",
            "\n",
            "FIRST 6 ROWS:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         date              Close                High                 Low  \\\n",
              "0         NaN               AAPL                AAPL                AAPL   \n",
              "1  2008-01-02  5.849119186401367  6.0118287377815385   5.780373950029967   \n",
              "2  2008-01-03   5.85181999206543   5.925669548226654   5.784575011135818   \n",
              "3  2008-01-04  5.405121803283691   5.793882389595054   5.370298873762459   \n",
              "4  2008-01-07  5.332773208618164    5.51169317331155  5.1103238843778405   \n",
              "5  2008-01-08  5.140944004058838   5.477469947151359   5.127435045283893   \n",
              "\n",
              "                 Open      Volume ticker  \n",
              "0                AAPL        AAPL    NaN  \n",
              "1     5.9821089434096  1079178800   AAPL  \n",
              "2   5.866229974388948   842066400   AAPL  \n",
              "3   5.747351346743581  1455832000   AAPL  \n",
              "4   5.441145493849881  2072193200   AAPL  \n",
              "5  5.4078224949057185  1523816000   AAPL  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c9122d94-3c6a-47aa-ab86-b0da7e4d434c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Open</th>\n",
              "      <th>Volume</th>\n",
              "      <th>ticker</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2008-01-02</td>\n",
              "      <td>5.849119186401367</td>\n",
              "      <td>6.0118287377815385</td>\n",
              "      <td>5.780373950029967</td>\n",
              "      <td>5.9821089434096</td>\n",
              "      <td>1079178800</td>\n",
              "      <td>AAPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2008-01-03</td>\n",
              "      <td>5.85181999206543</td>\n",
              "      <td>5.925669548226654</td>\n",
              "      <td>5.784575011135818</td>\n",
              "      <td>5.866229974388948</td>\n",
              "      <td>842066400</td>\n",
              "      <td>AAPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2008-01-04</td>\n",
              "      <td>5.405121803283691</td>\n",
              "      <td>5.793882389595054</td>\n",
              "      <td>5.370298873762459</td>\n",
              "      <td>5.747351346743581</td>\n",
              "      <td>1455832000</td>\n",
              "      <td>AAPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2008-01-07</td>\n",
              "      <td>5.332773208618164</td>\n",
              "      <td>5.51169317331155</td>\n",
              "      <td>5.1103238843778405</td>\n",
              "      <td>5.441145493849881</td>\n",
              "      <td>2072193200</td>\n",
              "      <td>AAPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2008-01-08</td>\n",
              "      <td>5.140944004058838</td>\n",
              "      <td>5.477469947151359</td>\n",
              "      <td>5.127435045283893</td>\n",
              "      <td>5.4078224949057185</td>\n",
              "      <td>1523816000</td>\n",
              "      <td>AAPL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c9122d94-3c6a-47aa-ab86-b0da7e4d434c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c9122d94-3c6a-47aa-ab86-b0da7e4d434c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c9122d94-3c6a-47aa-ab86-b0da7e4d434c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-072d589f-6db8-4b85-b556-9cb62be941a3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-072d589f-6db8-4b85-b556-9cb62be941a3')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-072d589f-6db8-4b85-b556-9cb62be941a3 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"\\\\nDTYPES:\\\\n\\\", df\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2008-01-02 00:00:00\",\n        \"max\": \"2008-01-08 00:00:00\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2008-01-03\",\n          \"2008-01-08\",\n          \"2008-01-04\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Close\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"AAPL\",\n          \"5.849119186401367\",\n          \"5.140944004058838\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"High\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"AAPL\",\n          \"6.0118287377815385\",\n          \"5.477469947151359\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Low\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"AAPL\",\n          \"5.780373950029967\",\n          \"5.127435045283893\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Open\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"AAPL\",\n          \"5.9821089434096\",\n          \"5.4078224949057185\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Volume\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"AAPL\",\n          \"1079178800\",\n          \"1523816000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ticker\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"AAPL\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DTYPES:\n",
            " date      object\n",
            "Close     object\n",
            "High      object\n",
            "Low       object\n",
            "Open      object\n",
            "Volume    object\n",
            "ticker    object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "!python src/data.py\n",
        "# inspect downloaded CSV without parse_dates\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"data/raw/aapl.csv\")\n",
        "print(\"COLUMNS:\", df.columns.tolist())\n",
        "print(\"\\nFIRST 6 ROWS:\")\n",
        "display(df.head(6))\n",
        "print(\"\\nDTYPES:\\n\", df.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wv9LEdDGZClK",
        "outputId": "ba43cedb-d19e-4f73-a7f2-65b2c260eafb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (18.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyarrow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ouga1vMawwN",
        "outputId": "acb6408b-248d-42bc-8d11-3c4af16eac6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved sample slice to data/sample_slice.csv\n",
            "Saved features to data/processed/aapl_features.parquet with 2202 rows\n"
          ]
        }
      ],
      "source": [
        "!python src/features.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_NnZeoUbIRr",
        "outputId": "31451ac5-d238-42ca-940c-1a948dcb0fe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: train_end=2011-04-04 00:00:00, test_start=2011-04-05 00:00:00, test_end=2011-06-29 00:00:00\n",
            "Fold 1: train_end=2011-06-29 00:00:00, test_start=2011-06-30 00:00:00, test_end=2011-09-23 00:00:00\n",
            "Fold 2: train_end=2011-09-23 00:00:00, test_start=2011-09-26 00:00:00, test_end=2011-12-19 00:00:00\n",
            "Fold 3: train_end=2011-12-19 00:00:00, test_start=2011-12-20 00:00:00, test_end=2012-03-16 00:00:00\n",
            "Fold 4: train_end=2012-03-16 00:00:00, test_start=2012-03-19 00:00:00, test_end=2012-06-12 00:00:00\n",
            "Fold 5: train_end=2012-06-12 00:00:00, test_start=2012-06-13 00:00:00, test_end=2012-09-06 00:00:00\n",
            "Fold 6: train_end=2012-09-06 00:00:00, test_start=2012-09-07 00:00:00, test_end=2012-12-04 00:00:00\n",
            "Fold 7: train_end=2012-12-04 00:00:00, test_start=2012-12-05 00:00:00, test_end=2013-03-04 00:00:00\n",
            "Fold 8: train_end=2013-03-04 00:00:00, test_start=2013-03-05 00:00:00, test_end=2013-05-29 00:00:00\n",
            "Fold 9: train_end=2013-05-29 00:00:00, test_start=2013-05-30 00:00:00, test_end=2013-08-22 00:00:00\n",
            "Fold 10: train_end=2013-08-22 00:00:00, test_start=2013-08-23 00:00:00, test_end=2013-11-15 00:00:00\n",
            "Fold 11: train_end=2013-11-15 00:00:00, test_start=2013-11-18 00:00:00, test_end=2014-02-13 00:00:00\n",
            "Fold 12: train_end=2014-02-13 00:00:00, test_start=2014-02-14 00:00:00, test_end=2014-05-12 00:00:00\n",
            "Fold 13: train_end=2014-05-12 00:00:00, test_start=2014-05-13 00:00:00, test_end=2014-08-06 00:00:00\n",
            "Fold 14: train_end=2014-08-06 00:00:00, test_start=2014-08-07 00:00:00, test_end=2014-10-30 00:00:00\n",
            "Fold 15: train_end=2014-10-30 00:00:00, test_start=2014-10-31 00:00:00, test_end=2015-01-28 00:00:00\n",
            "Fold 16: train_end=2015-01-28 00:00:00, test_start=2015-01-29 00:00:00, test_end=2015-04-24 00:00:00\n",
            "Fold 17: train_end=2015-04-24 00:00:00, test_start=2015-04-27 00:00:00, test_end=2015-07-21 00:00:00\n",
            "Fold 18: train_end=2015-07-21 00:00:00, test_start=2015-07-22 00:00:00, test_end=2015-10-14 00:00:00\n",
            "Fold 19: train_end=2015-10-14 00:00:00, test_start=2015-10-15 00:00:00, test_end=2016-01-11 00:00:00\n",
            "Fold 20: train_end=2016-01-11 00:00:00, test_start=2016-01-12 00:00:00, test_end=2016-04-07 00:00:00\n",
            "Fold 21: train_end=2016-04-07 00:00:00, test_start=2016-04-08 00:00:00, test_end=2016-07-01 00:00:00\n",
            "Fold 22: train_end=2016-07-01 00:00:00, test_start=2016-07-05 00:00:00, test_end=2016-09-27 00:00:00\n",
            "Saved preds and metrics to outputs/baseline_preds.json\n"
          ]
        }
      ],
      "source": [
        "!python src/train_baseline.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/eval.py --preds_json outputs/baseline_preds.json --out outputs/baseline_metrics.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6gbo3iIV7nd",
        "outputId": "f898de84-3e68-4a1e-be5e-52615aeed381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved metrics to outputs/baseline_metrics.json\n",
            "{\n",
            "  \"MAE_mean\": 0.012199034394008003,\n",
            "  \"MAE_lo\": 0.011022186807270566,\n",
            "  \"MAE_hi\": 0.013375881980745439,\n",
            "  \"RMSE_mean\": 0.016519844699163334,\n",
            "  \"RMSE_lo\": 0.014884440957546752,\n",
            "  \"RMSE_hi\": 0.018155248440779915,\n",
            "  \"DA_mean\": 50.362318840579704,\n",
            "  \"DA_lo\": 46.81180730245322,\n",
            "  \"DA_hi\": 53.91283037870619\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python - <<'PY'\n",
        "import pandas as pd, numpy as np, json, math\n",
        "from scipy import stats\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# load processed features and preds\n",
        "df = pd.read_parquet(\"data/processed/aapl_features.parquet\")\n",
        "blob = json.load(open(\"outputs/baseline_preds.json\"))\n",
        "\n",
        "# support both formats: list OR {\"per_fold\": [...], \"summary\": {...}}\n",
        "if isinstance(blob, dict) and \"per_fold\" in blob:\n",
        "    all_preds = blob[\"per_fold\"]\n",
        "else:\n",
        "    all_preds = blob\n",
        "\n",
        "# helper: compute RMSE\n",
        "def rmse(y_true, y_pred):\n",
        "    y_true = np.array(y_true); y_pred = np.array(y_pred)\n",
        "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
        "\n",
        "# reconstruct folds used in train_baseline.py (same rolling_origin logic)\n",
        "dates = sorted(df['date'].unique())\n",
        "initial_train_days=800; test_days=60; step_days=60\n",
        "folds=[]\n",
        "start_idx=0\n",
        "while True:\n",
        "    train_end = start_idx + initial_train_days - 1\n",
        "    test_start = train_end + 1\n",
        "    test_end = test_start + test_days - 1\n",
        "    if test_end >= len(dates): break\n",
        "    folds.append((dates[0], dates[train_end], dates[test_start], dates[test_end]))\n",
        "    start_idx += step_days\n",
        "\n",
        "naive_maes=[]; naive_rmses=[]; naive_das=[]\n",
        "ridge_maes=[]; ridge_rmses=[]; ridge_das=[]\n",
        "# load ridge per-fold results (make sure fold keys are ints)\n",
        "ridge_by_fold = {}\n",
        "for rec in all_preds:\n",
        "    fold_key = rec.get('fold')\n",
        "    if fold_key is None:\n",
        "        continue\n",
        "    ridge_by_fold[int(fold_key)] = rec\n",
        "\n",
        "for i,(train_start, train_end, test_start, test_end) in enumerate(folds):\n",
        "    test_mask = (df['date']>=test_start)&(df['date']<=test_end)\n",
        "    test_df = df[test_mask].copy().reset_index(drop=True)\n",
        "    # find global indices in original df\n",
        "    global_idx = df.index[(df['date']>=test_start) & (df['date']<=test_end)].tolist()\n",
        "\n",
        "    # Handle cases where global_idx might be too early for a previous day's close\n",
        "    # The 'ret_next' column is available, so this price-scale path is likely not taken.\n",
        "    # However, for robustness, ensuring prev_close is handled correctly.\n",
        "    if global_idx and min(global_idx) > 0:\n",
        "      prev_idx_for_close_comparison = [idx-1 for idx in global_idx]\n",
        "      prev_close_for_comparison = df.loc[prev_idx_for_close_comparison, 'Close'].values\n",
        "    else:\n",
        "      prev_close_for_comparison = np.array([])\n",
        "\n",
        "    # Decide whether we are working in returns or price scale\n",
        "    if 'ret_next' in df.columns:\n",
        "        # return-scale naive: predict previous day's return (ret_lag_1) if present else zero\n",
        "        y_true = df.loc[global_idx, 'ret_next'].values\n",
        "        if 'ret_lag_1' in df.columns:\n",
        "            naive_pred = df.loc[global_idx, 'ret_lag_1'].values\n",
        "            if len(naive_pred) != len(y_true):\n",
        "                naive_pred = naive_pred[:len(y_true)]\n",
        "        else:\n",
        "            naive_pred = np.zeros_like(y_true)\n",
        "\n",
        "        # Ensure y_true and naive_pred are not empty before computing metrics\n",
        "        if len(y_true) > 0 and len(naive_pred) > 0:\n",
        "            naive_maes.append(float(mean_absolute_error(y_true, naive_pred)))\n",
        "            naive_rmses.append(rmse(y_true, naive_pred))\n",
        "            da_naive = np.mean(np.sign(naive_pred) == np.sign(y_true))\n",
        "            naive_das.append(float(100.0 * da_naive))\n",
        "        else:\n",
        "            naive_maes.append(0.0)\n",
        "            naive_rmses.append(0.0)\n",
        "            naive_das.append(0.0)\n",
        "    else:\n",
        "        # This block is likely dead code since 'ret_next' is expected\n",
        "        # price-scale naive: previous day's Close predicts next-day close\n",
        "        y_true = df.loc[global_idx, 'close_next'].values\n",
        "        naive_pred = prev_close_for_comparison # Assuming prev_close_for_comparison here is Close_t for close_next_t+1\n",
        "        # Ensure y_true and naive_pred have the same length for MAE/RMSE\n",
        "        min_len = min(len(y_true), len(naive_pred))\n",
        "        if min_len == 0:\n",
        "            naive_maes.append(0.0)\n",
        "            naive_rmses.append(0.0)\n",
        "            naive_das.append(0.0)\n",
        "        else:\n",
        "            naive_maes.append(float(mean_absolute_error(y_true[:min_len], naive_pred[:min_len])))\n",
        "            naive_rmses.append(rmse(y_true[:min_len], naive_pred[:min_len]))\n",
        "            # For directional accuracy, compare (y_true - prev_close) with (naive_pred - prev_close)\n",
        "            if len(prev_close_for_comparison) >= min_len:\n",
        "                da_naive = np.mean(np.sign(naive_pred[:min_len] - prev_close_for_comparison[:min_len]) == np.sign(y_true[:min_len] - prev_close_for_comparison[:min_len]))\n",
        "            else:\n",
        "                da_naive = 0.0 # Not enough data for comparison\n",
        "            naive_das.append(float(100.0 * da_naive))\n",
        "\n",
        "    # ridge per-fold\n",
        "    ridge_rec = ridge_by_fold.get(i)\n",
        "    if ridge_rec is None:\n",
        "        # fold missing in preds JSON\n",
        "        continue\n",
        "\n",
        "    ridge_y = np.array(ridge_rec.get('y_true', []))\n",
        "    ridge_p = np.array(ridge_rec.get('y_pred', []))\n",
        "    # filter finite and align\n",
        "    mask = np.isfinite(ridge_y) & np.isfinite(ridge_p)\n",
        "    ridge_y = ridge_y[mask]; ridge_p = ridge_p[mask]\n",
        "    if len(ridge_y) == 0:\n",
        "        continue\n",
        "\n",
        "    ridge_maes.append(float(mean_absolute_error(ridge_y, ridge_p)))\n",
        "    ridge_rmses.append(rmse(ridge_y, ridge_p))\n",
        "\n",
        "    # Directional accuracy for Ridge model (always on returns as per train_baseline.py)\n",
        "    da_ridge = np.mean(np.sign(ridge_p) == np.sign(ridge_y))\n",
        "    ridge_das.append(float(100.0 * da_ridge))\n",
        "\n",
        "def t_ci(x,alpha=0.95):\n",
        "    x = np.array(x); n=len(x)\n",
        "    mean = float(np.mean(x)) if n>0 else 0.0\n",
        "    se = float(np.std(x, ddof=1)/math.sqrt(n)) if n>1 else 0.0\n",
        "    t = stats.t.ppf((1+alpha)/2., n-1) if n>1 else 0.0\n",
        "    return mean, mean - t*se, mean + t*se\n",
        "\n",
        "out = {\n",
        " \"seasonal_naive\": {\n",
        "   \"MAE_mean\": t_ci(naive_maes)[0], \"MAE_lo\": t_ci(naive_maes)[1], \"MAE_hi\": t_ci(naive_maes)[2],\n",
        "   \"RMSE_mean\": t_ci(naive_rmses)[0], \"RMSE_lo\": t_ci(naive_rmses)[1], \"RMSE_hi\": t_ci(naive_rmses)[2],\n",
        "   \"DA_mean\": t_ci(naive_das)[0], \"DA_lo\": t_ci(naive_das)[1], \"DA_hi\": t_ci(naive_das)[2]\n",
        " },\n",
        " \"ridge_baseline\": {\n",
        "   \"MAE_mean\": t_ci(ridge_maes)[0], \"MAE_lo\": t_ci(ridge_maes)[1], \"MAE_hi\": t_ci(ridge_maes)[2],\n",
        "   \"RMSE_mean\": t_ci(ridge_rmses)[0], \"RMSE_lo\": t_ci(ridge_rmses)[1], \"RMSE_hi\": t_ci(ridge_rmses)[2],\n",
        "   \"DA_mean\": t_ci(ridge_das)[0], \"DA_lo\": t_ci(ridge_das)[1], \"DA_hi\": t_ci(ridge_das)[2]\n",
        " }\n",
        "}\n",
        "\n",
        "# Directly print the 'out' dictionary as pretty-formatted JSON\n",
        "print(json.dumps(out, indent=2))\n",
        "PY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZszrbKaa1q0",
        "outputId": "13224116-ad70-4f63-974a-8c1f43656614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"seasonal_naive\": {\n",
            "    \"MAE_mean\": 0.017733695857360576,\n",
            "    \"MAE_lo\": 0.015843833773274793,\n",
            "    \"MAE_hi\": 0.019623557941446358,\n",
            "    \"RMSE_mean\": 0.023151427251859143,\n",
            "    \"RMSE_lo\": 0.02059742925307313,\n",
            "    \"RMSE_hi\": 0.025705425250645158,\n",
            "    \"DA_mean\": 49.71014492753623,\n",
            "    \"DA_lo\": 46.96441334605667,\n",
            "    \"DA_hi\": 52.45587650901578\n",
            "  },\n",
            "  \"ridge_baseline\": {\n",
            "    \"MAE_mean\": 0.012199034394008003,\n",
            "    \"MAE_lo\": 0.011022186807270566,\n",
            "    \"MAE_hi\": 0.013375881980745439,\n",
            "    \"RMSE_mean\": 0.016519844699163334,\n",
            "    \"RMSE_lo\": 0.014884440957546752,\n",
            "    \"RMSE_hi\": 0.018155248440779915,\n",
            "    \"DA_mean\": 50.362318840579704,\n",
            "    \"DA_lo\": 46.81180730245322,\n",
            "    \"DA_hi\": 53.91283037870619\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72440d20",
        "outputId": "3bd7e849-c8e4-4f14-abd0-f54aa2661713"
      },
      "source": [
        "%%writefile src/train_lstm.py\n",
        "#!/usr/bin/env python3\n",
        "# save as src/train_lstm.py\n",
        "import argparse, os, json, math\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from scipy import stats\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ---------------------------\n",
        "# PyTorch LSTM model\n",
        "# ---------------------------\n",
        "class SeqDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        # X: (#samples, seq_len, n_features), y: (#samples,)\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = y.astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class LSTMRegressor(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=64, num_layers=1, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
        "                            batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, F)\n",
        "        out, _ = self.lstm(x)           # out: (B, T, hidden)\n",
        "        out = out[:, -1, :]             # take last timestep\n",
        "        out = self.fc(out)              # (B,1)\n",
        "        return out.squeeze(1)          # (B,)\n",
        "\n",
        "# ---------------------------\n",
        "# Utilities\n",
        "# ---------------------------\n",
        "def rmse(y_true, y_pred):\n",
        "    y_true = np.array(y_true); y_pred = np.array(y_pred)\n",
        "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
        "\n",
        "def t_ci(x, alpha=0.95):\n",
        "    x = np.array(x, dtype=float); n=len(x)\n",
        "    if n == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    mean = float(np.mean(x))\n",
        "    if n == 1:\n",
        "        return mean, mean, mean\n",
        "    se = float(np.std(x, ddof=1)/math.sqrt(n))\n",
        "    t = stats.t.ppf((1+alpha)/2., n-1)\n",
        "    return mean, mean - t*se, mean + t*se\n",
        "\n",
        "def directional_accuracy_percent(y_true, y_pred):\n",
        "    y_true = np.array(y_true, dtype=float)\n",
        "    y_pred = np.array(y_pred, dtype=float)\n",
        "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
        "    if mask.sum() == 0:\n",
        "        return 0.0\n",
        "    y_true = y_true[mask]; y_pred = y_pred[mask]\n",
        "    return float(100.0 * np.mean(np.sign(y_true) == np.sign(y_pred)))\n",
        "\n",
        "# ---------------------------\n",
        "# Build sliding sequences\n",
        "# ---------------------------\n",
        "def build_sequences(df, features, seq_len):\n",
        "    \"\"\"\n",
        "    Given sorted df, returns:\n",
        "      indices: list of indices i for which sequence X[i] ends at row i (so target is at row i)\n",
        "      X_seq: np.array (#samples, seq_len, n_features)\n",
        "      y: np.array (#samples,) -> df.loc[i, 'ret_next']\n",
        "    Only rows where full seq_len history exists are included.\n",
        "    \"\"\"\n",
        "    n = len(df)\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "    idx_list = []\n",
        "    arr = df[features].values\n",
        "    targets = df['ret_next'].values\n",
        "    for end_idx in range(seq_len - 1, n):\n",
        "        start_idx = end_idx - (seq_len - 1)\n",
        "        # ensure target exists and finite\n",
        "        if not np.isfinite(targets[end_idx]):\n",
        "            continue\n",
        "        seq = arr[start_idx:end_idx+1]  # shape (seq_len, n_features)\n",
        "        if np.any(~np.isfinite(seq)):\n",
        "            continue\n",
        "        X_list.append(seq)\n",
        "        y_list.append(targets[end_idx])\n",
        "        idx_list.append(end_idx)\n",
        "    if len(X_list) == 0:\n",
        "        return np.empty((0, seq_len, len(features))), np.empty((0,)), []\n",
        "    return np.stack(X_list), np.array(y_list), idx_list\n",
        "\n",
        "# ---------------------------\n",
        "# Train per-fold LSTM\n",
        "# ---------------------------\n",
        "def train_epoch(model, loader, opt, loss_fn, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for Xb, yb in loader:\n",
        "        Xb = Xb.to(device); yb = yb.to(device)\n",
        "        opt.zero_grad()\n",
        "        out = model(Xb)\n",
        "        loss = loss_fn(out, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += float(loss.item()) * Xb.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def predict(model, loader, device):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    trues = []\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in loader:\n",
        "            Xb = Xb.to(device)\n",
        "            out = model(Xb).cpu().numpy()\n",
        "            preds.append(out)\n",
        "            trues.append(yb.numpy())\n",
        "    if len(preds) == 0:\n",
        "        return np.array([]), np.array([])\n",
        "    return np.concatenate(trues), np.concatenate(preds)\n",
        "\n",
        "# ---------------------------\n",
        "# Main: rolling folds\n",
        "# ---------------------------\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--features\", default=\"data/processed/aapl_features.parquet\")\n",
        "    parser.add_argument(\"--out\", default=\"outputs/lstm_preds.json\")\n",
        "    parser.add_argument(\"--seq_len\", type=int, default=10)\n",
        "    parser.add_argument(\"--hidden\", type=int, default=64)\n",
        "    parser.add_argument(\"--epochs\", type=int, default=20)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=64)\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    parser.add_argument(\"--device\", default=\"cpu\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    df = pd.read_parquet(args.features).sort_values('date').reset_index(drop=True)\n",
        "    # choose features to feed LSTM per-timestep\n",
        "    features = [c for c in df.columns if c.startswith('ret_lag_') or c.startswith('ret_roll_') or c in ['ret']]\n",
        "    if len(features) == 0:\n",
        "        raise SystemExit(\"No features found for LSTM. Check your processed parquet columns.\")\n",
        "\n",
        "    # rolling-origin folds (same as baseline)\n",
        "    dates = sorted(df['date'].unique())\n",
        "    initial_train_days=800; test_days=60; step_days=60\n",
        "    folds=[]\n",
        "    start_idx=0\n",
        "    while True:\n",
        "        train_end = start_idx + initial_train_days - 1\n",
        "        test_start = train_end + 1\n",
        "        test_end = test_start + test_days - 1\n",
        "        if test_end >= len(dates): break\n",
        "        folds.append((dates[0], dates[train_end], dates[test_start], dates[test_end]))\n",
        "        start_idx += step_days\n",
        "\n",
        "    device = torch.device(args.device if torch.cuda.is_available() or args.device=='cpu' else 'cpu')\n",
        "    out_recs = []\n",
        "\n",
        "    for idx, (train_start, train_end, test_start, test_end) in enumerate(folds):\n",
        "        print(f\"[Fold {idx}] train_end={train_end}, test_start={test_start}, test_end={test_end}\")\n",
        "        # split df by date\n",
        "        train_mask = df['date'] <= train_end\n",
        "        test_mask = (df['date'] >= test_start) & (df['date'] <= test_end)\n",
        "        df_train = df[train_mask].reset_index(drop=True)\n",
        "        df_test = df[test_mask].reset_index(drop=True)\n",
        "\n",
        "        # Build sequences from the full train portion to scale properly and then build for test using scaler\n",
        "        X_train_seq, y_train_seq, idxs_train = build_sequences(df_train, features, args.seq_len)\n",
        "        # for test sequences we need context that may include rows inside train - so build sequences on concatenated df up to test_end\n",
        "        df_upto_test = df[df['date'] <= test_end].reset_index(drop=True)\n",
        "        X_all_seq, y_all_seq, idxs_all = build_sequences(df_upto_test, features, args.seq_len)\n",
        "\n",
        "        # identify which sequences correspond to test indices (those whose end idx is in df_upto_test and date between test_start/test_end)\n",
        "        test_end_positions = []\n",
        "        # map idxs_all (which are indices in df_upto_test) to dates; need to pick those with date in test range\n",
        "        dates_all = df_upto_test['date'].values\n",
        "        for pos, end_idx in enumerate(idxs_all):\n",
        "            dt = dates_all[end_idx]\n",
        "            if (dt >= np.datetime64(test_start)) and (dt <= np.datetime64(test_end)):\n",
        "                test_end_positions.append(pos)\n",
        "\n",
        "        # now prepare X_test_seq and y_test_seq\n",
        "        if len(test_end_positions) == 0:\n",
        "            print(f\"  no test sequences for fold {idx}, skipping\")\n",
        "            continue\n",
        "        X_test_seq = X_all_seq[test_end_positions]\n",
        "        y_test_seq = y_all_seq[test_end_positions]\n",
        "\n",
        "        # scale features: fit scaler on flattened training sequences (all timesteps)\n",
        "        n_feats = len(features)\n",
        "        if X_train_seq.shape[0] == 0:\n",
        "            print(\"  no train sequences for this fold; skipping\")\n",
        "            continue\n",
        "        # flatten to (num_samples * seq_len, n_feats)\n",
        "        flat_train = X_train_seq.reshape(-1, n_feats)\n",
        "        scaler = StandardScaler().fit(flat_train)\n",
        "        # apply scaler\n",
        "        X_train_seq_scaled = scaler.transform(flat_train).reshape(X_train_seq.shape)\n",
        "        X_test_seq_scaled = scaler.transform(X_test_seq.reshape(-1, n_feats)).reshape(X_test_seq.shape)\n",
        "\n",
        "        # build dataloaders\n",
        "        train_ds = SeqDataset(X_train_seq_scaled, y_train_seq)\n",
        "        test_ds = SeqDataset(X_test_seq_scaled, y_test_seq)\n",
        "        train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True)\n",
        "        test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "        # model\n",
        "        model = LSTMRegressor(input_size=n_feats, hidden_size=args.hidden).to(device)\n",
        "        loss_fn = nn.MSELoss()\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "        # training loop\n",
        "        for ep in range(args.epochs):\n",
        "            tr_loss = train_epoch(model, train_loader, opt, loss_fn, device)\n",
        "            if (ep+1) % 5 == 0 or ep == 0 or ep == args.epochs-1:\n",
        "                print(f\"   Epoch {ep+1}/{args.epochs}, train_loss={tr_loss:.6f}\")\n",
        "\n",
        "        # predict on test\n",
        "        y_true, y_pred = predict(model, test_loader, device)\n",
        "        # convert to plain floats\n",
        "        y_true_list = [float(x) for x in y_true.tolist()]\n",
        "        y_pred_list = [float(x) for x in y_pred.tolist()]\n",
        "\n",
        "        mae_val = float(mean_absolute_error(y_true, y_pred))\n",
        "        rmse_val = rmse(y_true, y_pred)\n",
        "        da_val = directional_accuracy_percent(y_true, y_pred)\n",
        "\n",
        "        # record\n",
        "        out_rec = {\n",
        "            \"fold\": int(idx),\n",
        "            \"model\": \"lstm_numerical\",\n",
        "            \"train_start\": pd.to_datetime(str(train_start)).isoformat(),\n",
        "            \"train_end\": pd.to_datetime(str(train_end)).isoformat(),\n",
        "            \"test_start\": pd.to_datetime(str(test_start)).isoformat(),\n",
        "            \"test_end\": pd.to_datetime(str(test_end)).isoformat(),\n",
        "            \"y_true\": y_true_list,\n",
        "            \"y_pred\": y_pred_list,\n",
        "            \"metrics\": {\n",
        "                \"MAE\": mae_val,\n",
        "                \"RMSE\": rmse_val,\n",
        "                \"DA_percent\": da_val\n",
        "            }\n",
        "        }\n",
        "        out_recs.append(out_rec)\n",
        "\n",
        "    # summary across folds\n",
        "    maes = [r[\"metrics\"][\"MAE\"] for r in out_recs]\n",
        "    rmses = [r[\"metrics\"][\"RMSE\"] for r in out_recs]\n",
        "    das = [r[\"metrics\"][\"DA_percent\"] for r in out_recs]\n",
        "    summary = {\n",
        "        \"MAE\": {\"mean\": t_ci(maes)[0], \"lo\": t_ci(maes)[1], \"hi\": t_ci(maes)[2]},\n",
        "        \"RMSE\": {\"mean\": t_ci(rmses)[0], \"lo\": t_ci(rmses)[1], \"hi\": t_ci(rmses)[2]},\n",
        "        \"DA_percent\": {\"mean\": t_ci(das)[0], \"lo\": t_ci(das)[1], \"hi\": t_ci(das)[2]}\n",
        "    }\n",
        "\n",
        "    os.makedirs(os.path.dirname(args.out), exist_ok=True)\n",
        "    out_blob = {\"per_fold\": out_recs, \"summary\": summary}\n",
        "    with open(args.out, \"w\") as f:\n",
        "        json.dump(out_blob, f, indent=2)\n",
        "    print(\"Saved LSTM preds+metrics to\", args.out)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/train_lstm.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/features.py --input data/raw/aapl.csv --out data/processed/aapl_features.parquet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSTASdMfjdAf",
        "outputId": "b929ee11-5a27-47df-9770-4d1b963c8263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved sample slice to data/sample_slice.csv\n",
            "Saved features to data/processed/aapl_features.parquet with 2202 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/train_lstm.py --features data/processed/aapl_features.parquet --out outputs/lstm_preds.json --seq_len 10 --epochs 20 --hidden 64 --batch_size 64 --lr 1e-3 --device cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQJwz3zSjqJ0",
        "outputId": "3701009a-370c-4ca3-fcf7-7591e434ae33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] train_end=2011-04-04 00:00:00, test_start=2011-04-05 00:00:00, test_end=2011-06-29 00:00:00\n",
            "   Epoch 1/20, train_loss=0.002470\n",
            "   Epoch 5/20, train_loss=0.000611\n",
            "   Epoch 10/20, train_loss=0.000561\n",
            "   Epoch 15/20, train_loss=0.000554\n",
            "   Epoch 20/20, train_loss=0.000500\n",
            "[Fold 1] train_end=2011-06-29 00:00:00, test_start=2011-06-30 00:00:00, test_end=2011-09-23 00:00:00\n",
            "   Epoch 1/20, train_loss=0.002058\n",
            "   Epoch 5/20, train_loss=0.000585\n",
            "   Epoch 10/20, train_loss=0.000506\n",
            "   Epoch 15/20, train_loss=0.000469\n",
            "   Epoch 20/20, train_loss=0.000450\n",
            "[Fold 2] train_end=2011-09-23 00:00:00, test_start=2011-09-26 00:00:00, test_end=2011-12-19 00:00:00\n",
            "   Epoch 1/20, train_loss=0.003233\n",
            "   Epoch 5/20, train_loss=0.000544\n",
            "   Epoch 10/20, train_loss=0.000502\n",
            "   Epoch 15/20, train_loss=0.000475\n",
            "   Epoch 20/20, train_loss=0.000441\n",
            "[Fold 3] train_end=2011-12-19 00:00:00, test_start=2011-12-20 00:00:00, test_end=2012-03-16 00:00:00\n",
            "   Epoch 1/20, train_loss=0.001922\n",
            "   Epoch 5/20, train_loss=0.000539\n",
            "   Epoch 10/20, train_loss=0.000521\n",
            "   Epoch 15/20, train_loss=0.000469\n",
            "   Epoch 20/20, train_loss=0.000447\n",
            "[Fold 4] train_end=2012-03-16 00:00:00, test_start=2012-03-19 00:00:00, test_end=2012-06-12 00:00:00\n",
            "   Epoch 1/20, train_loss=0.003219\n",
            "   Epoch 5/20, train_loss=0.000543\n",
            "   Epoch 10/20, train_loss=0.000494\n",
            "   Epoch 15/20, train_loss=0.000468\n",
            "   Epoch 20/20, train_loss=0.000432\n",
            "[Fold 5] train_end=2012-06-12 00:00:00, test_start=2012-06-13 00:00:00, test_end=2012-09-06 00:00:00\n",
            "   Epoch 1/20, train_loss=0.001053\n",
            "   Epoch 5/20, train_loss=0.000682\n",
            "   Epoch 10/20, train_loss=0.000482\n",
            "   Epoch 15/20, train_loss=0.000462\n",
            "   Epoch 20/20, train_loss=0.000442\n",
            "[Fold 6] train_end=2012-09-06 00:00:00, test_start=2012-09-07 00:00:00, test_end=2012-12-04 00:00:00\n",
            "   Epoch 1/20, train_loss=0.003853\n",
            "   Epoch 5/20, train_loss=0.000497\n",
            "   Epoch 10/20, train_loss=0.000462\n",
            "   Epoch 15/20, train_loss=0.000433\n",
            "   Epoch 20/20, train_loss=0.000429\n",
            "[Fold 7] train_end=2012-12-04 00:00:00, test_start=2012-12-05 00:00:00, test_end=2013-03-04 00:00:00\n",
            "   Epoch 1/20, train_loss=0.001694\n",
            "   Epoch 5/20, train_loss=0.000505\n",
            "   Epoch 10/20, train_loss=0.000454\n",
            "   Epoch 15/20, train_loss=0.000426\n",
            "   Epoch 20/20, train_loss=0.000403\n",
            "[Fold 8] train_end=2013-03-04 00:00:00, test_start=2013-03-05 00:00:00, test_end=2013-05-29 00:00:00\n",
            "   Epoch 1/20, train_loss=0.002717\n",
            "   Epoch 5/20, train_loss=0.000496\n",
            "   Epoch 10/20, train_loss=0.000457\n",
            "   Epoch 15/20, train_loss=0.000453\n",
            "   Epoch 20/20, train_loss=0.000419\n",
            "[Fold 9] train_end=2013-05-29 00:00:00, test_start=2013-05-30 00:00:00, test_end=2013-08-22 00:00:00\n",
            "   Epoch 1/20, train_loss=0.001183\n",
            "   Epoch 5/20, train_loss=0.000506\n",
            "   Epoch 10/20, train_loss=0.000473\n",
            "   Epoch 15/20, train_loss=0.000441\n",
            "   Epoch 20/20, train_loss=0.000422\n",
            "[Fold 10] train_end=2013-08-22 00:00:00, test_start=2013-08-23 00:00:00, test_end=2013-11-15 00:00:00\n",
            "   Epoch 1/20, train_loss=0.004381\n",
            "   Epoch 5/20, train_loss=0.000504\n",
            "   Epoch 10/20, train_loss=0.000447\n",
            "   Epoch 15/20, train_loss=0.000431\n",
            "   Epoch 20/20, train_loss=0.000400\n",
            "[Fold 11] train_end=2013-11-15 00:00:00, test_start=2013-11-18 00:00:00, test_end=2014-02-13 00:00:00\n",
            "   Epoch 1/20, train_loss=0.003703\n",
            "   Epoch 5/20, train_loss=0.000487\n",
            "   Epoch 10/20, train_loss=0.000453\n",
            "   Epoch 15/20, train_loss=0.000439\n",
            "   Epoch 20/20, train_loss=0.000411\n",
            "[Fold 12] train_end=2014-02-13 00:00:00, test_start=2014-02-14 00:00:00, test_end=2014-05-12 00:00:00\n",
            "   Epoch 1/20, train_loss=0.000867\n",
            "   Epoch 5/20, train_loss=0.000485\n",
            "   Epoch 10/20, train_loss=0.000437\n",
            "   Epoch 15/20, train_loss=0.000411\n",
            "   Epoch 20/20, train_loss=0.000386\n",
            "[Fold 13] train_end=2014-05-12 00:00:00, test_start=2014-05-13 00:00:00, test_end=2014-08-06 00:00:00\n",
            "   Epoch 1/20, train_loss=0.000802\n",
            "   Epoch 5/20, train_loss=0.000454\n",
            "   Epoch 10/20, train_loss=0.000406\n",
            "   Epoch 15/20, train_loss=0.000391\n",
            "   Epoch 20/20, train_loss=0.000385\n",
            "[Fold 14] train_end=2014-08-06 00:00:00, test_start=2014-08-07 00:00:00, test_end=2014-10-30 00:00:00\n",
            "   Epoch 1/20, train_loss=0.003121\n",
            "   Epoch 5/20, train_loss=0.000448\n",
            "   Epoch 10/20, train_loss=0.000419\n",
            "   Epoch 15/20, train_loss=0.000387\n",
            "   Epoch 20/20, train_loss=0.000374\n",
            "[Fold 15] train_end=2014-10-30 00:00:00, test_start=2014-10-31 00:00:00, test_end=2015-01-28 00:00:00\n",
            "   Epoch 1/20, train_loss=0.003859\n",
            "   Epoch 5/20, train_loss=0.000446\n",
            "   Epoch 10/20, train_loss=0.000407\n",
            "   Epoch 15/20, train_loss=0.000380\n",
            "   Epoch 20/20, train_loss=0.000375\n",
            "[Fold 16] train_end=2015-01-28 00:00:00, test_start=2015-01-29 00:00:00, test_end=2015-04-24 00:00:00\n",
            "   Epoch 1/20, train_loss=0.000835\n",
            "   Epoch 5/20, train_loss=0.000431\n",
            "   Epoch 10/20, train_loss=0.000392\n",
            "   Epoch 15/20, train_loss=0.000378\n",
            "   Epoch 20/20, train_loss=0.000363\n",
            "[Fold 17] train_end=2015-04-24 00:00:00, test_start=2015-04-27 00:00:00, test_end=2015-07-21 00:00:00\n",
            "   Epoch 1/20, train_loss=0.000743\n",
            "   Epoch 5/20, train_loss=0.000420\n",
            "   Epoch 10/20, train_loss=0.000388\n",
            "   Epoch 15/20, train_loss=0.000372\n",
            "   Epoch 20/20, train_loss=0.000350\n",
            "[Fold 18] train_end=2015-07-21 00:00:00, test_start=2015-07-22 00:00:00, test_end=2015-10-14 00:00:00\n",
            "   Epoch 1/20, train_loss=0.001396\n",
            "   Epoch 5/20, train_loss=0.000423\n",
            "   Epoch 10/20, train_loss=0.000407\n",
            "   Epoch 15/20, train_loss=0.000365\n",
            "   Epoch 20/20, train_loss=0.000344\n",
            "[Fold 19] train_end=2015-10-14 00:00:00, test_start=2015-10-15 00:00:00, test_end=2016-01-11 00:00:00\n",
            "   Epoch 1/20, train_loss=0.001275\n",
            "   Epoch 5/20, train_loss=0.000436\n",
            "   Epoch 10/20, train_loss=0.000392\n",
            "   Epoch 15/20, train_loss=0.000371\n",
            "   Epoch 20/20, train_loss=0.000357\n",
            "[Fold 20] train_end=2016-01-11 00:00:00, test_start=2016-01-12 00:00:00, test_end=2016-04-07 00:00:00\n",
            "   Epoch 1/20, train_loss=0.001106\n",
            "   Epoch 5/20, train_loss=0.000410\n",
            "   Epoch 10/20, train_loss=0.000386\n",
            "   Epoch 15/20, train_loss=0.000370\n",
            "   Epoch 20/20, train_loss=0.000372\n",
            "[Fold 21] train_end=2016-04-07 00:00:00, test_start=2016-04-08 00:00:00, test_end=2016-07-01 00:00:00\n",
            "   Epoch 1/20, train_loss=0.001280\n",
            "   Epoch 5/20, train_loss=0.000410\n",
            "   Epoch 10/20, train_loss=0.000396\n",
            "   Epoch 15/20, train_loss=0.000369\n",
            "   Epoch 20/20, train_loss=0.000429\n",
            "[Fold 22] train_end=2016-07-01 00:00:00, test_start=2016-07-05 00:00:00, test_end=2016-09-27 00:00:00\n",
            "   Epoch 1/20, train_loss=0.000959\n",
            "   Epoch 5/20, train_loss=0.000404\n",
            "   Epoch 10/20, train_loss=0.000370\n",
            "   Epoch 15/20, train_loss=0.000355\n",
            "   Epoch 20/20, train_loss=0.000346\n",
            "Saved LSTM preds+metrics to outputs/lstm_preds.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/eval.py --preds_json outputs/lstm_preds.json --out outputs/lstm_metrics.json\n",
        "!cat outputs/lstm_metrics.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP9EF2YIkKXw",
        "outputId": "bde68130-4139-484d-a46b-261aad1f8a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved metrics to outputs/lstm_metrics.json\n",
            "{\n",
            "  \"MAE_mean\": 0.012866363107648635,\n",
            "  \"MAE_lo\": 0.011597102762979476,\n",
            "  \"MAE_hi\": 0.014135623452317794,\n",
            "  \"RMSE_mean\": 0.017185575751011716,\n",
            "  \"RMSE_lo\": 0.015485040282239262,\n",
            "  \"RMSE_hi\": 0.01888611121978417,\n",
            "  \"DA_mean\": 50.14492753623189,\n",
            "  \"DA_lo\": 46.44490355990235,\n",
            "  \"DA_hi\": 53.84495151256143\n",
            "}\n",
            "{\n",
            "  \"MAE_mean\": 0.012866363107648635,\n",
            "  \"MAE_lo\": 0.011597102762979476,\n",
            "  \"MAE_hi\": 0.014135623452317794,\n",
            "  \"RMSE_mean\": 0.017185575751011716,\n",
            "  \"RMSE_lo\": 0.015485040282239262,\n",
            "  \"RMSE_hi\": 0.01888611121978417,\n",
            "  \"DA_mean\": 50.14492753623189,\n",
            "  \"DA_lo\": 46.44490355990235,\n",
            "  \"DA_hi\": 53.84495151256143\n",
            "}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/build_text_embeddings.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "build_text_embeddings.py\n",
        "\n",
        "Robust SBERT embedding builder for a news CSV.\n",
        "\n",
        "Features:\n",
        " - Detects date column (configurable with --date_col). Falls back to common names.\n",
        " - If date column is an integer (epoch), converts to datetime (configurable unit).\n",
        " - Allows specifying text column name (--text_col).\n",
        " - Computes SBERT embeddings for entire dataset in batches.\n",
        " - Saves parquet with columns: date (datetime normalized to date), text_emb (list of floats), optionally other meta.\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "from typing import Optional, List\n",
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# sentence-transformers\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception as e:\n",
        "    print(\"ERROR: sentence_transformers not available. Install with `pip install sentence-transformers`.\", file=sys.stderr)\n",
        "    raise\n",
        "\n",
        "COMMON_DATE_COLS = [\"date\", \"created_utc\", \"timestamp\", \"created\", \"publish_date\", \"published_at\", \"time\"]\n",
        "\n",
        "def detect_date_col(df: pd.DataFrame, prefer: Optional[str] = None) -> Optional[str]:\n",
        "    if prefer and prefer in df.columns:\n",
        "        return prefer\n",
        "    for c in COMMON_DATE_COLS:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    # otherwise try to find a datetime-like column\n",
        "    for c in df.columns:\n",
        "        if pd.api.types.is_datetime64_any_dtype(df[c]):\n",
        "            return c\n",
        "    # try numeric columns that look like epoch (very large ints)\n",
        "    for c in df.columns:\n",
        "        if pd.api.types.is_integer_dtype(df[c]) or pd.api.types.is_float_dtype(df[c]):\n",
        "            series = df[c].dropna()\n",
        "            if len(series) == 0:\n",
        "                continue\n",
        "            v = series.iloc[0]\n",
        "            # heuristic: epoch seconds are > 1e9 (since 2001) and < 1e11\n",
        "            if isinstance(v, (int, float)) and (1e9 < abs(v) < 1e12):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "def parse_dates_column(series: pd.Series, unit: str = \"s\") -> pd.Series:\n",
        "    # If already datetime, return normalized datetimes\n",
        "    if pd.api.types.is_datetime64_any_dtype(series):\n",
        "        return pd.to_datetime(series).dt.normalize()\n",
        "    # If numeric, treat as epoch\n",
        "    if pd.api.types.is_integer_dtype(series) or pd.api.types.is_float_dtype(series):\n",
        "        # use pandas to_datetime with unit\n",
        "        return pd.to_datetime(series, unit=unit, errors=\"coerce\").dt.normalize()\n",
        "    # otherwise try parsing strings\n",
        "    return pd.to_datetime(series, errors=\"coerce\").dt.normalize()\n",
        "\n",
        "def main():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--news_csv\", required=True, help=\"Input CSV of news (must contain text column)\")\n",
        "    p.add_argument(\"--out\", required=True, help=\"Output parquet path (will include columns 'date','text_emb')\")\n",
        "    p.add_argument(\"--text_col\", default=None, help=\"Column name containing the text (title/body). If not set, tries common choices.\")\n",
        "    p.add_argument(\"--date_col\", default=None, help=\"Column name containing date/timestamp. If not set, auto-detects.\")\n",
        "    p.add_argument(\"--epoch_unit\", choices=[\"s\", \"ms\"], default=\"s\", help=\"If date column is numeric epoch, unit (seconds 's' or milliseconds 'ms').\")\n",
        "    p.add_argument(\"--model_name\", default=\"sentence-transformers/all-MiniLM-L6-v2\", help=\"SBERT model name\")\n",
        "    p.add_argument(\"--batch_size\", type=int, default=128)\n",
        "    p.add_argument(\"--device\", default=\"cpu\")\n",
        "    p.add_argument(\"--text_cols_try\", nargs=\"*\", default=[\"title\", \"headline\", \"body\", \"content\", \"text\"], help=\"Text columns to try if --text_col not provided\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    path = Path(args.news_csv)\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"{path} does not exist\")\n",
        "\n",
        "    # Read a small sample first to inspect columns\n",
        "    sample = pd.read_csv(path, nrows=10)\n",
        "    print(\"CSV columns:\", sample.columns.tolist())\n",
        "\n",
        "    # decide text column\n",
        "    text_col = args.text_col\n",
        "    if text_col is None:\n",
        "        for c in args.text_cols_try:\n",
        "            if c in sample.columns:\n",
        "                text_col = c\n",
        "                break\n",
        "    if text_col is None:\n",
        "        # fallback: choose first object dtype column that's not the detected date_col\n",
        "        for c in sample.columns:\n",
        "            if sample[c].dtype == object:\n",
        "                text_col = c\n",
        "                break\n",
        "    if text_col is None:\n",
        "        raise ValueError(\"Could not infer text column. Provide --text_col explicitly.\")\n",
        "    print(\"Using text column:\", text_col)\n",
        "\n",
        "    # read full CSV (we'll not parse dates yet)\n",
        "    df = pd.read_csv(path)\n",
        "    print(\"Read full CSV shape:\", df.shape)\n",
        "\n",
        "    # detect date column\n",
        "    date_col = args.date_col or detect_date_col(df)\n",
        "    if date_col is None:\n",
        "        print(\"No date column detected; resulting parquet will have NaT for 'date'. You can pass --date_col to specify.\")\n",
        "        df[\"date_parsed\"] = pd.NaT\n",
        "    else:\n",
        "        print(\"Using date column:\", date_col)\n",
        "        df[\"date_parsed\"] = parse_dates_column(df[date_col], unit=args.epoch_unit)\n",
        "\n",
        "    # normalize to midnight (date-only)\n",
        "    df[\"date_parsed\"] = pd.to_datetime(df[\"date_parsed\"], errors=\"coerce\").dt.normalize()\n",
        "\n",
        "    # build a simple text series (concatenate title+body if both exist)\n",
        "    # If text_col points to a combined field, use it\n",
        "    text_series = df[text_col].astype(str).fillna(\"\").values\n",
        "    # If there's a 'body' column and text_col is only title, try to combine\n",
        "    if \"body\" in df.columns and text_col != \"body\":\n",
        "        # prefer title + body if available\n",
        "        text_series = (df[text_col].fillna(\"\").astype(str) + \". \" + df[\"body\"].fillna(\"\").astype(str)).values\n",
        "\n",
        "    # initialize SBERT model\n",
        "    print(\"Loading SBERT model:\", args.model_name)\n",
        "    model = SentenceTransformer(args.model_name, device=args.device)\n",
        "\n",
        "    # compute embeddings in batches\n",
        "    n = len(text_series)\n",
        "    batch_size = max(1, args.batch_size)\n",
        "    embeddings: List[np.ndarray] = []\n",
        "    print(f\"Computing embeddings for {n} rows with batch_size={batch_size} on device={args.device}\")\n",
        "    for i in tqdm(range(0, n, batch_size)):\n",
        "        batch_texts = text_series[i : i + batch_size].tolist()\n",
        "        emb = model.encode(batch_texts, show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=False)\n",
        "        embeddings.append(emb)\n",
        "    embeddings = np.vstack(embeddings)  # shape (n, emb_dim)\n",
        "    print(\"Embeddings shape:\", embeddings.shape)\n",
        "\n",
        "    # attach embeddings (as python lists) to dataframe\n",
        "    df_out = pd.DataFrame(\n",
        "        {\n",
        "            \"date\": pd.to_datetime(df[\"date_parsed\"]).dt.normalize(),\n",
        "            \"text_emb\": [e.tolist() for e in embeddings],\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # if you want to keep other metadata (e.g., title, url), add them:\n",
        "    # df_out[\"title\"] = df.get(\"title\", pd.Series([\"\"]*len(df)))\n",
        "    # df_out[\"url\"] = df.get(\"url\", pd.Series([\"\"]*len(df)))\n",
        "\n",
        "    out_path = Path(args.out)\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    df_out.to_parquet(out_path, index=False)\n",
        "    print(\"Wrote embeddings to\", out_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKxcUskSk3mV",
        "outputId": "c076a0f8-683b-4365-a4e6-6792bc38eb6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/build_text_embeddings.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch tqdm pandas pyarrow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11CkYMT8k5Ux",
        "outputId": "b528ed0e-1cdb-4d26-926a-03b001ae866e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (18.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python src/build_text_embeddings.py \\\n",
        "  --news_csv data/raw/RedditNews.csv \\\n",
        "  --out data/processed/text_embeddings.parquet \\\n",
        "  --batch_size 128 \\\n",
        "  --device cpu \\\n",
        "  --text_col News \\\n",
        "  --date_col Date \\\n",
        "  --epoch_unit s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzmxEM9RC8FZ",
        "outputId": "3ce7886d-8782-4a73-db6c-499fed2b3f24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV columns: ['Date', 'News']\n",
            "Using text column: News\n",
            "Read full CSV shape: (73608, 2)\n",
            "Using date column: Date\n",
            "Loading SBERT model: sentence-transformers/all-MiniLM-L6-v2\n",
            "Computing embeddings for 73608 rows with batch_size=128 on device=cpu\n",
            "Embeddings shape: (73608, 384)\n",
            "Wrote embeddings to data/processed/text_embeddings.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-09 09:43:26.053493: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762681406.109795   10159 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762681406.127977   10159 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762681406.179382   10159 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762681406.179461   10159 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762681406.179465   10159 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762681406.179467   10159 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-09 09:43:26.192983: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\r  0%|          | 0/576 [00:00<?, ?it/s]\r  0%|          | 1/576 [00:01<18:42,  1.95s/it]\r  0%|          | 2/576 [00:03<17:16,  1.81s/it]\r  1%|          | 3/576 [00:05<16:19,  1.71s/it]\r  1%|          | 4/576 [00:06<15:13,  1.60s/it]\r  1%|          | 5/576 [00:09<19:10,  2.02s/it]\r  1%|          | 6/576 [00:10<17:37,  1.86s/it]\r  1%|          | 7/576 [00:12<16:44,  1.77s/it]\r  1%|         | 8/576 [00:14<16:22,  1.73s/it]\r  2%|         | 9/576 [00:15<16:22,  1.73s/it]\r  2%|         | 10/576 [00:17<16:44,  1.78s/it]\r  2%|         | 11/576 [00:22<26:04,  2.77s/it]\r  2%|         | 12/576 [00:26<29:45,  3.17s/it]\r  2%|         | 13/576 [00:30<29:42,  3.17s/it]\r  2%|         | 14/576 [00:32<26:29,  2.83s/it]\r  3%|         | 15/576 [00:36<30:15,  3.24s/it]\r  3%|         | 16/576 [00:38<28:02,  3.00s/it]\r  3%|         | 17/576 [00:40<25:47,  2.77s/it]\r  3%|         | 18/576 [00:42<22:25,  2.41s/it]\r  3%|         | 19/576 [00:44<19:52,  2.14s/it]\r  3%|         | 20/576 [00:45<18:45,  2.02s/it]\r  4%|         | 21/576 [00:48<19:46,  2.14s/it]\r  4%|         | 22/576 [00:49<18:03,  1.96s/it]\r  4%|         | 23/576 [00:51<17:13,  1.87s/it]\r  4%|         | 24/576 [00:52<16:00,  1.74s/it]\r  4%|         | 25/576 [00:54<15:40,  1.71s/it]\r  5%|         | 26/576 [00:56<15:21,  1.68s/it]\r  5%|         | 27/576 [00:57<14:58,  1.64s/it]\r  5%|         | 28/576 [01:00<16:53,  1.85s/it]\r  5%|         | 29/576 [01:01<16:31,  1.81s/it]\r  5%|         | 30/576 [01:03<16:10,  1.78s/it]\r  5%|         | 31/576 [01:04<15:19,  1.69s/it]\r  6%|         | 32/576 [01:06<15:01,  1.66s/it]\r  6%|         | 33/576 [01:07<14:19,  1.58s/it]\r  6%|         | 34/576 [01:09<13:37,  1.51s/it]\r  6%|         | 35/576 [01:11<14:39,  1.63s/it]\r  6%|         | 36/576 [01:13<16:11,  1.80s/it]\r  6%|         | 37/576 [01:14<15:14,  1.70s/it]\r  7%|         | 38/576 [01:16<14:37,  1.63s/it]\r  7%|         | 39/576 [01:17<14:19,  1.60s/it]\r  7%|         | 40/576 [01:19<14:40,  1.64s/it]\r  7%|         | 41/576 [01:21<14:16,  1.60s/it]\r  7%|         | 42/576 [01:22<13:53,  1.56s/it]\r  7%|         | 43/576 [01:24<15:38,  1.76s/it]\r  8%|         | 44/576 [01:26<15:34,  1.76s/it]\r  8%|         | 45/576 [01:28<14:58,  1.69s/it]\r  8%|         | 46/576 [01:29<14:59,  1.70s/it]\r  8%|         | 47/576 [01:31<14:54,  1.69s/it]\r  8%|         | 48/576 [01:32<14:32,  1.65s/it]\r  9%|         | 49/576 [01:34<14:10,  1.61s/it]\r  9%|         | 50/576 [01:36<15:41,  1.79s/it]\r  9%|         | 51/576 [01:39<17:03,  1.95s/it]\r  9%|         | 52/576 [01:40<16:24,  1.88s/it]\r  9%|         | 53/576 [01:42<15:15,  1.75s/it]\r  9%|         | 54/576 [01:43<14:57,  1.72s/it]\r 10%|         | 55/576 [01:45<14:13,  1.64s/it]\r 10%|         | 56/576 [01:46<14:05,  1.63s/it]\r 10%|         | 57/576 [01:48<14:41,  1.70s/it]\r 10%|         | 58/576 [01:51<16:31,  1.91s/it]\r 10%|         | 59/576 [01:52<15:19,  1.78s/it]\r 10%|         | 60/576 [01:54<15:10,  1.76s/it]\r 11%|         | 61/576 [01:56<15:15,  1.78s/it]\r 11%|         | 62/576 [01:57<14:50,  1.73s/it]\r 11%|         | 63/576 [01:59<14:33,  1.70s/it]\r 11%|         | 64/576 [02:01<14:57,  1.75s/it]\r 11%|        | 65/576 [02:03<16:51,  1.98s/it]\r 11%|        | 66/576 [02:05<16:00,  1.88s/it]\r 12%|        | 67/576 [02:07<15:44,  1.86s/it]\r 12%|        | 68/576 [02:08<15:20,  1.81s/it]\r 12%|        | 69/576 [02:11<16:19,  1.93s/it]\r 12%|        | 70/576 [02:12<15:24,  1.83s/it]\r 12%|        | 71/576 [02:15<16:47,  2.00s/it]\r 12%|        | 72/576 [02:17<16:32,  1.97s/it]\r 13%|        | 73/576 [02:18<15:17,  1.82s/it]\r 13%|        | 74/576 [02:20<14:50,  1.77s/it]\r 13%|        | 75/576 [02:21<14:41,  1.76s/it]\r 13%|        | 76/576 [02:23<14:54,  1.79s/it]\r 13%|        | 77/576 [02:25<14:37,  1.76s/it]\r 14%|        | 78/576 [02:27<16:17,  1.96s/it]\r 14%|        | 79/576 [02:29<15:41,  1.90s/it]\r 14%|        | 80/576 [02:31<15:02,  1.82s/it]\r 14%|        | 81/576 [02:33<14:57,  1.81s/it]\r 14%|        | 82/576 [02:34<14:08,  1.72s/it]\r 14%|        | 83/576 [02:36<13:46,  1.68s/it]\r 15%|        | 84/576 [02:37<13:35,  1.66s/it]\r 15%|        | 85/576 [02:40<14:58,  1.83s/it]\r 15%|        | 86/576 [02:42<15:22,  1.88s/it]\r 15%|        | 87/576 [02:43<14:25,  1.77s/it]\r 15%|        | 88/576 [02:45<14:05,  1.73s/it]\r 15%|        | 89/576 [02:46<13:26,  1.66s/it]\r 16%|        | 90/576 [02:48<13:38,  1.68s/it]\r 16%|        | 91/576 [02:49<12:56,  1.60s/it]\r 16%|        | 92/576 [02:52<14:22,  1.78s/it]\r 16%|        | 93/576 [02:54<15:06,  1.88s/it]\r 16%|        | 94/576 [02:55<14:24,  1.79s/it]\r 16%|        | 95/576 [02:57<14:01,  1.75s/it]\r 17%|        | 96/576 [02:58<13:34,  1.70s/it]\r 17%|        | 97/576 [03:01<14:47,  1.85s/it]\r 17%|        | 98/576 [03:02<14:10,  1.78s/it]\r 17%|        | 99/576 [03:04<14:25,  1.81s/it]\r 17%|        | 100/576 [03:06<14:57,  1.88s/it]\r 18%|        | 101/576 [03:08<14:09,  1.79s/it]\r 18%|        | 102/576 [03:09<13:15,  1.68s/it]\r 18%|        | 103/576 [03:11<13:06,  1.66s/it]\r 18%|        | 104/576 [03:12<12:48,  1.63s/it]\r 18%|        | 105/576 [03:14<12:42,  1.62s/it]\r 18%|        | 106/576 [03:16<12:51,  1.64s/it]\r 19%|        | 107/576 [03:18<14:35,  1.87s/it]\r 19%|        | 108/576 [03:20<13:46,  1.77s/it]\r 19%|        | 109/576 [03:21<13:20,  1.71s/it]\r 19%|        | 110/576 [03:23<12:59,  1.67s/it]\r 19%|        | 111/576 [03:24<12:54,  1.67s/it]\r 19%|        | 112/576 [03:26<12:47,  1.66s/it]\r 20%|        | 113/576 [03:28<12:39,  1.64s/it]\r 20%|        | 114/576 [03:30<14:10,  1.84s/it]\r 20%|        | 115/576 [03:32<13:46,  1.79s/it]\r 20%|        | 116/576 [03:33<12:57,  1.69s/it]\r 20%|        | 117/576 [03:35<12:37,  1.65s/it]\r 20%|        | 118/576 [03:36<12:40,  1.66s/it]\r 21%|        | 119/576 [03:38<12:08,  1.59s/it]\r 21%|        | 120/576 [03:39<11:57,  1.57s/it]\r 21%|        | 121/576 [03:41<12:42,  1.68s/it]\r 21%|        | 122/576 [03:44<14:13,  1.88s/it]\r 21%|       | 123/576 [03:45<13:32,  1.79s/it]\r 22%|       | 124/576 [03:47<13:16,  1.76s/it]\r 22%|       | 125/576 [03:48<12:58,  1.73s/it]\r 22%|       | 126/576 [03:50<12:23,  1.65s/it]\r 22%|       | 127/576 [03:51<11:57,  1.60s/it]\r 22%|       | 128/576 [03:53<11:34,  1.55s/it]\r 22%|       | 129/576 [03:55<13:44,  1.84s/it]\r 23%|       | 130/576 [03:57<13:02,  1.76s/it]\r 23%|       | 131/576 [03:58<12:18,  1.66s/it]\r 23%|       | 132/576 [04:00<12:16,  1.66s/it]\r 23%|       | 133/576 [04:02<12:14,  1.66s/it]\r 23%|       | 134/576 [04:03<11:56,  1.62s/it]\r 23%|       | 135/576 [04:05<11:35,  1.58s/it]\r 24%|       | 136/576 [04:07<12:34,  1.72s/it]\r 24%|       | 137/576 [04:09<13:25,  1.83s/it]\r 24%|       | 138/576 [04:10<12:34,  1.72s/it]\r 24%|       | 139/576 [04:12<11:57,  1.64s/it]\r 24%|       | 140/576 [04:13<11:29,  1.58s/it]\r 24%|       | 141/576 [04:15<11:19,  1.56s/it]\r 25%|       | 142/576 [04:16<11:11,  1.55s/it]\r 25%|       | 143/576 [04:18<10:54,  1.51s/it]\r 25%|       | 144/576 [04:20<13:00,  1.81s/it]\r 25%|       | 145/576 [04:22<12:53,  1.79s/it]\r 25%|       | 146/576 [04:24<12:27,  1.74s/it]\r 26%|       | 147/576 [04:25<11:59,  1.68s/it]\r 26%|       | 148/576 [04:27<11:47,  1.65s/it]\r 26%|       | 149/576 [04:28<11:34,  1.63s/it]\r 26%|       | 150/576 [04:30<12:10,  1.72s/it]\r 26%|       | 151/576 [04:32<13:21,  1.89s/it]\r 26%|       | 152/576 [04:34<13:16,  1.88s/it]\r 27%|       | 153/576 [04:36<12:30,  1.77s/it]\r 27%|       | 154/576 [04:37<12:10,  1.73s/it]\r 27%|       | 155/576 [04:39<11:26,  1.63s/it]\r 27%|       | 156/576 [04:40<11:05,  1.59s/it]\r 27%|       | 157/576 [04:42<11:15,  1.61s/it]\r 27%|       | 158/576 [04:44<12:00,  1.72s/it]\r 28%|       | 159/576 [04:46<13:04,  1.88s/it]\r 28%|       | 160/576 [04:48<12:22,  1.79s/it]\r 28%|       | 161/576 [04:50<12:15,  1.77s/it]\r 28%|       | 162/576 [04:51<11:55,  1.73s/it]\r 28%|       | 163/576 [04:53<12:16,  1.78s/it]\r 28%|       | 164/576 [04:55<12:09,  1.77s/it]\r 29%|       | 165/576 [04:57<12:48,  1.87s/it]\r 29%|       | 166/576 [04:59<13:03,  1.91s/it]\r 29%|       | 167/576 [05:01<12:27,  1.83s/it]\r 29%|       | 168/576 [05:02<11:52,  1.75s/it]\r 29%|       | 169/576 [05:04<11:59,  1.77s/it]\r 30%|       | 170/576 [05:06<11:43,  1.73s/it]\r 30%|       | 171/576 [05:07<11:37,  1.72s/it]\r 30%|       | 172/576 [05:10<12:54,  1.92s/it]\r 30%|       | 173/576 [05:12<13:02,  1.94s/it]\r 30%|       | 174/576 [05:13<12:18,  1.84s/it]\r 30%|       | 175/576 [05:15<11:55,  1.78s/it]\r 31%|       | 176/576 [05:16<11:19,  1.70s/it]\r 31%|       | 177/576 [05:18<11:23,  1.71s/it]\r 31%|       | 178/576 [05:20<10:50,  1.63s/it]\r 31%|       | 179/576 [05:21<11:14,  1.70s/it]\r 31%|      | 180/576 [05:24<12:40,  1.92s/it]\r 31%|      | 181/576 [05:25<12:00,  1.82s/it]\r 32%|      | 182/576 [05:27<11:32,  1.76s/it]\r 32%|      | 183/576 [05:29<11:02,  1.68s/it]\r 32%|      | 184/576 [05:30<11:13,  1.72s/it]\r 32%|      | 185/576 [05:32<11:23,  1.75s/it]\r 32%|      | 186/576 [05:35<12:25,  1.91s/it]\r 32%|      | 187/576 [05:37<13:00,  2.01s/it]\r 33%|      | 188/576 [05:38<12:18,  1.90s/it]\r 33%|      | 189/576 [05:40<12:02,  1.87s/it]\r 33%|      | 190/576 [05:42<11:52,  1.85s/it]\r 33%|      | 191/576 [05:44<11:37,  1.81s/it]\r 33%|      | 192/576 [05:45<11:09,  1.74s/it]\r 34%|      | 193/576 [05:48<13:18,  2.08s/it]\r 34%|      | 194/576 [05:50<12:38,  1.99s/it]\r 34%|      | 195/576 [05:52<12:12,  1.92s/it]\r 34%|      | 196/576 [05:53<11:55,  1.88s/it]\r 34%|      | 197/576 [05:55<11:51,  1.88s/it]\r 34%|      | 198/576 [05:57<11:28,  1.82s/it]\r 35%|      | 199/576 [05:59<12:07,  1.93s/it]\r 35%|      | 200/576 [06:01<12:17,  1.96s/it]\r 35%|      | 201/576 [06:03<11:44,  1.88s/it]\r 35%|      | 202/576 [06:05<11:19,  1.82s/it]\r 35%|      | 203/576 [06:06<11:05,  1.78s/it]\r 35%|      | 204/576 [06:08<11:30,  1.86s/it]\r 36%|      | 205/576 [06:10<11:16,  1.82s/it]\r 36%|      | 206/576 [06:13<12:47,  2.07s/it]\r 36%|      | 207/576 [06:15<12:38,  2.05s/it]\r 36%|      | 208/576 [06:17<12:10,  1.98s/it]\r 36%|      | 209/576 [06:18<11:33,  1.89s/it]\r 36%|      | 210/576 [06:20<11:11,  1.83s/it]\r 37%|      | 211/576 [06:22<11:02,  1.82s/it]\r 37%|      | 212/576 [06:24<11:38,  1.92s/it]\r 37%|      | 213/576 [06:26<12:05,  2.00s/it]\r 37%|      | 214/576 [06:28<11:22,  1.89s/it]\r 37%|      | 215/576 [06:30<11:11,  1.86s/it]\r 38%|      | 216/576 [06:31<10:53,  1.82s/it]\r 38%|      | 217/576 [06:33<10:39,  1.78s/it]\r 38%|      | 218/576 [06:35<10:25,  1.75s/it]\r 38%|      | 219/576 [06:37<11:11,  1.88s/it]\r 38%|      | 220/576 [06:39<11:19,  1.91s/it]\r 38%|      | 221/576 [06:41<11:02,  1.87s/it]\r 39%|      | 222/576 [06:42<10:21,  1.76s/it]\r 39%|      | 223/576 [06:43<09:43,  1.65s/it]\r 39%|      | 224/576 [06:45<09:40,  1.65s/it]\r 39%|      | 225/576 [06:47<09:33,  1.63s/it]\r 39%|      | 226/576 [06:48<09:45,  1.67s/it]\r 39%|      | 227/576 [06:51<10:51,  1.87s/it]\r 40%|      | 228/576 [06:52<10:23,  1.79s/it]\r 40%|      | 229/576 [06:54<10:03,  1.74s/it]\r 40%|      | 230/576 [06:56<09:56,  1.72s/it]\r 40%|      | 231/576 [06:57<09:36,  1.67s/it]\r 40%|      | 232/576 [06:59<09:16,  1.62s/it]\r 40%|      | 233/576 [07:00<09:13,  1.61s/it]\r 41%|      | 234/576 [07:03<10:49,  1.90s/it]\r 41%|      | 235/576 [07:04<10:16,  1.81s/it]\r 41%|      | 236/576 [07:06<10:11,  1.80s/it]\r 41%|      | 237/576 [07:08<09:49,  1.74s/it]\r 41%|     | 238/576 [07:09<09:34,  1.70s/it]\r 41%|     | 239/576 [07:11<09:29,  1.69s/it]\r 42%|     | 240/576 [07:13<09:07,  1.63s/it]\r 42%|     | 241/576 [07:15<10:29,  1.88s/it]\r 42%|     | 242/576 [07:17<10:07,  1.82s/it]\r 42%|     | 243/576 [07:18<09:45,  1.76s/it]\r 42%|     | 244/576 [07:20<09:35,  1.73s/it]\r 43%|     | 245/576 [07:22<09:29,  1.72s/it]\r 43%|     | 246/576 [07:23<09:27,  1.72s/it]\r 43%|     | 247/576 [07:25<09:17,  1.69s/it]\r 43%|     | 248/576 [07:28<10:45,  1.97s/it]\r 43%|     | 249/576 [07:29<10:13,  1.88s/it]\r 43%|     | 250/576 [07:31<09:53,  1.82s/it]\r 44%|     | 251/576 [07:33<09:51,  1.82s/it]\r 44%|     | 252/576 [07:35<09:41,  1.80s/it]\r 44%|     | 253/576 [07:36<09:20,  1.73s/it]\r 44%|     | 254/576 [07:38<09:05,  1.70s/it]\r 44%|     | 255/576 [07:40<10:22,  1.94s/it]\r 44%|     | 256/576 [07:42<09:53,  1.86s/it]\r 45%|     | 257/576 [07:44<09:40,  1.82s/it]\r 45%|     | 258/576 [07:45<09:30,  1.79s/it]\r 45%|     | 259/576 [07:47<09:14,  1.75s/it]\r 45%|     | 260/576 [07:49<09:17,  1.76s/it]\r 45%|     | 261/576 [07:51<09:26,  1.80s/it]\r 45%|     | 262/576 [07:53<10:43,  2.05s/it]\r 46%|     | 263/576 [07:55<10:14,  1.96s/it]\r 46%|     | 264/576 [07:57<09:39,  1.86s/it]\r 46%|     | 265/576 [07:59<09:34,  1.85s/it]\r 46%|     | 266/576 [08:00<09:23,  1.82s/it]\r 46%|     | 267/576 [08:02<09:00,  1.75s/it]\r 47%|     | 268/576 [08:04<09:58,  1.94s/it]\r 47%|     | 269/576 [08:06<10:14,  2.00s/it]\r 47%|     | 270/576 [08:08<09:29,  1.86s/it]\r 47%|     | 271/576 [08:09<08:45,  1.72s/it]\r 47%|     | 272/576 [08:11<08:44,  1.73s/it]\r 47%|     | 273/576 [08:13<08:28,  1.68s/it]\r 48%|     | 274/576 [08:14<08:29,  1.69s/it]\r 48%|     | 275/576 [08:17<09:25,  1.88s/it]\r 48%|     | 276/576 [08:19<09:51,  1.97s/it]\r 48%|     | 277/576 [08:21<09:16,  1.86s/it]\r 48%|     | 278/576 [08:22<09:03,  1.82s/it]\r 48%|     | 279/576 [08:24<08:50,  1.79s/it]\r 49%|     | 280/576 [08:26<08:44,  1.77s/it]\r 49%|     | 281/576 [08:27<08:42,  1.77s/it]\r 49%|     | 282/576 [08:30<09:21,  1.91s/it]\r 49%|     | 283/576 [08:32<09:25,  1.93s/it]\r 49%|     | 284/576 [08:33<08:43,  1.79s/it]\r 49%|     | 285/576 [08:35<08:30,  1.75s/it]\r 50%|     | 286/576 [08:36<08:14,  1.70s/it]\r 50%|     | 287/576 [08:38<08:14,  1.71s/it]\r 50%|     | 288/576 [08:40<08:23,  1.75s/it]\r 50%|     | 289/576 [08:42<09:15,  1.94s/it]\r 50%|     | 290/576 [08:44<08:56,  1.87s/it]\r 51%|     | 291/576 [08:46<08:41,  1.83s/it]\r 51%|     | 292/576 [08:47<08:19,  1.76s/it]\r 51%|     | 293/576 [08:49<07:59,  1.70s/it]\r 51%|     | 294/576 [08:51<07:54,  1.68s/it]\r 51%|     | 295/576 [08:52<08:07,  1.73s/it]\r 51%|    | 296/576 [08:55<09:21,  2.01s/it]\r 52%|    | 297/576 [08:57<08:58,  1.93s/it]\r 52%|    | 298/576 [08:58<08:33,  1.85s/it]\r 52%|    | 299/576 [09:00<08:15,  1.79s/it]\r 52%|    | 300/576 [09:02<08:03,  1.75s/it]\r 52%|    | 301/576 [09:04<08:05,  1.77s/it]\r 52%|    | 302/576 [09:05<07:55,  1.73s/it]\r 53%|    | 303/576 [09:08<09:05,  2.00s/it]\r 53%|    | 304/576 [09:10<08:58,  1.98s/it]\r 53%|    | 305/576 [09:11<08:27,  1.87s/it]\r 53%|    | 306/576 [09:13<08:17,  1.84s/it]\r 53%|    | 307/576 [09:15<08:07,  1.81s/it]\r 53%|    | 308/576 [09:17<07:50,  1.76s/it]\r 54%|    | 309/576 [09:18<07:54,  1.78s/it]\r 54%|    | 310/576 [09:21<09:03,  2.04s/it]\r 54%|    | 311/576 [09:23<08:18,  1.88s/it]\r 54%|    | 312/576 [09:24<07:57,  1.81s/it]\r 54%|    | 313/576 [09:26<07:43,  1.76s/it]\r 55%|    | 314/576 [09:28<07:36,  1.74s/it]\r 55%|    | 315/576 [09:29<07:18,  1.68s/it]\r 55%|    | 316/576 [09:31<07:21,  1.70s/it]\r 55%|    | 317/576 [09:33<08:25,  1.95s/it]\r 55%|    | 318/576 [09:35<08:09,  1.90s/it]\r 55%|    | 319/576 [09:37<07:53,  1.84s/it]\r 56%|    | 320/576 [09:39<07:37,  1.79s/it]\r 56%|    | 321/576 [09:40<07:47,  1.83s/it]\r 56%|    | 322/576 [09:42<07:25,  1.76s/it]\r 56%|    | 323/576 [09:44<08:17,  1.96s/it]\r 56%|    | 324/576 [09:47<08:26,  2.01s/it]\r 56%|    | 325/576 [09:48<08:01,  1.92s/it]\r 57%|    | 326/576 [09:50<07:44,  1.86s/it]\r 57%|    | 327/576 [09:52<07:29,  1.80s/it]\r 57%|    | 328/576 [09:53<07:14,  1.75s/it]\r 57%|    | 329/576 [09:55<07:19,  1.78s/it]\r 57%|    | 330/576 [09:58<08:28,  2.07s/it]\r 57%|    | 331/576 [10:00<08:18,  2.03s/it]\r 58%|    | 332/576 [10:02<08:11,  2.02s/it]\r 58%|    | 333/576 [10:03<07:39,  1.89s/it]\r 58%|    | 334/576 [10:05<07:31,  1.87s/it]\r 58%|    | 335/576 [10:07<07:21,  1.83s/it]\r 58%|    | 336/576 [10:10<08:08,  2.04s/it]\r 59%|    | 337/576 [10:11<07:53,  1.98s/it]\r 59%|    | 338/576 [10:13<07:31,  1.90s/it]\r 59%|    | 339/576 [10:15<07:15,  1.84s/it]\r 59%|    | 340/576 [10:16<07:03,  1.80s/it]\r 59%|    | 341/576 [10:18<06:50,  1.75s/it]\r 59%|    | 342/576 [10:20<06:58,  1.79s/it]\r 60%|    | 343/576 [10:22<07:28,  1.93s/it]\r 60%|    | 344/576 [10:24<07:20,  1.90s/it]\r 60%|    | 345/576 [10:26<07:00,  1.82s/it]\r 60%|    | 346/576 [10:27<06:45,  1.76s/it]\r 60%|    | 347/576 [10:29<06:27,  1.69s/it]\r 60%|    | 348/576 [10:31<06:28,  1.70s/it]\r 61%|    | 349/576 [10:33<06:41,  1.77s/it]\r 61%|    | 350/576 [10:35<07:30,  1.99s/it]\r 61%|    | 351/576 [10:37<07:09,  1.91s/it]\r 61%|    | 352/576 [10:38<06:45,  1.81s/it]\r 61%|   | 353/576 [10:40<06:32,  1.76s/it]\r 61%|   | 354/576 [10:42<06:16,  1.70s/it]\r 62%|   | 355/576 [10:43<06:25,  1.74s/it]\r 62%|   | 356/576 [10:45<06:11,  1.69s/it]\r 62%|   | 357/576 [10:47<06:51,  1.88s/it]\r 62%|   | 358/576 [10:49<06:42,  1.85s/it]\r 62%|   | 359/576 [10:51<06:28,  1.79s/it]\r 62%|   | 360/576 [10:52<06:14,  1.73s/it]\r 63%|   | 361/576 [10:54<06:04,  1.70s/it]\r 63%|   | 362/576 [10:55<05:55,  1.66s/it]\r 63%|   | 363/576 [10:57<05:54,  1.66s/it]\r 63%|   | 364/576 [11:00<06:43,  1.90s/it]\r 63%|   | 365/576 [11:01<06:31,  1.85s/it]\r 64%|   | 366/576 [11:03<06:20,  1.81s/it]\r 64%|   | 367/576 [11:05<06:09,  1.77s/it]\r 64%|   | 368/576 [11:06<06:06,  1.76s/it]\r 64%|   | 369/576 [11:08<05:50,  1.69s/it]\r 64%|   | 370/576 [11:10<05:51,  1.71s/it]\r 64%|   | 371/576 [11:12<06:10,  1.81s/it]\r 65%|   | 372/576 [11:14<06:10,  1.81s/it]\r 65%|   | 373/576 [11:15<05:45,  1.70s/it]\r 65%|   | 374/576 [11:17<05:42,  1.69s/it]\r 65%|   | 375/576 [11:18<05:28,  1.63s/it]\r 65%|   | 376/576 [11:20<05:22,  1.61s/it]\r 65%|   | 377/576 [11:21<05:16,  1.59s/it]\r 66%|   | 378/576 [11:23<05:40,  1.72s/it]\r 66%|   | 379/576 [11:26<06:11,  1.89s/it]\r 66%|   | 380/576 [11:27<05:48,  1.78s/it]\r 66%|   | 381/576 [11:29<05:40,  1.75s/it]\r 66%|   | 382/576 [11:31<05:42,  1.76s/it]\r 66%|   | 383/576 [11:32<05:26,  1.69s/it]\r 67%|   | 384/576 [11:34<05:16,  1.65s/it]\r 67%|   | 385/576 [11:36<05:25,  1.70s/it]\r 67%|   | 386/576 [11:38<06:22,  2.01s/it]\r 67%|   | 387/576 [11:40<06:13,  1.98s/it]\r 67%|   | 388/576 [11:42<05:46,  1.85s/it]\r 68%|   | 389/576 [11:43<05:31,  1.77s/it]\r 68%|   | 390/576 [11:45<05:19,  1.72s/it]\r 68%|   | 391/576 [11:47<05:20,  1.73s/it]\r 68%|   | 392/576 [11:49<05:56,  1.94s/it]\r 68%|   | 393/576 [11:51<06:18,  2.07s/it]\r 68%|   | 394/576 [11:53<06:07,  2.02s/it]\r 69%|   | 395/576 [11:55<05:48,  1.92s/it]\r 69%|   | 396/576 [11:57<05:32,  1.85s/it]\r 69%|   | 397/576 [11:58<05:25,  1.82s/it]\r 69%|   | 398/576 [12:00<05:26,  1.83s/it]\r 69%|   | 399/576 [12:03<06:12,  2.10s/it]\r 69%|   | 400/576 [12:05<05:48,  1.98s/it]\r 70%|   | 401/576 [12:06<05:26,  1.86s/it]\r 70%|   | 402/576 [12:08<05:18,  1.83s/it]\r 70%|   | 403/576 [12:10<05:17,  1.83s/it]\r 70%|   | 404/576 [12:12<05:14,  1.83s/it]\r 70%|   | 405/576 [12:14<05:29,  1.93s/it]\r 70%|   | 406/576 [12:16<05:35,  1.97s/it]\r 71%|   | 407/576 [12:18<05:18,  1.88s/it]\r 71%|   | 408/576 [12:19<05:06,  1.82s/it]\r 71%|   | 409/576 [12:21<05:00,  1.80s/it]\r 71%|   | 410/576 [12:23<04:59,  1.80s/it]\r 71%|  | 411/576 [12:24<04:45,  1.73s/it]\r 72%|  | 412/576 [12:27<05:06,  1.87s/it]\r 72%|  | 413/576 [12:29<05:16,  1.94s/it]\r 72%|  | 414/576 [12:31<05:07,  1.90s/it]\r 72%|  | 415/576 [12:32<04:55,  1.84s/it]\r 72%|  | 416/576 [12:34<04:40,  1.75s/it]\r 72%|  | 417/576 [12:35<04:30,  1.70s/it]\r 73%|  | 418/576 [12:37<04:16,  1.62s/it]\r 73%|  | 419/576 [12:39<04:36,  1.76s/it]\r 73%|  | 420/576 [12:41<04:41,  1.80s/it]\r 73%|  | 421/576 [12:42<04:31,  1.75s/it]\r 73%|  | 422/576 [12:44<04:19,  1.69s/it]\r 73%|  | 423/576 [12:46<04:16,  1.67s/it]\r 74%|  | 424/576 [12:47<04:12,  1.66s/it]\r 74%|  | 425/576 [12:49<04:12,  1.67s/it]\r 74%|  | 426/576 [12:51<04:43,  1.89s/it]\r 74%|  | 427/576 [12:54<04:58,  2.00s/it]\r 74%|  | 428/576 [12:55<04:45,  1.93s/it]\r 74%|  | 429/576 [12:58<04:52,  1.99s/it]\r 75%|  | 430/576 [13:00<04:50,  1.99s/it]\r 75%|  | 431/576 [13:01<04:37,  1.92s/it]\r 75%|  | 432/576 [13:03<04:46,  1.99s/it]\r 75%|  | 433/576 [13:06<05:10,  2.17s/it]\r 75%|  | 434/576 [13:08<05:01,  2.12s/it]\r 76%|  | 435/576 [13:10<04:47,  2.04s/it]\r 76%|  | 436/576 [13:12<04:33,  1.95s/it]\r 76%|  | 437/576 [13:13<04:17,  1.85s/it]\r 76%|  | 438/576 [13:15<04:12,  1.83s/it]\r 76%|  | 439/576 [13:18<04:45,  2.08s/it]\r 76%|  | 440/576 [13:19<04:30,  1.99s/it]\r 77%|  | 441/576 [13:21<04:22,  1.94s/it]\r 77%|  | 442/576 [13:23<04:07,  1.85s/it]\r 77%|  | 443/576 [13:25<04:06,  1.85s/it]\r 77%|  | 444/576 [13:26<03:55,  1.78s/it]\r 77%|  | 445/576 [13:29<04:17,  1.97s/it]\r 77%|  | 446/576 [13:31<04:26,  2.05s/it]\r 78%|  | 447/576 [13:33<04:11,  1.95s/it]\r 78%|  | 448/576 [13:35<04:08,  1.94s/it]\r 78%|  | 449/576 [13:37<04:03,  1.92s/it]\r 78%|  | 450/576 [13:38<03:54,  1.86s/it]\r 78%|  | 451/576 [13:40<03:44,  1.80s/it]\r 78%|  | 452/576 [13:43<04:20,  2.10s/it]\r 79%|  | 453/576 [13:44<04:03,  1.98s/it]\r 79%|  | 454/576 [13:46<03:50,  1.89s/it]\r 79%|  | 455/576 [13:48<03:42,  1.84s/it]\r 79%|  | 456/576 [13:50<03:41,  1.85s/it]\r 79%|  | 457/576 [13:52<03:40,  1.85s/it]\r 80%|  | 458/576 [13:54<03:54,  1.98s/it]\r 80%|  | 459/576 [13:56<04:06,  2.11s/it]\r 80%|  | 460/576 [13:58<04:02,  2.09s/it]\r 80%|  | 461/576 [14:00<03:53,  2.03s/it]\r 80%|  | 462/576 [14:02<03:45,  1.98s/it]\r 80%|  | 463/576 [14:04<03:45,  2.00s/it]\r 81%|  | 464/576 [14:07<04:05,  2.20s/it]\r 81%|  | 465/576 [14:09<03:59,  2.16s/it]\r 81%|  | 466/576 [14:11<03:48,  2.08s/it]\r 81%|  | 467/576 [14:13<03:44,  2.06s/it]\r 81%| | 468/576 [14:15<03:39,  2.03s/it]\r 81%| | 469/576 [14:17<03:31,  1.98s/it]\r 82%| | 470/576 [14:19<03:52,  2.19s/it]\r 82%| | 471/576 [14:21<03:49,  2.18s/it]\r 82%| | 472/576 [14:24<03:52,  2.24s/it]\r 82%| | 473/576 [14:26<03:44,  2.18s/it]\r 82%| | 474/576 [14:28<03:41,  2.18s/it]\r 82%| | 475/576 [14:30<03:31,  2.10s/it]\r 83%| | 476/576 [14:33<03:53,  2.34s/it]\r 83%| | 477/576 [14:35<03:46,  2.28s/it]\r 83%| | 478/576 [14:37<03:38,  2.23s/it]\r 83%| | 479/576 [14:39<03:36,  2.23s/it]\r 83%| | 480/576 [14:41<03:25,  2.14s/it]\r 84%| | 481/576 [14:44<03:34,  2.26s/it]\r 84%| | 482/576 [14:46<03:31,  2.25s/it]\r 84%| | 483/576 [14:48<03:20,  2.16s/it]\r 84%| | 484/576 [14:50<03:08,  2.05s/it]\r 84%| | 485/576 [14:51<02:58,  1.96s/it]\r 84%| | 486/576 [14:53<02:55,  1.95s/it]\r 85%| | 487/576 [14:55<02:49,  1.90s/it]\r 85%| | 488/576 [14:58<03:09,  2.15s/it]\r 85%| | 489/576 [14:59<02:52,  1.98s/it]\r 85%| | 490/576 [15:01<02:51,  2.00s/it]\r 85%| | 491/576 [15:03<02:43,  1.92s/it]\r 85%| | 492/576 [15:05<02:37,  1.87s/it]\r 86%| | 493/576 [15:07<02:33,  1.85s/it]\r 86%| | 494/576 [15:09<02:49,  2.07s/it]\r 86%| | 495/576 [15:11<02:45,  2.05s/it]\r 86%| | 496/576 [15:13<02:38,  1.99s/it]\r 86%| | 497/576 [15:15<02:33,  1.94s/it]\r 86%| | 498/576 [15:17<02:29,  1.92s/it]\r 87%| | 499/576 [15:18<02:18,  1.79s/it]\r 87%| | 500/576 [15:20<02:11,  1.73s/it]\r 87%| | 501/576 [15:23<02:29,  1.99s/it]\r 87%| | 502/576 [15:24<02:24,  1.96s/it]\r 87%| | 503/576 [15:26<02:17,  1.89s/it]\r 88%| | 504/576 [15:28<02:14,  1.87s/it]\r 88%| | 505/576 [15:30<02:10,  1.83s/it]\r 88%| | 506/576 [15:31<02:03,  1.77s/it]\r 88%| | 507/576 [15:33<02:04,  1.80s/it]\r 88%| | 508/576 [15:36<02:14,  1.98s/it]\r 88%| | 509/576 [15:37<02:04,  1.86s/it]\r 89%| | 510/576 [15:39<02:00,  1.82s/it]\r 89%| | 511/576 [15:41<01:55,  1.77s/it]\r 89%| | 512/576 [15:42<01:51,  1.75s/it]\r 89%| | 513/576 [15:44<01:48,  1.71s/it]\r 89%| | 514/576 [15:46<01:49,  1.77s/it]\r 89%| | 515/576 [15:48<01:54,  1.88s/it]\r 90%| | 516/576 [15:50<01:46,  1.78s/it]\r 90%| | 517/576 [15:51<01:44,  1.78s/it]\r 90%| | 518/576 [15:53<01:37,  1.69s/it]\r 90%| | 519/576 [15:54<01:33,  1.65s/it]\r 90%| | 520/576 [15:56<01:30,  1.61s/it]\r 90%| | 521/576 [15:57<01:26,  1.57s/it]\r 91%| | 522/576 [16:00<01:43,  1.91s/it]\r 91%| | 523/576 [16:02<01:35,  1.80s/it]\r 91%| | 524/576 [16:03<01:30,  1.75s/it]\r 91%| | 525/576 [16:05<01:23,  1.64s/it]\r 91%|| 526/576 [16:06<01:18,  1.57s/it]\r 91%|| 527/576 [16:08<01:16,  1.57s/it]\r 92%|| 528/576 [16:09<01:15,  1.58s/it]\r 92%|| 529/576 [16:11<01:21,  1.73s/it]\r 92%|| 530/576 [16:13<01:21,  1.77s/it]\r 92%|| 531/576 [16:15<01:16,  1.71s/it]\r 92%|| 532/576 [16:16<01:13,  1.66s/it]\r 93%|| 533/576 [16:18<01:08,  1.60s/it]\r 93%|| 534/576 [16:19<01:07,  1.60s/it]\r 93%|| 535/576 [16:21<01:01,  1.50s/it]\r 93%|| 536/576 [16:22<00:59,  1.49s/it]\r 93%|| 537/576 [16:24<01:07,  1.72s/it]\r 93%|| 538/576 [16:26<01:04,  1.69s/it]\r 94%|| 539/576 [16:27<00:59,  1.61s/it]\r 94%|| 540/576 [16:29<00:55,  1.53s/it]\r 94%|| 541/576 [16:30<00:51,  1.48s/it]\r 94%|| 542/576 [16:31<00:49,  1.45s/it]\r 94%|| 543/576 [16:33<00:46,  1.41s/it]\r 94%|| 544/576 [16:34<00:44,  1.40s/it]\r 95%|| 545/576 [16:36<00:44,  1.44s/it]\r 95%|| 546/576 [16:38<00:48,  1.61s/it]\r 95%|| 547/576 [16:39<00:43,  1.51s/it]\r 95%|| 548/576 [16:40<00:40,  1.43s/it]\r 95%|| 549/576 [16:42<00:38,  1.42s/it]\r 95%|| 550/576 [16:43<00:36,  1.42s/it]\r 96%|| 551/576 [16:44<00:35,  1.42s/it]\r 96%|| 552/576 [16:46<00:32,  1.35s/it]\r 96%|| 553/576 [16:47<00:30,  1.32s/it]\r 96%|| 554/576 [16:49<00:31,  1.43s/it]\r 96%|| 555/576 [16:50<00:31,  1.50s/it]\r 97%|| 556/576 [16:52<00:30,  1.51s/it]\r 97%|| 557/576 [16:53<00:27,  1.46s/it]\r 97%|| 558/576 [16:54<00:25,  1.41s/it]\r 97%|| 559/576 [16:56<00:26,  1.53s/it]\r 97%|| 560/576 [16:58<00:23,  1.50s/it]\r 97%|| 561/576 [16:59<00:21,  1.43s/it]\r 98%|| 562/576 [17:00<00:19,  1.42s/it]\r 98%|| 563/576 [17:02<00:21,  1.64s/it]\r 98%|| 564/576 [17:04<00:18,  1.56s/it]\r 98%|| 565/576 [17:05<00:16,  1.49s/it]\r 98%|| 566/576 [17:06<00:14,  1.43s/it]\r 98%|| 567/576 [17:08<00:12,  1.40s/it]\r 99%|| 568/576 [17:09<00:10,  1.35s/it]\r 99%|| 569/576 [17:10<00:09,  1.32s/it]\r 99%|| 570/576 [17:12<00:07,  1.31s/it]\r 99%|| 571/576 [17:13<00:07,  1.45s/it]\r 99%|| 572/576 [17:16<00:06,  1.69s/it]\r 99%|| 573/576 [17:17<00:04,  1.60s/it]\r100%|| 574/576 [17:18<00:03,  1.51s/it]\r100%|| 575/576 [17:20<00:01,  1.48s/it]\r100%|| 576/576 [17:20<00:00,  1.07s/it]\r100%|| 576/576 [17:20<00:00,  1.81s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python - <<'PY'\n",
        "import pandas as pd, numpy as np, json, os\n",
        "pd.options.display.width = 160\n",
        "\n",
        "emb_path = \"data/processed/text_embeddings.parquet\"\n",
        "feat_path = \"data/processed/aapl_features.parquet\"\n",
        "\n",
        "def show(msg): print(\"\\n=== \" + msg + \" ===\")\n",
        "\n",
        "# 1) File existence\n",
        "show(\"Files present\")\n",
        "print(\"embeddings exists:\", os.path.exists(emb_path))\n",
        "print(\"features exists:\", os.path.exists(feat_path))\n",
        "\n",
        "if not os.path.exists(emb_path):\n",
        "    raise SystemExit(\"text_embeddings.parquet not found. Run build_text_embeddings.py first.\")\n",
        "if not os.path.exists(feat_path):\n",
        "    raise SystemExit(\"aapl_features.parquet not found. Run features.py first.\")\n",
        "\n",
        "# 2) Load small samples\n",
        "show(\"Load embedding file (head)\")\n",
        "df_emb = pd.read_parquet(emb_path)\n",
        "print(\"rows (dates) in embeddings:\", len(df_emb))\n",
        "print(\"columns:\", list(df_emb.columns))\n",
        "print(df_emb.head(5).to_dict('records')[:5])\n",
        "\n",
        "# 3) Embedding shape & dtype checks\n",
        "show(\"Embedding shape & dtype checks\")\n",
        "# ensure text_emb column exists\n",
        "if 'text_emb' not in df_emb.columns:\n",
        "    print(\"ERROR: no 'text_emb' column found.\")\n",
        "else:\n",
        "    # check element types and length distribution\n",
        "    lens = df_emb['text_emb'].apply(lambda x: len(x) if isinstance(x,(list,tuple,np.ndarray)) else -1)\n",
        "    print(\"embedding vector lengths (unique counts):\")\n",
        "    print(lens.value_counts().to_dict())\n",
        "    # sample one vector numeric stats\n",
        "    sample_idx = lens.idxmax()\n",
        "    vec = df_emb.loc[sample_idx,'text_emb']\n",
        "    if isinstance(vec,(list,tuple,np.ndarray)):\n",
        "        arr = np.array(vec, dtype=float)\n",
        "        print(\"sample vector dtype:\", arr.dtype, \"shape:\", arr.shape)\n",
        "        print(\"sample vector stats: mean {:.6f}, std {:.6f}, min {:.6f}, max {:.6f}\".format(arr.mean(), arr.std(), arr.min(), arr.max()))\n",
        "    else:\n",
        "        print(\"sample vector is not list-like:\", type(vec))\n",
        "\n",
        "# 4) NaN / null checks\n",
        "show(\"Null / NaN checks\")\n",
        "null_text_emb = df_emb['text_emb'].isna().sum()\n",
        "print(\"text_emb null count:\", null_text_emb)\n",
        "# if text_emb present but some entries not lists\n",
        "bad_entries = df_emb['text_emb'].apply(lambda x: not isinstance(x,(list,tuple,np.ndarray)))\n",
        "print(\"non-list text_emb entries:\", int(bad_entries.sum()))\n",
        "\n",
        "# 5) Date coverage vs features\n",
        "show(\"Date coverage alignment with aapl_features\")\n",
        "df_feat = pd.read_parquet(feat_path)\n",
        "df_feat['date'] = pd.to_datetime(df_feat['date']).dt.normalize()\n",
        "df_emb['date'] = pd.to_datetime(df_emb['date']).dt.normalize()\n",
        "dates_feat = set(df_feat['date'].unique())\n",
        "dates_emb = set(df_emb['date'].unique())\n",
        "common = sorted(list(dates_feat & dates_emb))\n",
        "print(\"unique stock dates:\", len(dates_feat))\n",
        "print(\"unique embedding dates:\", len(dates_emb))\n",
        "print(\"overlap dates:\", len(common), \"({:.1%})\".format(len(common)/len(dates_feat)))\n",
        "print(\"first 5 matched dates:\", common[:5])\n",
        "print(\"first 5 stock-only dates (example):\", list(sorted(dates_feat - dates_emb))[:5])\n",
        "print(\"first 5 emb-only dates (example):\", list(sorted(dates_emb - dates_feat))[:5])\n",
        "\n",
        "# 6) Quick sanity: check if embeddings were averaged (i.e., not identical across days)\n",
        "show(\"Sanity: variety across embeddings\")\n",
        "if len(df_emb) >= 2:\n",
        "    # compute pairwise distance sample\n",
        "    n = min(200, len(df_emb))\n",
        "    sample = df_emb['text_emb'].apply(lambda x: np.array(x,dtype=float)).values[:n]\n",
        "    means = [v.mean() for v in sample]\n",
        "    print(\"sample mean of vectors (first 10):\", [float(m) for m in means[:10]])\n",
        "    # compute std of means\n",
        "    print(\"std of vector means across sample:\", float(np.std(means)))\n",
        "else:\n",
        "    print(\"Not enough embedding dates to check variety.\")\n",
        "\n",
        "# 7) If everything looks OK, print one example of embeddings merged with features for a date\n",
        "show(\"Sample merged row (if overlap exists)\")\n",
        "if len(common) > 0:\n",
        "    d = common[0]\n",
        "    emb_row = df_emb[df_emb['date']==d].iloc[0]\n",
        "    feat_row = df_feat[df_feat['date']==d].iloc[0]\n",
        "    print(\"date:\", d)\n",
        "    print(\"feature columns sample:\", {c: float(feat_row[c]) for c in ['ret'] if 'ret' in feat_row})\n",
        "    print(\"embedding sample (first 8 dims):\", np.array(emb_row['text_emb'],dtype=float)[:8].tolist())\n",
        "else:\n",
        "    print(\"No overlapping dates to display merged example.\")\n",
        "\n",
        "print(\"\\\\nDone.\")\n",
        "PY\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmrFHt5cy4p2",
        "outputId": "f83c4269-0064-4b4e-9c1a-b506b16fff8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Files present ===\n",
            "embeddings exists: True\n",
            "features exists: True\n",
            "\n",
            "=== Load embedding file (head) ===\n",
            "rows (dates) in embeddings: 73608\n",
            "columns: ['date', 'text_emb']\n",
            "[{'date': Timestamp('2016-07-01 00:00:00'), 'text_emb': array([-3.27281952e-02,  6.92857727e-02, -7.82452598e-02,  8.92044380e-02,\n",
            "        6.16298765e-02,  1.90350413e-02, -8.91246349e-02,  6.94398955e-03,\n",
            "       -3.76664028e-02,  2.63277236e-02,  6.53022453e-02, -3.22593227e-02,\n",
            "        3.60599421e-02, -1.94225721e-02, -5.39694056e-02,  3.59263346e-02,\n",
            "       -1.24027148e-01, -1.39124002e-02, -1.76910777e-03,  2.78770085e-02,\n",
            "       -3.53500955e-02,  1.74325816e-02,  2.61090859e-03,  3.05023380e-02,\n",
            "        1.50437346e-02, -3.11678220e-02, -3.13687325e-02, -4.34636995e-02,\n",
            "        3.49768177e-02, -3.33533529e-03,  1.76618006e-02,  5.56184128e-02,\n",
            "       -6.74656481e-02, -1.82258561e-02, -1.83785427e-02,  8.04618653e-03,\n",
            "        7.20669329e-02,  6.66547045e-02,  1.88012384e-02, -4.78569837e-03,\n",
            "       -1.39132980e-02, -9.37933922e-02, -3.49856280e-02,  9.58536193e-02,\n",
            "        4.40516621e-02, -6.26409426e-02, -3.21618989e-02,  5.25336601e-02,\n",
            "        1.65332764e-04, -6.53562695e-03, -1.71005763e-02, -6.12066351e-02,\n",
            "        3.58315813e-03, -2.37587448e-02,  4.84590884e-03, -5.77914715e-02,\n",
            "       -2.89729666e-02,  5.60435141e-03, -2.53099315e-02, -5.36518451e-03,\n",
            "        1.81795601e-02,  8.17672610e-02, -1.36881741e-02, -3.06878723e-02,\n",
            "       -5.64581901e-02,  1.22430073e-02,  1.15449116e-01, -7.69965127e-02,\n",
            "        9.13751405e-03, -1.70822386e-02,  1.02887422e-01,  9.18923318e-03,\n",
            "        3.32713500e-03,  1.11917883e-01, -4.90902960e-02,  8.87434091e-03,\n",
            "        6.89878240e-02,  1.85430273e-02, -9.64900702e-02,  1.46401171e-02,\n",
            "       -6.56879786e-03, -6.30499274e-02,  1.86152607e-02,  2.40845792e-02,\n",
            "        4.62419577e-02,  4.72372118e-03, -3.24771553e-02,  2.42029484e-02,\n",
            "       -7.18413740e-02, -5.14152497e-02,  8.46844818e-03,  1.75100658e-02,\n",
            "        1.54583817e-02,  6.75433427e-02,  1.01157054e-01,  1.81111377e-02,\n",
            "        4.24352549e-02,  3.94804925e-02,  3.00030503e-02,  6.55465573e-03,\n",
            "       -2.96271518e-02,  5.11279376e-03,  4.00839485e-02,  7.25867897e-02,\n",
            "        5.71288876e-02,  8.28923360e-02, -8.52679685e-02,  6.30488573e-03,\n",
            "       -3.86301279e-02,  2.70912535e-02,  1.98008083e-02, -2.86139622e-02,\n",
            "       -4.12840731e-02,  3.98536026e-02,  2.60213781e-02,  8.87378529e-02,\n",
            "       -2.63951961e-02,  4.62167338e-02,  1.26010394e-02, -8.90870318e-02,\n",
            "       -2.77698785e-02,  3.25536132e-02, -1.18004784e-01,  1.68788116e-02,\n",
            "        8.79822671e-03, -3.60197239e-02,  8.07016194e-02,  1.54027449e-33,\n",
            "       -5.64047135e-02, -5.38013838e-02, -4.72047217e-02,  6.37518167e-02,\n",
            "        3.32211368e-02,  6.67956751e-03,  2.74927020e-02, -3.49865854e-02,\n",
            "        7.79714296e-03,  2.35485211e-02, -1.53581379e-02, -7.36749843e-02,\n",
            "       -2.11815536e-02, -1.17475651e-01, -3.12264077e-02,  1.24128148e-01,\n",
            "       -9.26285759e-02, -1.49181010e-02, -2.47159358e-02,  5.98479770e-02,\n",
            "        8.34563747e-02, -4.47756052e-02, -1.50711890e-02, -5.43401353e-02,\n",
            "        2.55745389e-02,  1.19298592e-01,  5.42075559e-02, -3.80021445e-02,\n",
            "       -3.80866267e-02, -3.87747586e-02, -6.62083039e-03, -3.25076841e-02,\n",
            "        9.03698131e-02, -5.17555848e-02,  3.14592645e-02,  5.71805518e-03,\n",
            "        7.31707141e-02, -2.25682966e-02, -3.55327912e-02,  4.27188687e-02,\n",
            "       -3.62003222e-02,  3.28042731e-02,  5.60419001e-02,  1.84336249e-02,\n",
            "       -3.75982746e-02, -2.83909775e-02,  5.89651912e-02,  1.91175472e-02,\n",
            "        9.22119170e-02,  1.48129677e-02, -4.50587608e-02, -2.23587360e-02,\n",
            "       -7.17387199e-02, -3.92071269e-02,  5.95069444e-03,  6.38456792e-02,\n",
            "       -4.62051593e-02, -3.39625706e-03,  3.59736907e-04, -6.25518151e-03,\n",
            "        3.45467851e-02, -6.30530268e-02, -2.76988149e-02,  3.78423557e-02,\n",
            "       -2.84100100e-02, -4.98048998e-02,  1.58017501e-02, -3.31313536e-02,\n",
            "        7.93953836e-02, -4.99554351e-02,  3.07907746e-03, -1.31643647e-02,\n",
            "       -6.86460286e-02, -7.72471055e-02,  3.73515580e-03,  1.34239830e-02,\n",
            "        6.35408014e-02, -2.60318033e-02, -2.86093983e-03,  2.99866032e-02,\n",
            "        1.20500274e-01,  6.86491430e-02,  3.13491449e-02,  3.74370553e-02,\n",
            "        7.13704005e-02,  6.64242506e-02,  2.88509158e-03,  3.73695977e-02,\n",
            "       -6.88656196e-02,  9.06304047e-02, -2.95707565e-02,  8.36330503e-02,\n",
            "        6.12998083e-02, -5.21667674e-02,  1.74557362e-02, -3.21332548e-33,\n",
            "       -3.38273384e-02, -8.05348903e-02,  3.55331488e-02,  8.54420885e-02,\n",
            "        2.83797886e-02, -1.29161090e-01, -4.95025925e-02,  4.69749719e-02,\n",
            "       -3.01505588e-02, -6.80494308e-02,  1.20363735e-01, -4.66952138e-02,\n",
            "        3.45015004e-02,  1.32580884e-02, -4.37389640e-03,  2.88783330e-02,\n",
            "       -2.58403886e-02,  7.61748329e-02, -4.99635153e-02,  5.73543794e-02,\n",
            "       -5.32970130e-02,  3.86585705e-02, -1.01340175e-01, -1.73649471e-02,\n",
            "        7.05654398e-02, -1.09260157e-02,  2.67018694e-02, -1.93326976e-02,\n",
            "       -1.88710764e-02, -3.04551311e-02, -3.85628790e-02,  9.83256008e-03,\n",
            "       -9.56109725e-03, -9.34611168e-03, -2.59209238e-02,  2.72570085e-02,\n",
            "       -1.34597840e-02,  4.12226804e-02,  4.69756760e-02,  2.96583120e-03,\n",
            "       -4.56515849e-02,  6.80891052e-02, -6.11980148e-02,  6.11306122e-03,\n",
            "       -2.52894592e-02,  1.18689714e-02,  8.12494829e-02,  4.20512445e-02,\n",
            "        1.04026631e-01, -6.28135949e-02,  1.33231999e-02, -3.07159927e-02,\n",
            "       -1.25024142e-02,  5.63742816e-02,  4.54774797e-02, -3.25214528e-02,\n",
            "       -6.78015314e-03,  2.27181017e-02,  7.32464492e-02, -4.77039814e-02,\n",
            "       -5.34314290e-02,  2.24716123e-02, -3.72819267e-02, -5.19384556e-02,\n",
            "        6.24561450e-03, -1.80159733e-02, -3.91233340e-03, -1.73514839e-02,\n",
            "       -4.82952595e-02,  2.37226207e-02,  1.47237852e-01, -2.20273560e-05,\n",
            "       -1.43778831e-01,  1.04745720e-02, -2.62042023e-02, -3.16374935e-02,\n",
            "       -8.87856930e-02,  7.92135298e-02, -3.00843846e-02, -7.83336721e-03,\n",
            "        5.88886254e-02,  1.80231314e-02, -5.27996384e-02,  1.54876513e-02,\n",
            "        1.03925094e-02,  4.11262037e-03,  1.81181971e-02, -3.92179862e-02,\n",
            "        6.38311133e-02, -8.98541361e-02,  8.86632595e-03, -2.12713163e-02,\n",
            "       -8.21982622e-02, -1.17884338e-01, -1.14609063e-01, -3.19143076e-08,\n",
            "       -3.12563293e-02,  9.55474153e-02, -1.04736656e-01, -6.58422932e-02,\n",
            "       -4.81011383e-02,  7.56862992e-03,  6.46149293e-02,  4.45210822e-02,\n",
            "        2.64730118e-03,  2.93080881e-02, -6.82016015e-02,  6.98585063e-02,\n",
            "        4.41618152e-02, -2.35832445e-02, -2.24761497e-02, -4.80242781e-02,\n",
            "       -1.21965585e-02,  5.88250086e-02,  3.11920960e-02,  8.29439983e-03,\n",
            "        4.90426123e-02, -1.19509939e-02,  3.45100053e-02, -1.43613655e-03,\n",
            "       -2.77369693e-02, -9.39910207e-03,  3.43322270e-02,  7.39931175e-03,\n",
            "        5.88265620e-02, -1.09495319e-01, -7.58035779e-02,  3.85131314e-03,\n",
            "       -2.06849296e-02, -5.90627752e-02,  3.73082832e-02,  5.55631192e-03,\n",
            "        4.38501127e-02, -2.51608342e-03, -8.96049291e-02, -5.49707152e-02,\n",
            "        1.25347143e-02,  5.12639657e-02, -1.98058300e-02, -2.16680602e-03,\n",
            "       -9.42573696e-03, -1.07280567e-01, -3.49309593e-02, -4.82618697e-02,\n",
            "       -4.39002141e-02,  3.75368148e-02,  9.20933858e-02, -5.73458150e-02,\n",
            "        2.59017684e-02, -3.82202491e-02, -9.71433055e-03, -5.23938946e-02,\n",
            "        2.26046541e-03,  6.87464653e-03, -2.54466701e-02,  6.19779713e-02,\n",
            "       -3.80622898e-03, -1.15310811e-02,  1.00598872e-01, -3.02381199e-02])}, {'date': Timestamp('2016-07-01 00:00:00'), 'text_emb': array([-3.16988006e-02,  1.04987040e-01,  5.40172048e-02, -3.19456644e-02,\n",
            "        2.52371673e-02, -7.12645501e-02,  1.10958330e-01, -5.40968776e-02,\n",
            "        1.08832559e-02,  1.12837933e-01, -1.22989304e-01, -4.63768020e-02,\n",
            "       -2.02897005e-02,  9.50480774e-02, -4.57851812e-02,  9.37173516e-03,\n",
            "        1.14867128e-02,  8.66074488e-02, -2.50897072e-02, -6.80652782e-02,\n",
            "       -3.80303413e-02, -6.93790242e-02,  2.80160311e-04,  5.60446419e-02,\n",
            "       -3.12702656e-02, -1.88142620e-02, -4.60375808e-02,  7.64983594e-02,\n",
            "       -2.72808038e-02, -1.88077055e-02, -3.51989977e-02, -8.70092735e-02,\n",
            "        4.24594954e-02,  3.93304601e-02,  8.84758309e-03,  1.15768574e-01,\n",
            "       -1.62223000e-02, -4.31088246e-02, -1.32770389e-01, -1.96838062e-02,\n",
            "        5.39364405e-02,  2.56105457e-02,  4.46418785e-02, -1.02133332e-02,\n",
            "        2.51402240e-02,  1.13422178e-01,  3.99602726e-02, -2.26962045e-02,\n",
            "       -2.69989427e-02,  4.00963165e-02,  4.56905812e-02, -2.07434669e-02,\n",
            "        4.08821777e-02, -7.21174330e-02, -2.07249019e-02,  1.00919336e-01,\n",
            "       -3.17439474e-02, -8.11671838e-03, -4.53870818e-02,  1.08758323e-02,\n",
            "       -1.47740729e-02, -2.86021736e-03, -4.71915752e-02,  4.40533236e-02,\n",
            "       -1.18869450e-02, -7.05729984e-03, -5.59788011e-02,  5.36091700e-02,\n",
            "       -2.38376502e-02, -5.21445200e-02,  7.70677030e-02, -3.70699428e-02,\n",
            "       -3.54308859e-02, -8.13984200e-02,  6.93830177e-02,  5.23133092e-02,\n",
            "        4.99870442e-03, -1.24842236e-02,  2.67450493e-02,  5.83457723e-02,\n",
            "        8.97062048e-02, -9.38741341e-02,  2.54647192e-02, -3.17529552e-02,\n",
            "        6.70004562e-02, -2.11333763e-02,  2.84097288e-02, -4.34868038e-02,\n",
            "       -1.15667889e-02,  4.69540060e-02,  1.76566187e-02, -2.01576296e-03,\n",
            "        1.13108993e-01,  1.45901563e-02,  9.35930014e-02,  4.89669144e-02,\n",
            "       -8.71372372e-02,  5.10585643e-02, -7.08451793e-02,  9.90619361e-02,\n",
            "       -2.09102519e-02,  2.91148890e-02,  7.22298026e-02,  7.52044991e-02,\n",
            "        4.82504629e-03,  2.38130465e-02,  9.75546613e-02, -5.79959489e-02,\n",
            "       -4.33968306e-02,  8.16111490e-02, -6.05594404e-02,  2.70263292e-02,\n",
            "        6.97194710e-02,  5.66024985e-03,  6.30018488e-02,  7.91138336e-02,\n",
            "       -5.39311543e-02,  8.46098438e-02,  6.66569769e-02, -7.57207349e-02,\n",
            "       -5.33650704e-02, -1.56421587e-02, -3.59734483e-02,  5.45774065e-02,\n",
            "       -5.28439507e-02,  1.44202933e-01,  1.86456963e-02,  3.39462932e-34,\n",
            "       -4.48567420e-02,  4.70834188e-02,  9.71365720e-02, -9.54278335e-02,\n",
            "       -4.93677855e-02,  6.41837642e-02,  5.26276939e-02, -5.16051762e-02,\n",
            "        3.54255624e-02, -5.27256392e-02, -5.78800067e-02,  1.95179470e-02,\n",
            "        7.02856556e-02, -5.35403229e-02, -2.99213156e-02, -2.32812688e-02,\n",
            "        7.17070699e-02,  1.89798176e-02,  1.46093313e-02,  7.02031925e-02,\n",
            "        7.60638118e-02,  6.04001991e-03, -7.25543872e-02, -2.87992209e-02,\n",
            "        4.04697768e-02,  5.85356690e-02, -1.84065793e-02, -5.83724864e-03,\n",
            "        4.89441343e-02,  2.98366304e-02, -6.74458966e-02, -3.33622023e-02,\n",
            "       -4.27334644e-02, -1.82107147e-02, -1.51739167e-02,  1.85979158e-02,\n",
            "       -2.04473138e-02, -7.36823445e-03, -4.01619561e-02, -7.04430044e-03,\n",
            "       -4.09141444e-02, -2.64248415e-03, -4.44268510e-02,  1.39733115e-02,\n",
            "       -3.32480762e-03, -3.52017321e-02,  3.17740589e-02,  4.34067845e-02,\n",
            "        1.05148576e-01, -2.22821273e-02, -2.98568327e-02, -1.58394743e-02,\n",
            "       -5.30386902e-02, -1.16782576e-01,  7.40540475e-02,  1.49147222e-02,\n",
            "        2.29622778e-02,  2.33744867e-02,  9.30633489e-03,  1.90364383e-02,\n",
            "       -6.06761836e-02, -1.02117155e-02, -5.52100874e-02,  8.85962546e-02,\n",
            "        5.39266467e-02,  4.22315337e-02, -3.40159461e-02, -4.76741567e-02,\n",
            "       -2.27971990e-02, -4.40327041e-02,  4.53937389e-02, -1.47244502e-02,\n",
            "       -2.17639264e-02, -7.86042213e-03, -7.43332878e-02,  2.61651427e-02,\n",
            "        1.99742652e-02, -5.23276180e-02, -8.58504772e-02,  4.47703972e-02,\n",
            "       -6.11126423e-02, -1.62542760e-02,  1.19695840e-02,  2.76631285e-02,\n",
            "       -8.54865834e-03, -2.63618715e-02,  6.05770871e-02,  4.10284474e-02,\n",
            "        1.07738689e-01,  5.98709099e-02,  2.19536759e-02, -8.62998217e-02,\n",
            "        1.85466111e-02,  7.29306489e-02, -1.37704924e-01, -9.87878748e-34,\n",
            "       -4.94584395e-03,  1.98009685e-02, -8.00134093e-02, -1.61377322e-02,\n",
            "       -1.52881839e-03, -3.05880298e-04,  4.86572832e-02,  5.84060922e-02,\n",
            "        8.32179911e-04, -1.09736277e-02,  2.64489949e-02, -3.27505320e-02,\n",
            "        5.17992228e-02, -6.10421486e-02, -8.19085911e-02,  5.08989207e-02,\n",
            "        3.51717360e-02,  4.45525115e-03, -2.11529415e-02,  1.62726697e-02,\n",
            "       -1.97020378e-02,  4.68762740e-02,  3.32913548e-02, -1.82624080e-03,\n",
            "       -3.48016284e-02, -2.74811164e-02,  2.51334794e-02,  1.68136731e-02,\n",
            "       -8.56948271e-02, -2.97549125e-02, -3.52194011e-02, -3.98785882e-02,\n",
            "       -4.16477509e-02,  1.46285584e-02,  5.32718189e-02,  3.18091698e-02,\n",
            "       -3.45680974e-02, -1.91796012e-02, -1.80464685e-02,  5.35519123e-02,\n",
            "       -3.04370113e-02,  2.11380087e-02, -6.76178513e-03,  1.09447055e-01,\n",
            "        2.80571822e-02,  8.82675871e-02, -8.92895907e-02,  5.67872934e-02,\n",
            "       -3.21153901e-03, -4.94134687e-02, -1.32684678e-01, -5.03459945e-02,\n",
            "       -4.22096364e-02, -1.85044203e-02,  4.81303874e-03,  2.58424375e-02,\n",
            "       -4.61171977e-02,  5.07823788e-02, -5.35953157e-02,  4.43296181e-03,\n",
            "        7.00149313e-02, -1.05882823e-01,  2.15788130e-02, -1.30928140e-02,\n",
            "        3.41654904e-02, -5.41277900e-02, -5.04426844e-03,  2.45476514e-02,\n",
            "       -9.08058733e-02,  6.74326941e-02, -6.12862855e-02,  1.81254081e-03,\n",
            "       -9.11145657e-02,  3.78346555e-02, -3.35317152e-03,  3.01951263e-02,\n",
            "       -1.97922457e-02,  9.85017791e-02,  6.09320439e-02,  8.87656491e-03,\n",
            "       -6.13484047e-02,  3.90255228e-02, -4.85913754e-02, -5.61698079e-02,\n",
            "        6.14618063e-02, -3.38841323e-03,  3.66061889e-02, -4.77611758e-02,\n",
            "        5.96874915e-02,  2.71276627e-02,  6.84627220e-02,  1.19226650e-04,\n",
            "        1.12350024e-02,  1.17068244e-02, -2.59907432e-02, -1.73073236e-08,\n",
            "        5.57189323e-02,  6.42400282e-03, -3.67570631e-02,  5.59596252e-03,\n",
            "       -8.73917043e-02,  7.27054151e-03, -4.51912265e-03, -1.22443236e-01,\n",
            "        1.38790356e-02, -8.27848539e-03, -6.36947677e-02,  3.75267141e-03,\n",
            "        2.55802851e-02,  1.06673250e-02,  3.83397471e-03,  2.37195846e-02,\n",
            "       -6.58590766e-03, -1.02875773e-02, -5.73544856e-03,  1.34756844e-02,\n",
            "       -3.15035470e-02, -1.27545362e-02, -4.59097959e-02, -6.42687753e-02,\n",
            "       -8.57103709e-03, -6.36652391e-03,  3.17610465e-02, -4.07274365e-02,\n",
            "       -2.85473373e-02,  3.54797617e-02, -4.65935804e-02, -3.18898670e-02,\n",
            "       -1.94488391e-02, -4.10310440e-02,  3.25964019e-02,  8.40540826e-02,\n",
            "        1.47649869e-02, -3.49527337e-02, -4.66037281e-02, -6.27248585e-02,\n",
            "       -5.83860055e-02,  5.67674413e-02,  3.65665443e-02, -2.85405200e-02,\n",
            "        7.03493059e-02,  1.91855766e-02,  5.17610162e-02, -9.09909308e-02,\n",
            "        2.34855246e-02, -1.36827743e-02, -5.70319965e-02, -1.23827420e-02,\n",
            "        1.00939041e-02,  2.18471345e-02,  8.55854973e-02, -1.12650543e-02,\n",
            "       -2.50171423e-02, -3.44895720e-02,  2.68011950e-02,  1.76261291e-02,\n",
            "       -1.64883733e-02, -1.85347609e-02, -5.09026572e-02, -3.25117558e-02])}, {'date': Timestamp('2016-07-01 00:00:00'), 'text_emb': array([ 3.36340293e-02, -1.63894277e-02,  6.81505725e-02, -1.41071677e-01,\n",
            "        4.51107398e-02, -3.46402079e-02,  3.75193246e-02, -2.14408766e-02,\n",
            "        1.04883481e-02, -3.19648162e-02, -5.48500381e-02,  2.46245656e-02,\n",
            "        4.18224707e-02,  3.77596309e-03, -3.03883366e-02,  1.48779415e-02,\n",
            "       -2.22983547e-02,  1.98727325e-02, -4.81989905e-02,  4.34991121e-02,\n",
            "        9.94973406e-02,  1.14322519e-02,  1.06318211e-02,  1.74418353e-02,\n",
            "        2.10506804e-02, -3.84669602e-02, -1.09932227e-02,  3.72754447e-02,\n",
            "       -1.22526847e-02,  1.74129475e-02,  7.18097910e-02, -1.68694891e-02,\n",
            "       -9.05192941e-02, -3.37923393e-02, -3.69195789e-02, -7.37239942e-02,\n",
            "       -1.22422976e-02, -1.04289930e-02,  4.45432663e-02, -2.87937932e-02,\n",
            "       -2.77112033e-02, -9.74771008e-02, -3.26511413e-02,  1.17908604e-02,\n",
            "        2.75718961e-02,  8.31499770e-02, -3.06982417e-02,  3.27667184e-02,\n",
            "       -2.04520449e-02,  1.19862985e-02,  1.69429816e-02,  6.74762279e-02,\n",
            "        4.21595387e-02, -9.96126533e-02, -2.90044826e-02,  1.20108705e-02,\n",
            "        2.14341898e-02, -1.61285643e-02,  8.76876432e-03,  3.57111283e-02,\n",
            "        3.08743375e-03, -4.37225588e-03,  3.42677440e-03,  3.80628519e-02,\n",
            "        8.38198289e-02,  9.54875164e-03, -6.29397109e-02, -2.40567047e-02,\n",
            "       -1.23167470e-01,  7.28710443e-02,  1.17740827e-02, -5.56241395e-03,\n",
            "        4.00882736e-02, -9.21587052e-05,  7.53664970e-02,  4.30275165e-02,\n",
            "       -3.70213464e-02,  8.20021629e-02, -1.77370161e-02,  3.49618420e-02,\n",
            "       -9.58630070e-02,  1.48519559e-03,  3.25461663e-02, -7.79216364e-02,\n",
            "        9.79277771e-03, -7.46065611e-03,  7.86467195e-02, -1.26492465e-02,\n",
            "       -4.64069247e-02,  5.45935780e-02, -9.94261280e-02, -9.28288475e-02,\n",
            "       -1.28887249e-02,  1.00353763e-01, -1.36575028e-01, -1.75554473e-02,\n",
            "        3.87312099e-02,  4.96963523e-02, -9.60362852e-02,  2.73067001e-02,\n",
            "       -1.06284976e-01,  8.85579810e-02,  5.64346462e-03,  2.27130409e-02,\n",
            "       -6.95075616e-02,  1.10566253e-02, -2.08955668e-02,  1.14876563e-02,\n",
            "        6.51123151e-02, -4.37598489e-02, -4.16310132e-02, -5.21982312e-02,\n",
            "        6.92002699e-02, -3.80911566e-02,  2.78770421e-02, -3.33530083e-02,\n",
            "       -3.77677158e-02, -9.19970572e-02, -6.73280805e-02, -4.90237996e-02,\n",
            "        2.31184550e-02,  3.42814513e-02, -1.58997737e-02,  5.10312803e-02,\n",
            "       -3.31948809e-02, -2.84447856e-02,  3.45112272e-02, -1.67676775e-33,\n",
            "        1.30647523e-02, -1.17147854e-02,  6.76214546e-02,  2.93136239e-02,\n",
            "       -6.16391525e-02,  4.40974924e-04, -9.52436328e-02, -6.21250411e-03,\n",
            "        5.86303324e-02, -9.35679823e-02, -6.36056289e-02,  1.92735158e-02,\n",
            "        1.34702204e-02,  9.29586217e-03,  2.92368103e-02,  6.07550368e-02,\n",
            "        5.89337908e-02,  5.95645746e-03,  1.93861928e-02, -7.64017180e-02,\n",
            "        6.92470744e-02, -7.91784823e-02,  7.69762546e-02, -2.11238433e-02,\n",
            "        8.89100581e-02, -3.07324175e-02,  3.63755673e-02,  3.23990472e-02,\n",
            "       -1.09109869e-02,  7.09627056e-03, -1.18325718e-01, -3.59197669e-02,\n",
            "       -9.23649780e-03,  5.19401208e-02, -3.29203717e-02,  3.87671366e-02,\n",
            "        1.17941275e-02,  2.97649708e-02,  1.87693425e-02,  3.51867899e-02,\n",
            "        6.34389371e-02, -3.26666757e-02, -7.32019171e-02,  2.13169884e-02,\n",
            "       -2.41675191e-02, -6.45322800e-02, -8.58583115e-03, -4.15132418e-02,\n",
            "        4.50034579e-03, -6.88858889e-03,  5.00686951e-02, -6.90811267e-03,\n",
            "       -9.64487530e-03, -4.83795144e-02,  1.05960913e-01,  8.46513081e-03,\n",
            "       -4.26191017e-02,  4.97322828e-02,  5.27490750e-02, -8.45933929e-02,\n",
            "       -4.85782698e-02,  3.96825783e-02, -4.43793759e-02,  9.91232097e-02,\n",
            "       -7.52552599e-02,  1.11874416e-02, -1.20304041e-02, -2.13877577e-02,\n",
            "       -6.85241669e-02, -3.06709409e-02, -4.02562879e-02,  6.29104152e-02,\n",
            "        4.92326804e-02, -5.87711744e-02, -1.10970093e-02,  2.50708871e-02,\n",
            "       -2.52281372e-02,  1.55612826e-02,  1.09219663e-01,  1.75876811e-03,\n",
            "       -1.86346974e-02, -2.75885817e-02,  3.16053331e-02, -2.47051157e-02,\n",
            "       -8.20340496e-03,  7.34255016e-02,  4.08415776e-03, -2.04417780e-02,\n",
            "        4.48129326e-02, -4.48684432e-02, -9.57629383e-02, -8.58073160e-02,\n",
            "        5.58198430e-02, -3.43651809e-02, -4.91428636e-02, -1.42343963e-33,\n",
            "       -4.26384062e-02,  3.91658582e-02, -1.54996598e-02,  1.97859462e-02,\n",
            "        3.33368890e-02,  7.76718184e-02,  6.31728321e-02,  3.16009484e-03,\n",
            "       -1.33337462e-02, -5.12309838e-03,  8.05429649e-03, -3.62066217e-02,\n",
            "        7.87565336e-02, -1.03979232e-02,  1.36685995e-02, -2.12858599e-02,\n",
            "        9.06927884e-02, -6.88399076e-02,  7.44621037e-04,  8.57532620e-02,\n",
            "       -4.33768407e-02,  2.02047285e-02, -9.86830220e-02,  3.02337594e-02,\n",
            "       -6.97063804e-02, -5.32623241e-03,  5.34976423e-02, -1.91146620e-02,\n",
            "       -6.59375191e-02, -1.25599742e-01, -2.75689666e-03, -7.51445303e-03,\n",
            "       -2.12445669e-02,  5.13959825e-02,  1.01403818e-01,  6.50557643e-03,\n",
            "       -2.21886318e-02, -2.17233859e-02,  1.36803545e-04,  8.12712908e-02,\n",
            "        1.06091285e-02, -6.55813217e-02,  2.07164884e-02, -4.37556161e-03,\n",
            "        4.92306128e-02,  6.29173731e-03,  9.76568670e-04, -7.57206529e-02,\n",
            "        3.85062546e-02,  7.53031373e-02,  5.18537723e-02,  1.32901236e-01,\n",
            "       -5.04352227e-02, -9.96934064e-03, -2.38441192e-02, -2.54623797e-02,\n",
            "       -3.06313951e-02,  7.25298515e-03, -2.83526685e-02,  5.44800311e-02,\n",
            "       -4.86808158e-02, -4.90045501e-03,  1.70198381e-02, -7.37201236e-03,\n",
            "        3.79311554e-02, -2.71357857e-02, -1.86045155e-01,  1.04396299e-01,\n",
            "        9.83963758e-02,  4.79009226e-02,  1.12597972e-01, -4.48861346e-02,\n",
            "       -4.45247516e-02, -1.52609069e-02,  7.16047287e-02, -7.54748425e-03,\n",
            "        1.51671087e-02,  7.56907230e-03, -1.33694559e-02,  2.75545642e-02,\n",
            "       -1.63901355e-02, -2.03809068e-02,  4.27254150e-03, -2.90326457e-02,\n",
            "       -2.30507143e-02, -7.39188045e-02, -6.34647384e-02, -1.28514826e-01,\n",
            "        3.62461396e-02, -6.38388917e-02,  5.30223809e-02, -5.35994675e-03,\n",
            "        3.14712338e-03, -7.10107908e-02,  1.99542567e-02, -1.98308392e-08,\n",
            "        5.98974787e-02,  1.97894871e-02, -5.11933351e-03,  2.09334996e-02,\n",
            "       -2.37429719e-02, -2.10152306e-02, -3.07454318e-02, -9.49396491e-02,\n",
            "        3.22023071e-02, -2.22739577e-02,  2.36535445e-02, -8.30631331e-03,\n",
            "       -8.46433267e-02, -8.82348120e-02, -4.46943678e-02,  1.15297036e-02,\n",
            "        4.14495915e-02, -4.88612503e-02,  2.38216016e-02,  6.07338697e-02,\n",
            "       -2.55346968e-04,  1.89692155e-02,  1.93085875e-02,  2.60100570e-02,\n",
            "        2.93951165e-02, -5.24834506e-02,  1.01837879e-02, -2.32158694e-02,\n",
            "       -2.20384412e-02,  5.40314987e-02,  1.03645050e-03, -4.91625257e-02,\n",
            "       -5.15567437e-02,  2.97502242e-03,  6.99015036e-02, -6.66595623e-02,\n",
            "        1.80170666e-02,  1.03881240e-01,  4.87864539e-02,  2.60320045e-02,\n",
            "        1.93625465e-02,  4.31605503e-02,  3.47635546e-03, -1.24886008e-02,\n",
            "       -5.15960231e-02,  2.52744332e-02,  1.67415887e-02, -6.35448471e-02,\n",
            "        5.64598478e-02,  2.31863223e-02, -7.39060938e-02,  7.39695653e-02,\n",
            "        5.31058796e-02, -2.90825088e-02,  4.89137918e-02,  5.64801879e-02,\n",
            "        5.63552193e-02, -2.65580080e-02, -3.38130891e-02, -4.77772765e-02,\n",
            "       -1.63155864e-03,  2.33569322e-03,  1.38152868e-01,  5.29725887e-02])}, {'date': Timestamp('2016-07-01 00:00:00'), 'text_emb': array([ 4.89868596e-02,  9.35921818e-02, -4.04195413e-02,  5.17741591e-02,\n",
            "        3.15695745e-03,  9.16799158e-02,  8.93122479e-02, -7.95463324e-02,\n",
            "       -5.38692400e-02,  2.31650695e-02, -8.36596452e-03, -3.99840325e-02,\n",
            "       -8.21468383e-02,  3.92640829e-02,  2.92389449e-02, -7.50221089e-02,\n",
            "        6.40600696e-02, -5.24384715e-02,  1.60565004e-02,  8.20919201e-02,\n",
            "        1.18577220e-01,  4.13747765e-02,  8.37754086e-02, -2.47297753e-02,\n",
            "       -5.73364347e-02, -5.49148396e-02,  4.30571772e-02,  3.86516079e-02,\n",
            "       -1.59655735e-02,  5.16017489e-02,  3.50604132e-02, -4.74151969e-03,\n",
            "        3.55329402e-02,  3.14527750e-02, -3.84144150e-02, -4.64448631e-02,\n",
            "        1.17902197e-02, -3.94474678e-02,  2.91829351e-02, -3.66321281e-02,\n",
            "        5.53015023e-02, -1.28297433e-01,  2.70803478e-02,  7.90668055e-02,\n",
            "       -2.70122401e-02,  7.06088915e-02,  2.65393406e-02, -5.19662015e-02,\n",
            "       -5.63188605e-02, -6.84999377e-02,  3.32568921e-02, -8.92127752e-02,\n",
            "        8.06828663e-02,  2.93184314e-02,  1.91988889e-02, -1.04489945e-01,\n",
            "        1.83885302e-02, -3.94026656e-03, -2.47482751e-02,  5.14169224e-03,\n",
            "       -4.28176634e-02,  1.83674041e-02,  9.18846484e-03, -4.38802177e-03,\n",
            "        4.55404818e-02,  2.77874339e-02, -6.56637326e-02,  3.95731553e-02,\n",
            "        8.13973173e-02,  1.00584157e-01,  6.89766854e-02, -1.01301512e-02,\n",
            "       -3.37243862e-02, -4.63055121e-03, -5.06406240e-02,  3.00587062e-03,\n",
            "       -7.27331964e-03,  3.34169120e-02,  1.73544586e-02, -1.10296637e-01,\n",
            "       -3.50172110e-02, -6.87207002e-03, -2.20776722e-03,  4.17020842e-02,\n",
            "       -1.68860592e-02, -6.78556785e-02,  3.70549262e-02, -2.21124403e-02,\n",
            "        4.77577932e-02, -3.75969112e-02, -3.48253697e-02, -6.66223047e-03,\n",
            "       -1.09130843e-02, -3.33887786e-02,  1.47635220e-02, -7.65170949e-03,\n",
            "       -4.62729633e-02, -5.37495278e-02, -2.55062878e-01,  4.89890575e-02,\n",
            "       -6.91378638e-02,  7.05993269e-03, -3.98809426e-02,  4.34048623e-02,\n",
            "        6.65394589e-02, -3.81083935e-02, -5.91115542e-02, -2.69373078e-02,\n",
            "       -5.85879479e-03, -1.82874396e-03,  2.64676437e-02,  5.48996776e-02,\n",
            "        4.98229712e-02,  1.49088437e-02,  3.64066176e-02, -1.71772651e-02,\n",
            "       -6.64450675e-02, -4.82509611e-03,  5.49524464e-02, -1.13312704e-02,\n",
            "        3.69708403e-03, -2.59226318e-02, -9.46837515e-02,  2.56320387e-02,\n",
            "       -2.44054180e-02,  1.42569738e-02,  4.83649075e-02, -1.49911428e-33,\n",
            "        2.14934871e-02, -5.23462296e-02,  2.61662696e-02, -5.32620773e-03,\n",
            "        4.40584384e-02,  6.88187182e-02,  5.79077117e-02, -2.89146584e-02,\n",
            "        4.12657037e-02,  1.35838455e-02,  7.30177760e-02, -1.21929318e-01,\n",
            "        4.13736217e-02,  1.54891880e-02, -4.10974473e-02,  2.38092691e-02,\n",
            "        7.99671039e-02,  2.41798349e-02,  1.93432700e-02,  5.92821091e-03,\n",
            "       -2.03589462e-02,  2.80040689e-03,  4.22393763e-03,  1.97909516e-03,\n",
            "       -4.62913997e-02, -1.22997127e-02, -2.58985674e-03, -4.46793362e-02,\n",
            "        3.90749499e-02,  1.03638265e-02, -3.25577741e-04,  1.18111141e-01,\n",
            "        7.02051222e-02, -5.14373854e-02,  9.51122306e-03,  2.90593673e-02,\n",
            "        1.12327456e-03,  8.19746312e-03, -4.49254997e-02, -8.80058110e-03,\n",
            "        2.39456017e-02,  1.83782894e-02,  1.85358524e-02,  3.62709500e-02,\n",
            "        9.52589326e-03, -2.72635054e-02, -1.47308214e-02, -3.68512757e-02,\n",
            "        1.90274138e-03,  6.32766262e-02,  2.21926421e-02,  2.51837745e-02,\n",
            "       -5.84710110e-03, -4.89037856e-02, -5.21039777e-02, -1.41088376e-02,\n",
            "        1.68530061e-03,  2.29828805e-02, -3.19629945e-02, -4.08856198e-02,\n",
            "        3.04071680e-02, -3.69228423e-02,  9.79127828e-03, -2.41148360e-02,\n",
            "       -4.07090737e-03, -7.68806413e-02,  3.04411333e-02, -9.38079059e-02,\n",
            "       -1.36737689e-01, -2.14475440e-03, -4.69749719e-02,  6.79233670e-02,\n",
            "        1.93101130e-02, -6.09610826e-02, -5.57313412e-02, -3.41559127e-02,\n",
            "       -6.04461739e-03, -1.47407930e-02,  1.76541489e-02, -5.81286587e-02,\n",
            "        5.16704991e-02, -5.46676889e-02,  9.41489115e-02, -4.77205254e-02,\n",
            "       -4.71487083e-02,  6.89530512e-03,  4.51659858e-02,  3.99222560e-02,\n",
            "        4.87107486e-02,  2.27239393e-02, -2.46941485e-02, -1.07333452e-01,\n",
            "        4.77088504e-02,  3.50351743e-02,  2.63387971e-02, -3.08970448e-34,\n",
            "       -9.30713713e-02, -6.27170131e-02, -3.83054502e-02,  2.77221613e-02,\n",
            "        1.29950270e-01, -3.95506471e-02, -3.70268635e-02, -4.77341563e-02,\n",
            "        3.22810188e-03,  9.05147865e-02, -3.58430594e-02, -4.88176532e-02,\n",
            "        7.52372220e-02,  4.11163792e-02,  8.64837393e-02,  4.16761711e-02,\n",
            "        3.35607380e-02, -4.15246561e-02, -4.49511595e-03,  2.84928605e-02,\n",
            "        1.34634553e-02,  2.78782006e-03,  2.77159251e-02, -6.59432542e-03,\n",
            "       -4.10998501e-02,  4.03271951e-02,  1.12772383e-01,  5.57461940e-02,\n",
            "       -9.41749811e-02, -2.81087775e-02, -5.11630662e-02,  7.38589540e-02,\n",
            "       -4.21441980e-02, -6.72004698e-03, -3.38704027e-02, -2.28034239e-02,\n",
            "       -7.14217871e-03,  9.52955708e-03,  3.60246375e-02,  1.11232907e-01,\n",
            "       -1.03739388e-02,  6.54688701e-02, -3.04851606e-02, -1.79471709e-02,\n",
            "       -5.25579080e-02, -1.33281527e-02,  5.55394106e-02,  3.61794718e-02,\n",
            "       -4.36000638e-02,  4.35936600e-02,  1.26256226e-02,  2.38757245e-02,\n",
            "       -3.16372514e-02, -2.53814757e-02, -3.11830286e-02, -8.26160312e-02,\n",
            "       -7.77922571e-02, -6.80201501e-02, -6.84661046e-02,  4.97686416e-02,\n",
            "        3.80848423e-02,  1.58242024e-02,  2.94894800e-02,  3.84079367e-02,\n",
            "       -1.31374784e-03,  3.75652569e-03, -7.11784661e-02, -2.12754309e-02,\n",
            "        8.96213278e-02,  1.10549880e-02,  3.03854756e-02, -1.12190112e-01,\n",
            "        6.62277192e-02, -2.50443332e-02, -3.39456387e-02, -9.05109495e-02,\n",
            "        5.14021739e-02,  2.38461420e-02, -1.91337764e-02, -1.06012709e-01,\n",
            "        4.76481095e-02, -1.12350218e-01,  6.79807141e-02,  4.50423732e-02,\n",
            "       -3.18382494e-02,  9.53890756e-02,  2.01409124e-02,  2.88060810e-02,\n",
            "       -1.22288743e-03,  4.52214815e-02, -1.56410113e-02, -1.82042494e-02,\n",
            "       -8.41914210e-03,  1.35214478e-02,  7.01465979e-02, -3.13729700e-08,\n",
            "        1.07033104e-02, -4.84220535e-02, -3.78733799e-02, -4.80471104e-02,\n",
            "        9.31963772e-02,  4.55376226e-03, -5.72276972e-02, -1.00896604e-01,\n",
            "        7.34882951e-02,  1.78386047e-02, -3.66020075e-04,  4.53919433e-02,\n",
            "        6.76804781e-02, -7.51115233e-02, -5.83332442e-02, -5.88715822e-02,\n",
            "        2.84416229e-02, -4.50164452e-02, -1.16696969e-01,  3.97176929e-02,\n",
            "       -1.83354709e-02, -6.28498523e-03, -3.02277915e-02, -1.66484434e-02,\n",
            "        2.55884677e-02,  9.02680308e-02,  6.99374974e-02,  2.59406548e-02,\n",
            "        4.62534316e-02,  7.90818706e-02,  3.18215741e-03,  3.04607991e-02,\n",
            "       -2.57411953e-02,  1.59708466e-02, -2.22229566e-02,  1.46807292e-02,\n",
            "        3.90843041e-02,  3.95897627e-02,  5.14730029e-02, -5.63731641e-02,\n",
            "       -1.04940496e-02, -4.96111140e-02,  2.61197928e-02,  4.79800366e-02,\n",
            "       -8.72922037e-03,  1.66336913e-02, -8.71463567e-02,  3.46066132e-02,\n",
            "        2.56784726e-02,  1.31198503e-02, -9.10945311e-02,  3.65708917e-02,\n",
            "        7.04028606e-02,  6.78506345e-02,  3.88657413e-02, -1.82409436e-02,\n",
            "        6.46516262e-03, -1.66273154e-02, -9.08719376e-03, -3.21710408e-02,\n",
            "        1.35757737e-02, -1.01742707e-01, -1.83619838e-02,  8.95372871e-03])}, {'date': Timestamp('2016-07-01 00:00:00'), 'text_emb': array([-1.88603904e-02,  3.34954038e-02,  6.87934458e-02, -7.19748763e-03,\n",
            "        1.24452591e-01,  2.66675018e-02,  1.82418488e-02,  7.71970823e-02,\n",
            "       -2.76356712e-02,  6.58336952e-02, -2.95038112e-02,  5.10199219e-02,\n",
            "       -7.83396065e-02, -1.67189725e-02, -6.81865886e-02,  1.13896199e-01,\n",
            "       -4.73318882e-02, -8.36213876e-05, -1.45647600e-02, -5.09958379e-02,\n",
            "       -9.29054990e-03,  1.20299282e-02,  5.70934489e-02,  4.15554233e-02,\n",
            "       -5.15017211e-02,  3.39983590e-03, -2.19377372e-02, -5.40704876e-02,\n",
            "       -3.85763533e-02,  2.76182368e-02, -1.47763109e-02,  6.13287743e-03,\n",
            "        1.56378113e-02,  8.58172681e-03, -2.87853368e-02,  5.39484173e-02,\n",
            "       -8.54114722e-03, -7.84805324e-03, -5.71176223e-02, -3.56048793e-02,\n",
            "        4.54838797e-02, -7.58749470e-02, -4.76507731e-02, -5.00001945e-02,\n",
            "        2.54152231e-02, -2.86276154e-02,  3.89068126e-04, -2.50879186e-03,\n",
            "       -6.01603370e-03, -6.05807379e-02,  2.10598987e-02, -1.01655081e-01,\n",
            "       -5.07371537e-02, -4.66312729e-02,  2.36412138e-02, -3.16142873e-03,\n",
            "        1.62529387e-02, -1.77587066e-02, -5.77953318e-03, -1.50519712e-02,\n",
            "       -1.19354371e-02, -6.90968856e-02, -2.92726178e-02, -1.21219857e-02,\n",
            "        4.68071438e-02,  1.60936430e-01,  2.05778074e-03,  2.04541869e-02,\n",
            "       -5.34671471e-02,  8.05358309e-03,  8.29610303e-02, -2.69620437e-02,\n",
            "        4.56654690e-02, -4.13379353e-03,  3.78512144e-02,  3.61038148e-02,\n",
            "        2.30250545e-02,  7.45895579e-02,  1.29762083e-01, -2.46197991e-02,\n",
            "        6.24177568e-02, -6.48524426e-03, -1.97146628e-02, -6.20904863e-02,\n",
            "       -3.66837345e-02, -2.27001458e-02,  3.13955173e-03, -1.59784425e-02,\n",
            "        9.72420424e-02,  2.06514634e-02, -4.52600941e-02, -4.73985672e-02,\n",
            "        8.39784667e-02,  2.78059244e-02, -4.79476377e-02,  5.14921658e-02,\n",
            "       -2.26683505e-02, -1.00731090e-01, -2.01202389e-02, -2.15860158e-02,\n",
            "        2.32676994e-02,  5.27722612e-02, -3.25716324e-02, -4.02526632e-02,\n",
            "       -3.37061025e-02,  8.13183840e-03, -5.22191674e-02, -7.66580226e-04,\n",
            "        8.93974826e-02,  5.15038073e-02,  9.68789868e-03,  5.73578291e-02,\n",
            "        7.01969787e-02,  5.42264059e-02, -4.26310562e-02,  2.47078836e-02,\n",
            "        7.61139914e-02, -1.30156884e-02,  2.93767415e-02, -9.63057205e-02,\n",
            "       -1.73795708e-02, -2.37582773e-02, -3.89824025e-02, -4.90497015e-02,\n",
            "       -2.30400991e-02,  5.04403301e-02, -9.31667015e-02,  5.37739706e-35,\n",
            "       -6.05042465e-02, -4.31205146e-03,  1.18780985e-01, -3.48031707e-02,\n",
            "        2.72152722e-02,  1.05566448e-02, -2.96389703e-02,  1.69442571e-03,\n",
            "        1.27544738e-02, -5.59067428e-02, -7.16527030e-02, -2.43695620e-02,\n",
            "        1.88380592e-02, -5.45392279e-03,  1.90806063e-03, -3.61540057e-02,\n",
            "       -6.95677325e-02,  2.65904125e-02,  2.54371688e-02, -3.09573989e-02,\n",
            "       -1.35336975e-02, -3.70067768e-02, -3.33547555e-02,  5.96478693e-02,\n",
            "        9.00275633e-02,  3.64443436e-02, -3.28197377e-03, -9.40020836e-04,\n",
            "        6.06838390e-02,  1.88631155e-02,  1.76435392e-02,  1.97577178e-02,\n",
            "       -2.80202944e-02,  2.77259704e-02,  4.63160127e-02, -1.84094757e-02,\n",
            "       -6.32224977e-02, -2.19523273e-02, -5.83741581e-03,  3.45123149e-02,\n",
            "       -2.63590980e-02,  5.83788082e-02, -2.23997161e-02, -4.69959155e-02,\n",
            "        1.15976043e-01,  1.45166386e-02,  5.42706549e-02, -4.58898842e-02,\n",
            "        5.29735815e-04,  2.97350269e-02, -7.30782375e-02,  1.19473845e-01,\n",
            "       -3.45335305e-02, -2.06629913e-02,  8.49452391e-02, -5.20833172e-02,\n",
            "       -3.62238958e-02,  3.86491194e-02, -1.54324034e-02, -1.21736489e-01,\n",
            "       -3.59262899e-02,  1.34787947e-01, -3.97213362e-02, -2.41260161e-03,\n",
            "       -4.79714572e-02,  5.18562570e-02, -1.06669016e-01, -1.22627979e-02,\n",
            "       -8.66634846e-02,  1.12349644e-01,  8.69486779e-02, -6.68355748e-02,\n",
            "       -1.08012795e-01, -5.33438623e-02, -6.43597320e-02, -6.57132193e-02,\n",
            "       -1.91076733e-02,  4.64880876e-02,  1.34238442e-02,  4.11004350e-02,\n",
            "        8.53607729e-02,  4.86975946e-02,  3.36682075e-03, -2.52210088e-02,\n",
            "       -2.27858666e-02, -2.55778655e-02, -6.54311059e-03,  1.10539533e-02,\n",
            "        4.78132069e-02,  9.00036748e-03, -1.28996512e-02,  3.06165451e-03,\n",
            "        2.15545297e-02,  5.66336736e-02, -1.47417605e-01, -1.20415107e-33,\n",
            "       -9.41072125e-03, -4.29431163e-02,  6.59215748e-02,  2.53765583e-02,\n",
            "        5.55286109e-02, -4.13186699e-02, -8.05102736e-02, -7.12357312e-02,\n",
            "        8.53484496e-02,  3.14194076e-02,  8.53667632e-02,  5.27181961e-02,\n",
            "        3.55114490e-02,  1.97289102e-02,  8.91401991e-03, -3.61910127e-02,\n",
            "        7.94209354e-03,  3.58620845e-02, -1.21301219e-01, -4.45142724e-02,\n",
            "        1.25238011e-02,  4.18083072e-02,  4.40939888e-03,  1.11044057e-01,\n",
            "       -3.10898889e-02, -8.18903558e-03, -2.55055521e-02,  6.82645962e-02,\n",
            "        4.90447357e-02, -6.82642236e-02, -2.79516280e-02,  8.60601813e-02,\n",
            "       -1.04359210e-01,  3.04955579e-02,  2.49972567e-02,  3.93609963e-02,\n",
            "       -5.31484336e-02,  1.00018848e-02,  3.00556626e-02,  4.07901295e-02,\n",
            "        4.00287472e-02,  1.69576388e-02, -6.96206018e-02,  3.72145092e-04,\n",
            "       -1.55137144e-02,  2.09383313e-02, -4.24742177e-02, -2.65597980e-02,\n",
            "        1.11650024e-02, -6.33918308e-03, -3.01813092e-02,  8.16029236e-02,\n",
            "       -3.43380943e-02, -1.61972344e-02,  1.09079806e-03, -6.61872700e-02,\n",
            "       -1.25937667e-02,  8.97833891e-03, -1.77671127e-02, -1.67116635e-02,\n",
            "       -8.34767299e-04, -1.43282283e-02,  1.69921778e-02,  3.54275741e-02,\n",
            "        2.20654551e-02, -6.18506484e-02,  4.37348746e-02,  6.36766776e-02,\n",
            "        3.34529430e-02, -5.95821738e-02,  4.51712199e-02,  3.36438827e-02,\n",
            "       -2.65425090e-02, -4.01635654e-02, -3.66717651e-02, -8.50774813e-03,\n",
            "       -7.31126890e-02, -1.42367622e-02, -4.84350249e-02, -5.12189902e-02,\n",
            "        6.77461997e-02, -3.18637304e-02,  7.80648962e-02, -2.71874527e-03,\n",
            "        7.44473329e-03,  1.33485291e-02, -3.45715657e-02,  1.77370086e-02,\n",
            "       -2.13490818e-02,  1.34399846e-01, -1.12467967e-02, -9.56644937e-02,\n",
            "        1.72187686e-02,  8.39420334e-02,  5.80599606e-02, -1.90077571e-08,\n",
            "        5.56968600e-02, -1.39543619e-02, -2.69176927e-03, -1.89401628e-03,\n",
            "       -7.60141388e-03, -1.48373712e-02, -6.11236058e-02,  6.11221641e-02,\n",
            "       -3.48792821e-02,  3.31252515e-02,  2.62938607e-02,  1.23119280e-01,\n",
            "       -2.00961437e-02,  1.49696857e-01, -2.61689536e-02, -1.86074320e-02,\n",
            "       -3.55178304e-02, -9.30219889e-03,  4.80840215e-03,  7.35342829e-03,\n",
            "        1.27054518e-02,  1.30299050e-02, -8.62516388e-02, -3.56863551e-02,\n",
            "        4.32925224e-02, -1.28294500e-02,  7.97777027e-02, -4.19520959e-02,\n",
            "       -6.27557281e-03,  3.00091449e-02, -4.27080616e-02, -1.80754270e-02,\n",
            "       -6.56463057e-02,  5.58347292e-02, -1.31621240e-02, -3.42061967e-02,\n",
            "       -2.40406971e-02,  5.64767160e-02, -1.78408511e-02,  6.55512419e-03,\n",
            "       -2.50057746e-02,  1.13737531e-01, -2.14570947e-02,  5.16956672e-02,\n",
            "       -6.99923411e-02, -7.13278651e-02, -2.71476470e-02,  6.81074932e-02,\n",
            "       -1.08121224e-02,  6.83158189e-02,  5.92454076e-02, -6.86311275e-02,\n",
            "        2.11050902e-02, -5.91949336e-02,  1.15705794e-02, -8.41439962e-02,\n",
            "        5.92256198e-03, -1.59337893e-02, -9.77719873e-02, -4.50903811e-02,\n",
            "        4.24975194e-02, -1.27333879e-01,  4.67155203e-02,  4.54838062e-03])}]\n",
            "\n",
            "=== Embedding shape & dtype checks ===\n",
            "embedding vector lengths (unique counts):\n",
            "{384: 73608}\n",
            "sample vector dtype: float64 shape: (384,)\n",
            "sample vector stats: mean -0.000021, std 0.051031, min -0.143779, max 0.147238\n",
            "\n",
            "=== Null / NaN checks ===\n",
            "text_emb null count: 0\n",
            "non-list text_emb entries: 0\n",
            "\n",
            "=== Date coverage alignment with aapl_features ===\n",
            "unique stock dates: 2202\n",
            "unique embedding dates: 2943\n",
            "overlap dates: 2032 (92.3%)\n",
            "first 5 matched dates: [Timestamp('2008-06-09 00:00:00'), Timestamp('2008-06-10 00:00:00'), Timestamp('2008-06-11 00:00:00'), Timestamp('2008-06-12 00:00:00'), Timestamp('2008-06-13 00:00:00')]\n",
            "first 5 stock-only dates (example): [Timestamp('2008-02-01 00:00:00'), Timestamp('2008-02-04 00:00:00'), Timestamp('2008-02-05 00:00:00'), Timestamp('2008-02-06 00:00:00'), Timestamp('2008-02-07 00:00:00')]\n",
            "first 5 emb-only dates (example): [Timestamp('2008-06-08 00:00:00'), Timestamp('2008-06-14 00:00:00'), Timestamp('2008-06-15 00:00:00'), Timestamp('2008-06-21 00:00:00'), Timestamp('2008-06-22 00:00:00')]\n",
            "\n",
            "=== Sanity: variety across embeddings ===\n",
            "sample mean of vectors (first 10): [-2.093386154947141e-05, 0.0004988277470620908, -0.0006760395642218439, 0.00034575642394224504, 0.0002238198686198937, -0.0005642870531207314, 0.0016400004659128387, -0.00014466060847498352, -0.0010956593548199407, -1.959088009410396e-07]\n",
            "std of vector means across sample: 0.0006014995579984578\n",
            "\n",
            "=== Sample merged row (if overlap exists) ===\n",
            "date: 2008-06-09 00:00:00\n",
            "feature columns sample: {'ret': -0.021948153946094395}\n",
            "embedding sample (first 8 dims): [-0.016874436289072037, -0.04225020483136177, -0.03897373378276825, 0.02955367974936962, 0.028775179758667946, 0.08181743323802948, -0.0015296884812414646, -0.08492889255285263]\n",
            "\\nDone.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# code for early fusion\n",
        "\n",
        "%%writefile src/train_multimodal_lstm.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_multimodal_lstm.py\n",
        "\n",
        "Updated version with:\n",
        " - robust embedding alignment + aggregation\n",
        " - overlap statistics printing\n",
        " - emb_fill modes: zero, ffill, hybrid (ffill up to N then zero)\n",
        " - robust target detection / computation\n",
        " - fixed shape handling for sequences (no incorrect reshape)\n",
        " - debug prints\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "import json\n",
        "from typing import Tuple, Optional, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# PyTorch (if not needed, you can change to any other framework)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# -----------------------\n",
        "# Helpers\n",
        "# -----------------------\n",
        "COMMON_TARGET_NAMES = [\"target\", \"y\", \"ret\", \"returns\", \"target_return\", \"next_return\"]\n",
        "COMMON_DATE_COLS = [\"date\", \"created_utc\", \"timestamp\", \"created\", \"publish_date\", \"published_at\", \"time\"]\n",
        "\n",
        "\n",
        "def detect_date_col(df: pd.DataFrame, prefer: Optional[str] = None) -> Optional[str]:\n",
        "    if prefer and prefer in df.columns:\n",
        "        return prefer\n",
        "    for c in COMMON_DATE_COLS:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    for c in df.columns:\n",
        "        if pd.api.types.is_datetime64_any_dtype(df[c]):\n",
        "            return c\n",
        "    for c in df.columns:\n",
        "        if pd.api.types.is_integer_dtype(df[c]) or pd.api.types.is_float_dtype(df[c]):\n",
        "            series = df[c].dropna()\n",
        "            if len(series) == 0:\n",
        "                continue\n",
        "            v = series.iloc[0]\n",
        "            if isinstance(v, (int, float)) and (1e9 < abs(v) < 1e12):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "\n",
        "def parse_dates_column(series: pd.Series, unit: str = \"s\") -> pd.Series:\n",
        "    if pd.api.types.is_datetime64_any_dtype(series):\n",
        "        return pd.to_datetime(series).dt.normalize()\n",
        "    if pd.api.types.is_integer_dtype(series) or pd.api.types.is_float_dtype(series):\n",
        "        return pd.to_datetime(series, unit=unit, errors=\"coerce\").dt.normalize()\n",
        "    return pd.to_datetime(series, errors=\"coerce\").dt.normalize()\n",
        "\n",
        "\n",
        "def align_embeddings_to_trade_dates(\n",
        "    df_num: pd.DataFrame,\n",
        "    df_text: pd.DataFrame,\n",
        "    emb_agg: str = \"mean\",\n",
        "    emb_fill: str = \"zero\",\n",
        "    emb_ffill_limit: int = 0,\n",
        "    debug: bool = False,\n",
        ") -> Tuple[pd.DataFrame, int]:\n",
        "    \"\"\"\n",
        "    Align text embeddings to the dates present in df_num (trading dates).\n",
        "    Returns (df_out, emb_dim) where df_out has columns ['date','text_emb'] (text_emb is list of floats).\n",
        "    emb_fill: 'zero' | 'ffill' | 'hybrid' (hybrid == ffill up to emb_ffill_limit then zeros)\n",
        "    emb_agg: aggregation over multiple text items per date: mean|sum|max\n",
        "    \"\"\"\n",
        "    df_num = df_num.copy()\n",
        "    df_num[\"date\"] = pd.to_datetime(df_num[\"date\"]).dt.normalize()\n",
        "    df_text = df_text.copy()\n",
        "    if \"date\" in df_text.columns:\n",
        "        df_text[\"date\"] = pd.to_datetime(df_text[\"date\"]).dt.normalize()\n",
        "    else:\n",
        "        # ensure there's at least a date column (maybe named differently)\n",
        "        date_col = detect_date_col(df_text)\n",
        "        if date_col:\n",
        "            df_text[\"date\"] = pd.to_datetime(df_text[date_col]).dt.normalize()\n",
        "        else:\n",
        "            df_text[\"date\"] = pd.NaT\n",
        "\n",
        "    if \"text_emb\" not in df_text.columns:\n",
        "        raise ValueError(\"text embeddings dataframe must contain 'text_emb' column\")\n",
        "\n",
        "    # Convert list/array to numpy arrays and drop invalids\n",
        "    def to_arr(x):\n",
        "        try:\n",
        "            if isinstance(x, (list, tuple, np.ndarray)):\n",
        "                arr = np.asarray(x, dtype=float)\n",
        "                return arr\n",
        "            # sometimes stored as string representation\n",
        "            if isinstance(x, str):\n",
        "                # try to eval safely: fallback - not ideal but often necessary\n",
        "                import ast\n",
        "\n",
        "                v = ast.literal_eval(x)\n",
        "                arr = np.asarray(v, dtype=float)\n",
        "                return arr\n",
        "        except Exception:\n",
        "            return None\n",
        "        return None\n",
        "\n",
        "    df_text[\"_emb_arr\"] = df_text[\"text_emb\"].apply(to_arr)\n",
        "    df_text = df_text[~df_text[\"_emb_arr\"].isna()].copy()\n",
        "    if df_text.shape[0] == 0:\n",
        "        raise ValueError(\"No valid embeddings found in text embeddings dataframe (column 'text_emb')\")\n",
        "\n",
        "    emb_dim = df_text[\"_emb_arr\"].iloc[0].shape[0]\n",
        "    # keep only rows with matching embedding dim\n",
        "    df_text = df_text[df_text[\"_emb_arr\"].apply(lambda x: x.shape[0] == emb_dim)].copy()\n",
        "\n",
        "    # Build DataFrame with one column per embedding dimension indexed by date\n",
        "    emb_cols = [f\"emb_{i}\" for i in range(emb_dim)]\n",
        "    emb_df = pd.DataFrame(df_text[\"_emb_arr\"].tolist(), columns=emb_cols, index=df_text[\"date\"])\n",
        "    emb_df.index.name = \"date\"\n",
        "\n",
        "    # aggregate multiple items per date\n",
        "    if emb_agg == \"mean\":\n",
        "        emb_daily = emb_df.groupby(emb_df.index).mean()\n",
        "    elif emb_agg == \"sum\":\n",
        "        emb_daily = emb_df.groupby(emb_df.index).sum()\n",
        "    elif emb_agg == \"max\":\n",
        "        emb_daily = emb_df.groupby(emb_df.index).max()\n",
        "    else:\n",
        "        emb_daily = emb_df.groupby(emb_df.index).mean()\n",
        "\n",
        "    # alignment index (we align to stock feature dates)\n",
        "    stock_dates = pd.to_datetime(df_num[\"date\"].unique()).normalize()\n",
        "    emb_index = pd.DatetimeIndex(sorted(stock_dates))\n",
        "    emb_daily = emb_daily.reindex(emb_index)  # missing dates -> NaNs\n",
        "\n",
        "    # Overlap stats\n",
        "    unique_stock_dates = len(stock_dates)\n",
        "    unique_emb_dates = emb_daily.dropna(how=\"all\").shape[0]\n",
        "    overlap = len(set(stock_dates).intersection(set(emb_daily.dropna(how=\"all\").index)))\n",
        "    if debug:\n",
        "        print(\"=== Embedding/Feature date overlap statistics ===\")\n",
        "        print(\"Unique stock feature dates:\", unique_stock_dates)\n",
        "        print(\"Unique embedding dates:\", unique_emb_dates)\n",
        "        print(f\"Overlap (stock  emb): {overlap} ({overlap/unique_stock_dates*100:.2f}% of stock dates)\")\n",
        "        stock_dt_sorted = sorted(stock_dates)\n",
        "        emb_dt_sorted = sorted(emb_daily.dropna(how=\"all\").index)\n",
        "        print(\"Example overlap dates (first 10):\", list(sorted(set(stock_dt_sorted).intersection(set(emb_dt_sorted))))[:10])\n",
        "        print(\"Example stock-only dates (first 10):\", stock_dt_sorted[:10])\n",
        "        print(\"Example emb-only dates (first 10):\", emb_dt_sorted[:10])\n",
        "        print(\"emb_fill:\", emb_fill, \"emb_ffill_limit:\", emb_ffill_limit)\n",
        "        print(\"===============================================\")\n",
        "\n",
        "    # Apply filling strategy\n",
        "    if emb_fill == \"zero\":\n",
        "        emb_filled = emb_daily.fillna(0.0)\n",
        "    elif emb_fill == \"ffill\":\n",
        "        emb_filled = emb_daily.ffill(limit=emb_ffill_limit)\n",
        "    elif emb_fill in (\"hybrid\", \"ffill_then_zero\"):\n",
        "        emb_filled = emb_daily.ffill(limit=emb_ffill_limit).fillna(0.0)\n",
        "    else:\n",
        "        emb_filled = emb_daily.fillna(0.0)\n",
        "\n",
        "    # Convert back to list column in the same order as df_num's dates\n",
        "    df_dates_order = pd.DataFrame({\"date\": pd.to_datetime(df_num[\"date\"]).dt.normalize()})\n",
        "    # Use emb_filled rows by df_dates_order order (for index alignment)\n",
        "    # If some dates are outside emb index, reindex will produce NaNs -> fill as per above logic\n",
        "    emb_for_dates = emb_filled.reindex(df_dates_order[\"date\"].values).fillna(0.0)\n",
        "    emb_list = emb_for_dates.values.tolist()\n",
        "    df_out = df_dates_order.copy()\n",
        "    df_out[\"text_emb\"] = emb_list\n",
        "    return df_out, emb_dim\n",
        "\n",
        "\n",
        "def detect_or_compute_target(df: pd.DataFrame, target_col_arg: Optional[str], debug: bool = False) -> Tuple[pd.DataFrame, str]:\n",
        "    \"\"\"\n",
        "    Ensure df has a 'target' column. If none present and price column exists, compute next-day pct change.\n",
        "    Returns (df, target_col_name)\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    if target_col_arg and target_col_arg in df.columns:\n",
        "        if debug:\n",
        "            print(f\"Using provided target column: {target_col_arg}\")\n",
        "        return df, target_col_arg\n",
        "\n",
        "    for c in COMMON_TARGET_NAMES:\n",
        "        if c in df.columns:\n",
        "            if debug:\n",
        "                print(f\"Found existing target column: {c}\")\n",
        "            return df, c\n",
        "\n",
        "    # try to compute from price columns\n",
        "    price_col = None\n",
        "    if \"adj_close\" in df.columns:\n",
        "        price_col = \"adj_close\"\n",
        "    elif \"close\" in df.columns:\n",
        "        price_col = \"close\"\n",
        "    elif \"Close\" in df.columns:\n",
        "        price_col = \"Close\"\n",
        "\n",
        "    if price_col:\n",
        "        if debug:\n",
        "            print(f\"Computing target as next-day pct change of price col '{price_col}'\")\n",
        "        df = df.sort_values(\"date\").copy()\n",
        "        df[\"target\"] = df[price_col].pct_change().shift(-1)  # next-day return\n",
        "        return df, \"target\"\n",
        "\n",
        "    raise ValueError(\"No 'target' column found in features parquet. Provide --target_col or include a price column like 'close' or 'adj_close' to compute one.\")\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Simple LSTM model\n",
        "# -----------------------\n",
        "class SimpleLSTM(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int = 64, num_layers: int = 1, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, input_size)\n",
        "        out, (hn, cn) = self.lstm(x)  # out: (batch, seq_len, hidden_size)\n",
        "        # use last timestep\n",
        "        last = out[:, -1, :]\n",
        "        y = self.head(last)\n",
        "        return y.squeeze(-1)\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Sequence creation\n",
        "# -----------------------\n",
        "def create_sequences(features_df: pd.DataFrame, seq_len: int, feature_cols: List[str], target_col: str) -> Tuple[np.ndarray, np.ndarray, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Build sequences from a dataframe sorted by date.\n",
        "    Returns (X, y, df_sorted) where:\n",
        "     - X shape = (n_samples, seq_len, n_features)\n",
        "     - y shape = (n_samples,)\n",
        "    \"\"\"\n",
        "    df = features_df.sort_values(\"date\").reset_index(drop=True).copy()\n",
        "    vals = df[feature_cols].values.astype(float)\n",
        "    targets = df[target_col].values.astype(float)\n",
        "\n",
        "    n = len(df)\n",
        "    seqs = []\n",
        "    ys = []\n",
        "    indices = []\n",
        "    for i in range(n - seq_len):\n",
        "        seq = vals[i : i + seq_len]\n",
        "        tgt = targets[i + seq_len]  # predict next day's target\n",
        "        if np.isnan(tgt):\n",
        "            continue\n",
        "        seqs.append(seq)\n",
        "        ys.append(tgt)\n",
        "        indices.append(i)\n",
        "    if len(seqs) == 0:\n",
        "        return np.zeros((0, seq_len, len(feature_cols))), np.zeros((0,)), df\n",
        "    X = np.stack(seqs, axis=0)\n",
        "    y = np.array(ys, dtype=float)\n",
        "    return X, y, df\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Main\n",
        "# -----------------------\n",
        "def main():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--features\", required=True, help=\"Features parquet with 'date' and feature columns\")\n",
        "    p.add_argument(\"--text_emb\", required=True, help=\"Text embeddings parquet with 'date' and 'text_emb' (list) columns\")\n",
        "    p.add_argument(\"--seq_len\", type=int, default=10)\n",
        "    p.add_argument(\"--emb_agg\", default=\"mean\", choices=[\"mean\", \"sum\", \"max\"])\n",
        "    p.add_argument(\"--emb_fill\", default=\"zero\", choices=[\"zero\", \"ffill\", \"hybrid\"])\n",
        "    p.add_argument(\"--emb_ffill_limit\", type=int, default=0, help=\"Days to forward-fill embeddings for ffill/hybrid\")\n",
        "    p.add_argument(\"--target_col\", default=None, help=\"Name of target column if present\")\n",
        "    p.add_argument(\"--epochs\", type=int, default=10)\n",
        "    p.add_argument(\"--batch_size\", type=int, default=64)\n",
        "    p.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    p.add_argument(\"--device\", default=\"cpu\", help=\"cpu or cuda\")\n",
        "    p.add_argument(\"--out\", default=\"outputs/multimodal_lstm_preds.json\")\n",
        "    p.add_argument(\"--debug\", action=\"store_true\")\n",
        "    p.add_argument(\"--flatten_for_classical\", action=\"store_true\", help=\"If set, flatten sequences into single vectors (seq_len * features) for classical ML models. If not set, keep (batch,seq_len,features) for LSTM.\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    device = torch.device(args.device if torch.cuda.is_available() and args.device != \"cpu\" else \"cpu\")\n",
        "    if args.debug:\n",
        "        print(\"Using device:\", device)\n",
        "\n",
        "    # Load features & embeddings\n",
        "    features_path = Path(args.features)\n",
        "    text_emb_path = Path(args.text_emb)\n",
        "    if not features_path.exists():\n",
        "        raise FileNotFoundError(args.features)\n",
        "    if not text_emb_path.exists():\n",
        "        raise FileNotFoundError(args.text_emb)\n",
        "\n",
        "    if args.debug:\n",
        "        print(\"Loading features from:\", features_path)\n",
        "    df_features = pd.read_parquet(features_path)\n",
        "    if args.debug:\n",
        "        print(\"Loading embeddings from:\", text_emb_path)\n",
        "    df_text = pd.read_parquet(text_emb_path)\n",
        "\n",
        "    # Ensure date exists in features\n",
        "    if \"date\" not in df_features.columns:\n",
        "        date_col = detect_date_col(df_features)\n",
        "        if date_col:\n",
        "            df_features[\"date\"] = pd.to_datetime(df_features[date_col]).dt.normalize()\n",
        "        else:\n",
        "            raise ValueError(\"Features parquet must contain a 'date' column or a detectable datetime column.\")\n",
        "\n",
        "    # Ensure text date exists\n",
        "    if \"date\" not in df_text.columns:\n",
        "        date_col = detect_date_col(df_text)\n",
        "        if date_col:\n",
        "            df_text[\"date\"] = pd.to_datetime(df_text[date_col]).dt.normalize()\n",
        "        else:\n",
        "            df_text[\"date\"] = pd.NaT\n",
        "\n",
        "    # Align embeddings -> df_aligned has text_emb for every stock date in df_features order\n",
        "    if args.debug:\n",
        "        print(\"Aligning/aggregating embeddings to trading dates...\")\n",
        "    df_text_aligned, emb_dim = align_embeddings_to_trade_dates(\n",
        "        df_num=df_features,\n",
        "        df_text=df_text,\n",
        "        emb_agg=args.emb_agg,\n",
        "        emb_fill=args.emb_fill,\n",
        "        emb_ffill_limit=args.emb_ffill_limit,\n",
        "        debug=args.debug,\n",
        "    )\n",
        "    if args.debug:\n",
        "        print(f\"Detected embedding dimension: {emb_dim}\")\n",
        "\n",
        "    # Merge aligned embeddings into df_features (keep df_features order)\n",
        "    df_features = df_features.sort_values(\"date\").reset_index(drop=True)\n",
        "    df_text_aligned = df_text_aligned.reset_index(drop=True)\n",
        "    if len(df_text_aligned) != len(df_features):\n",
        "        # safer align by date merge on 'date' preserving df_features order\n",
        "        df_merged = pd.merge(df_features, df_text_aligned, on=\"date\", how=\"left\", sort=False)\n",
        "    else:\n",
        "        df_merged = pd.concat([df_features.reset_index(drop=True), df_text_aligned[\"text_emb\"].reset_index(drop=True)], axis=1)\n",
        "\n",
        "    # If some text_emb are still NaN (shouldn't after filling), convert to zeros\n",
        "    if df_merged[\"text_emb\"].isna().any():\n",
        "        if args.debug:\n",
        "            print(\"Warning: some text_emb are NaN after alignment; filling with zeros.\")\n",
        "        df_merged[\"text_emb\"] = df_merged[\"text_emb\"].apply(lambda x: [0.0] * emb_dim if (pd.isna(x) or x is None) else x)\n",
        "\n",
        "    # Detect or compute target\n",
        "    df_merged, target_col = detect_or_compute_target(df_merged, args.target_col, debug=args.debug)\n",
        "\n",
        "    # Build feature columns: numeric columns in features + embedding dimensions expanded as separate columns\n",
        "    non_feature_cols = set([\"date\", target_col, \"text_emb\"])\n",
        "    candidate_feature_cols = [c for c in df_merged.columns if c not in non_feature_cols and np.issubdtype(df_merged[c].dtype, np.number)]\n",
        "    # Expand embedding dims into separate columns emb_0, emb_1, ...\n",
        "    emb_cols = [f\"emb_{i}\" for i in range(emb_dim)]\n",
        "    emb_vals = np.vstack(df_merged[\"text_emb\"].apply(lambda x: np.asarray(x, dtype=float)).values)\n",
        "    emb_df = pd.DataFrame(emb_vals, columns=emb_cols, index=df_merged.index)\n",
        "    df_features_expanded = pd.concat([df_merged.reset_index(drop=True), emb_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    feature_cols = candidate_feature_cols + emb_cols\n",
        "    if args.debug:\n",
        "        print(\"Feature columns used (sample):\", feature_cols[:10], \"... total:\", len(feature_cols))\n",
        "\n",
        "    # Create sequences\n",
        "    X, y, df_sorted = create_sequences(df_features_expanded[[\"date\"] + feature_cols + [target_col]], seq_len=args.seq_len, feature_cols=feature_cols, target_col=target_col)\n",
        "    if args.debug:\n",
        "        print(\"Created sequences:\")\n",
        "        print(\"X shape:\", X.shape)\n",
        "        print(\"y shape:\", y.shape)\n",
        "\n",
        "    if X.shape[0] == 0:\n",
        "        raise ValueError(\"No sequences created (maybe too short dataset relative to seq_len or all NaN targets).\")\n",
        "\n",
        "    # Split train/test simple chronological split (80/20)\n",
        "    n_samples = X.shape[0]\n",
        "    train_n = int(n_samples * 0.8)\n",
        "    X_train = X[:train_n]\n",
        "    y_train = y[:train_n]\n",
        "    X_test = X[train_n:]\n",
        "    y_test = y[train_n:]\n",
        "\n",
        "    # Optionally flatten for classical models (user requested earlier)\n",
        "    if args.flatten_for_classical:\n",
        "        # each sample -> vector of length seq_len * n_features\n",
        "        n_samples_train, seq_len_local, n_feats_ts = X_train.shape\n",
        "        X_train_flat = X_train.reshape(n_samples_train, seq_len_local * n_feats_ts)\n",
        "        X_test_flat = X_test.reshape(X_test.shape[0], seq_len_local * n_feats_ts)\n",
        "        # create data loaders from flat arrays\n",
        "        train_dataset = TensorDataset(torch.from_numpy(X_train_flat).float(), torch.from_numpy(y_train).float())\n",
        "        test_dataset = TensorDataset(torch.from_numpy(X_test_flat).float(), torch.from_numpy(y_test).float())\n",
        "        # For simplicity we won't train an LSTM if flatten_for_classical is True;\n",
        "        # user should plug in classical model. We'll run a trivial baseline (ridge-like) here if desired.\n",
        "        if args.debug:\n",
        "            print(\"flatten_for_classical requested: returning flattened arrays for classical model training.\")\n",
        "        # Save predictions placeholder: just predict mean of train targets on test\n",
        "        preds = [float(np.mean(y_train))] * len(y_test)\n",
        "        out_dict = {\"preds\": preds, \"y_test\": y_test.tolist(), \"method\": \"baseline_mean_due_to_flatten_flag\"}\n",
        "        Path(args.out).parent.mkdir(parents=True, exist_ok=True)\n",
        "        with open(args.out, \"w\") as f:\n",
        "            json.dump(out_dict, f)\n",
        "        print(\"Wrote fallback predictions to\", args.out)\n",
        "        return\n",
        "\n",
        "    # Training with PyTorch LSTM\n",
        "    input_size = X_train.shape[2]\n",
        "    model = SimpleLSTM(input_size=input_size, hidden_size=64, num_layers=1, dropout=0.0).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    train_ds = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())\n",
        "    test_ds = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float())\n",
        "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "    if args.debug:\n",
        "        print(\"Starting training: epochs:\", args.epochs, \"train_samples:\", len(train_ds), \"test_samples:\", len(test_ds))\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(args.epochs):\n",
        "        total_loss = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(xb)\n",
        "            loss = criterion(out, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * xb.size(0)\n",
        "        avg_loss = total_loss / len(train_ds)\n",
        "        if args.debug:\n",
        "            print(f\"[Epoch {epoch+1}/{args.epochs}] train loss: {avg_loss:.6f}\")\n",
        "\n",
        "    # Evaluation: predict on test set\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    trues = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            xb = xb.to(device)\n",
        "            out = model(xb)\n",
        "            preds.extend(out.cpu().numpy().tolist())\n",
        "            trues.extend(yb.numpy().tolist())\n",
        "\n",
        "    preds = np.array(preds)\n",
        "    trues = np.array(trues)\n",
        "\n",
        "    # helper to compute metrics for a pair of arrays\n",
        "    def compute_metrics(a_true: np.ndarray, a_pred: np.ndarray):\n",
        "        mae_v = np.mean(np.abs(a_pred - a_true))\n",
        "        rmse_v = np.sqrt(np.mean((a_pred - a_true) ** 2))\n",
        "        if len(a_true) >= 2:\n",
        "            da_v = 100.0 * np.mean(np.sign(a_pred) == np.sign(a_true))\n",
        "        else:\n",
        "            da_v = 0.0\n",
        "        return mae_v, rmse_v, da_v\n",
        "\n",
        "    # point estimates\n",
        "    mae, rmse, da = compute_metrics(trues, preds)\n",
        "\n",
        "    # bootstrap confidence intervals (95%) for MAE, RMSE, DA\n",
        "    n_test_pts = len(trues)\n",
        "    n_boot = 1000\n",
        "    seed = 42\n",
        "    if n_test_pts >= 2:\n",
        "        rng = np.random.default_rng(seed)\n",
        "        mae_bs = np.empty(n_boot, dtype=float)\n",
        "        rmse_bs = np.empty(n_boot, dtype=float)\n",
        "        da_bs = np.empty(n_boot, dtype=float)\n",
        "        for i in range(n_boot):\n",
        "            idx = rng.integers(0, n_test_pts, size=n_test_pts)  # with-replacement indices\n",
        "            t_sample = trues[idx]\n",
        "            p_sample = preds[idx]\n",
        "            m, r, d = compute_metrics(t_sample, p_sample)\n",
        "            mae_bs[i] = m\n",
        "            rmse_bs[i] = r\n",
        "            da_bs[i] = d\n",
        "        # 2.5th and 97.5th percentiles\n",
        "        mae_lo, mae_hi = np.percentile(mae_bs, [2.5, 97.5]).tolist()\n",
        "        rmse_lo, rmse_hi = np.percentile(rmse_bs, [2.5, 97.5]).tolist()\n",
        "        da_lo, da_hi = np.percentile(da_bs, [2.5, 97.5]).tolist()\n",
        "    else:\n",
        "        # not enough points to bootstrap reliably\n",
        "        mae_lo = mae_hi = float(mae)\n",
        "        rmse_lo = rmse_hi = float(rmse)\n",
        "        da_lo = da_hi = float(da)\n",
        "\n",
        "    out_metrics = {\n",
        "        \"MAE_mean\": float(mae),\n",
        "        \"MAE_lo\": float(mae_lo),\n",
        "        \"MAE_hi\": float(mae_hi),\n",
        "        \"RMSE_mean\": float(rmse),\n",
        "        \"RMSE_lo\": float(rmse_lo),\n",
        "        \"RMSE_hi\": float(rmse_hi),\n",
        "        \"DA_mean\": float(da),\n",
        "        \"DA_lo\": float(da_lo),\n",
        "        \"DA_hi\": float(da_hi),\n",
        "        \"n_test\": int(n_test_pts),\n",
        "        \"bootstrap_samples\": int(n_boot) if n_test_pts >= 2 else 0,\n",
        "        \"bootstrap_seed\": int(seed) if n_test_pts >= 2 else None,\n",
        "    }\n",
        "\n",
        "    Path(args.out).parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(args.out, \"w\") as f:\n",
        "        json.dump({\"preds\": preds.tolist(), \"y_test\": trues.tolist(), \"metrics\": out_metrics}, f)\n",
        "\n",
        "    print(\"Saved predictions+metrics to\", args.out)\n",
        "    if args.debug:\n",
        "        print(\"Metrics:\", json.dumps(out_metrics, indent=2))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaVy_li_QEK7",
        "outputId": "6690311d-d306-4b29-be2f-19b54b655632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/train_multimodal_lstm.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#updated\n",
        "\n",
        "%%bash\n",
        "python src/train_multimodal_lstm.py \\\n",
        "  --features data/processed/aapl_features.parquet \\\n",
        "  --text_emb data/processed/text_embeddings.parquet \\\n",
        "  --seq_len 10 --emb_agg mean --emb_fill zero \\\n",
        "  --epochs 30 --batch_size 64 --device cpu --debug \\\n",
        "  --out outputs/multimodal_lstm_preds.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vO2qYw4p_Ceo",
        "outputId": "66161bae-9c4d-4c8f-cac4-422f0e51783e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Loading features from: data/processed/aapl_features.parquet\n",
            "Loading embeddings from: data/processed/text_embeddings.parquet\n",
            "Aligning/aggregating embeddings to trading dates...\n",
            "=== Embedding/Feature date overlap statistics ===\n",
            "Unique stock feature dates: 2202\n",
            "Unique embedding dates: 2032\n",
            "Overlap (stock  emb): 2032 (92.28% of stock dates)\n",
            "Example overlap dates (first 10): [Timestamp('2008-06-09 00:00:00'), Timestamp('2008-06-10 00:00:00'), Timestamp('2008-06-11 00:00:00'), Timestamp('2008-06-12 00:00:00'), Timestamp('2008-06-13 00:00:00'), Timestamp('2008-06-16 00:00:00'), Timestamp('2008-06-17 00:00:00'), Timestamp('2008-06-18 00:00:00'), Timestamp('2008-06-19 00:00:00'), Timestamp('2008-06-20 00:00:00')]\n",
            "Example stock-only dates (first 10): [Timestamp('2008-02-01 00:00:00'), Timestamp('2008-02-04 00:00:00'), Timestamp('2008-02-05 00:00:00'), Timestamp('2008-02-06 00:00:00'), Timestamp('2008-02-07 00:00:00'), Timestamp('2008-02-08 00:00:00'), Timestamp('2008-02-11 00:00:00'), Timestamp('2008-02-12 00:00:00'), Timestamp('2008-02-13 00:00:00'), Timestamp('2008-02-14 00:00:00')]\n",
            "Example emb-only dates (first 10): [Timestamp('2008-06-09 00:00:00'), Timestamp('2008-06-10 00:00:00'), Timestamp('2008-06-11 00:00:00'), Timestamp('2008-06-12 00:00:00'), Timestamp('2008-06-13 00:00:00'), Timestamp('2008-06-16 00:00:00'), Timestamp('2008-06-17 00:00:00'), Timestamp('2008-06-18 00:00:00'), Timestamp('2008-06-19 00:00:00'), Timestamp('2008-06-20 00:00:00')]\n",
            "emb_fill: zero emb_ffill_limit: 0\n",
            "===============================================\n",
            "Detected embedding dimension: 384\n",
            "Found existing target column: ret\n",
            "Feature columns used (sample): ['Close', 'High', 'Low', 'Open', 'Volume', 'close_next', 'ret_next', 'ret_lag_1', 'ret_lag_2', 'ret_lag_3'] ... total: 399\n",
            "Created sequences:\n",
            "X shape: (2192, 10, 399)\n",
            "y shape: (2192,)\n",
            "Starting training: epochs: 30 train_samples: 1753 test_samples: 439\n",
            "[Epoch 1/30] train loss: 0.038650\n",
            "[Epoch 2/30] train loss: 0.000687\n",
            "[Epoch 3/30] train loss: 0.000493\n",
            "[Epoch 4/30] train loss: 0.000449\n",
            "[Epoch 5/30] train loss: 0.000448\n",
            "[Epoch 6/30] train loss: 0.000447\n",
            "[Epoch 7/30] train loss: 0.000448\n",
            "[Epoch 8/30] train loss: 0.000448\n",
            "[Epoch 9/30] train loss: 0.000448\n",
            "[Epoch 10/30] train loss: 0.000449\n",
            "[Epoch 11/30] train loss: 0.000449\n",
            "[Epoch 12/30] train loss: 0.000449\n",
            "[Epoch 13/30] train loss: 0.000448\n",
            "[Epoch 14/30] train loss: 0.000448\n",
            "[Epoch 15/30] train loss: 0.000448\n",
            "[Epoch 16/30] train loss: 0.000448\n",
            "[Epoch 17/30] train loss: 0.000449\n",
            "[Epoch 18/30] train loss: 0.000449\n",
            "[Epoch 19/30] train loss: 0.000448\n",
            "[Epoch 20/30] train loss: 0.000452\n",
            "[Epoch 21/30] train loss: 0.000450\n",
            "[Epoch 22/30] train loss: 0.000448\n",
            "[Epoch 23/30] train loss: 0.000448\n",
            "[Epoch 24/30] train loss: 0.000448\n",
            "[Epoch 25/30] train loss: 0.000449\n",
            "[Epoch 26/30] train loss: 0.000448\n",
            "[Epoch 27/30] train loss: 0.000448\n",
            "[Epoch 28/30] train loss: 0.000449\n",
            "[Epoch 29/30] train loss: 0.000450\n",
            "[Epoch 30/30] train loss: 0.000448\n",
            "Saved predictions+metrics to outputs/multimodal_lstm_preds.json\n",
            "Metrics: {\n",
            "  \"MAE_mean\": 0.01146069980269019,\n",
            "  \"MAE_lo\": 0.010475470832575919,\n",
            "  \"MAE_hi\": 0.012432509572439345,\n",
            "  \"RMSE_mean\": 0.015826077597370515,\n",
            "  \"RMSE_lo\": 0.014273181754080868,\n",
            "  \"RMSE_hi\": 0.01741469094826735,\n",
            "  \"DA_mean\": 49.65831435079727,\n",
            "  \"DA_lo\": 44.86902050113895,\n",
            "  \"DA_hi\": 54.21412300683372,\n",
            "  \"n_test\": 439,\n",
            "  \"bootstrap_samples\": 1000,\n",
            "  \"bootstrap_seed\": 42\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/eval2.py\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "eval.py\n",
        "\n",
        "Flexible evaluator for prediction JSON outputs. Supports multiple input formats:\n",
        "\n",
        "1) New training script format:\n",
        "   {\n",
        "     \"preds\": [..],\n",
        "     \"y_test\": [..],\n",
        "     \"metrics\": { optional precomputed metrics }\n",
        "   }\n",
        "\n",
        "2) Older \"per_fold\" format:\n",
        "   { \"per_fold\": [ { \"preds\": [...], \"y_test\": [...] }, ... ] }\n",
        "\n",
        "3) A plain list of fold records:\n",
        "   [ { \"preds\": [...], \"y_test\": [...] }, ... ]\n",
        "\n",
        "This script computes:\n",
        " - MAE, RMSE, Directional Accuracy (DA)\n",
        " - 95% CI via bootstrap (default 1000 samples)\n",
        " - For multiple folds, returns mean + 2.5/97.5 percentiles across folds.\n",
        "\n",
        "Writes JSON to --out path.\n",
        "\"\"\"\n",
        "import argparse\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------\n",
        "# Metrics / utils\n",
        "# ---------------------\n",
        "def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
        "    mae = float(np.mean(np.abs(y_pred - y_true)))\n",
        "    rmse = float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
        "    # directional accuracy: equality of sign (zero treated as sign 0)\n",
        "    sign_equal = (np.sign(y_pred) == np.sign(y_true))\n",
        "    da = 100.0 * float(np.mean(sign_equal))\n",
        "    return {\"MAE\": mae, \"RMSE\": rmse, \"DA\": da}\n",
        "\n",
        "\n",
        "def bootstrap_sample_metrics(y_true: np.ndarray, y_pred: np.ndarray, n_boot: int = 1000, seed: int = 42) -> Dict[str, Tuple[float, float, float]]:\n",
        "    \"\"\"\n",
        "    For a single fold: sample indices with replacement and compute bootstrap distribution\n",
        "    Returns dict: key -> (mean, lo, hi)\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(seed)\n",
        "    n = len(y_true)\n",
        "    if n == 0:\n",
        "        raise ValueError(\"Empty test set for bootstrap.\")\n",
        "    stats = {\"MAE\": [], \"RMSE\": [], \"DA\": []}\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.randint(0, n, size=n)\n",
        "        yt = y_true[idx]\n",
        "        yp = y_pred[idx]\n",
        "        m = compute_metrics(yt, yp)\n",
        "        stats[\"MAE\"].append(m[\"MAE\"])\n",
        "        stats[\"RMSE\"].append(m[\"RMSE\"])\n",
        "        stats[\"DA\"].append(m[\"DA\"])\n",
        "    out = {}\n",
        "    for k, arr in stats.items():\n",
        "        arr = np.array(arr)\n",
        "        out[k] = (float(arr.mean()), float(np.percentile(arr, 2.5)), float(np.percentile(arr, 97.5)))\n",
        "    return out\n",
        "\n",
        "\n",
        "def aggregate_fold_metrics(fold_metrics: List[Dict[str, float]]) -> Dict[str, Tuple[float, float, float]]:\n",
        "    \"\"\"\n",
        "    Given a list of per-fold metrics (dicts with keys MAE, RMSE, DA),\n",
        "    return (mean, lo, hi) where lo/hi are 2.5/97.5 percentiles across folds.\n",
        "    \"\"\"\n",
        "    arrs = {\"MAE\": [], \"RMSE\": [], \"DA\": []}\n",
        "    for fm in fold_metrics:\n",
        "        arrs[\"MAE\"].append(fm[\"MAE\"])\n",
        "        arrs[\"RMSE\"].append(fm[\"RMSE\"])\n",
        "        arrs[\"DA\"].append(fm[\"DA\"])\n",
        "    out = {}\n",
        "    for k, vals in arrs.items():\n",
        "        vals = np.array(vals)\n",
        "        out[k] = (float(vals.mean()), float(np.percentile(vals, 2.5)), float(np.percentile(vals, 97.5)))\n",
        "    return out\n",
        "\n",
        "\n",
        "# ---------------------\n",
        "# Loader helpers\n",
        "# ---------------------\n",
        "def load_json(path: Path) -> Any:\n",
        "    with open(path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def ensure_array(x) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Convert list-like to numpy array. Handles nested lists etc.\n",
        "    \"\"\"\n",
        "    return np.asarray(x, dtype=float)\n",
        "\n",
        "\n",
        "def detect_preds_structure(data: Any) -> Tuple[str, Any]:\n",
        "    \"\"\"\n",
        "    Return a tuple (kind, payload)\n",
        "    kind: \"single\", \"folds\", \"legacy_per_fold\"\n",
        "    payload: for \"single\" -> dict with 'preds' and 'y_test'\n",
        "             for \"folds\" -> list of per-fold dicts (each with preds,y_test)\n",
        "    \"\"\"\n",
        "    if isinstance(data, dict):\n",
        "        if \"preds\" in data and \"y_test\" in data:\n",
        "            return \"single\", {\"preds\": data[\"preds\"], \"y_test\": data[\"y_test\"], \"metrics\": data.get(\"metrics\", None)}\n",
        "        if \"per_fold\" in data and isinstance(data[\"per_fold\"], list):\n",
        "            return \"folds\", data[\"per_fold\"]\n",
        "        # sometimes authors put folds under 'folds' or 'results'\n",
        "        for k in (\"folds\", \"results\", \"perFold\"):\n",
        "            if k in data and isinstance(data[k], list):\n",
        "                return \"folds\", data[k]\n",
        "        # If dict of many entries that look like fold keys, attempt to coerce\n",
        "        # Otherwise fail and try list detection below\n",
        "    if isinstance(data, list):\n",
        "        # list of fold-like objects? ensure each has preds & y_test\n",
        "        if len(data) > 0 and all(isinstance(x, dict) and \"preds\" in x and \"y_test\" in x for x in data):\n",
        "            return \"folds\", data\n",
        "    raise ValueError(\"Unrecognized preds_json structure. Expect single dict with 'preds' and 'y_test', or a list/dict containing per-fold entries.\")\n",
        "\n",
        "\n",
        "# ---------------------\n",
        "# Main\n",
        "# ---------------------\n",
        "def main():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--preds_json\", required=True, help=\"Predictions JSON created by training script\")\n",
        "    p.add_argument(\"--out\", required=True, help=\"Output metrics JSON path\")\n",
        "    p.add_argument(\"--n_boot\", type=int, default=1000, help=\"Bootstrap samples when computing CIs for a single-fold preds file\")\n",
        "    p.add_argument(\"--seed\", type=int, default=42, help=\"Bootstrap RNG seed\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    preds_path = Path(args.preds_json)\n",
        "    if not preds_path.exists():\n",
        "        raise FileNotFoundError(args.preds_json)\n",
        "\n",
        "    data = load_json(preds_path)\n",
        "\n",
        "    kind, payload = detect_preds_structure(data)\n",
        "\n",
        "    out_metrics: Dict[str, Any] = {}\n",
        "    out_payload = {\"source\": str(preds_path), \"input_kind\": kind}\n",
        "\n",
        "    if kind == \"single\":\n",
        "        preds = ensure_array(payload[\"preds\"])\n",
        "        y_test = ensure_array(payload[\"y_test\"])\n",
        "        if preds.shape != y_test.shape:\n",
        "            # try to flatten if preds nested\n",
        "            preds = preds.flatten()\n",
        "            y_test = y_test.flatten()\n",
        "            if preds.shape != y_test.shape:\n",
        "                raise ValueError(f\"preds and y_test shapes mismatch after flatten: {preds.shape} vs {y_test.shape}\")\n",
        "        # compute point metrics\n",
        "        m = compute_metrics(y_test, preds)\n",
        "        # bootstrap per-sample for CI\n",
        "        boot = bootstrap_sample_metrics(y_test, preds, n_boot=args.n_boot, seed=args.seed)\n",
        "        out_metrics[\"MAE_mean\"], out_metrics[\"MAE_lo\"], out_metrics[\"MAE_hi\"] = boot[\"MAE\"]\n",
        "        out_metrics[\"RMSE_mean\"], out_metrics[\"RMSE_lo\"], out_metrics[\"RMSE_hi\"] = boot[\"RMSE\"]\n",
        "        out_metrics[\"DA_mean\"], out_metrics[\"DA_lo\"], out_metrics[\"DA_hi\"] = boot[\"DA\"]\n",
        "        out_metrics[\"n_test\"] = int(len(y_test))\n",
        "        out_metrics[\"bootstrap_samples\"] = int(args.n_boot)\n",
        "        out_metrics[\"bootstrap_seed\"] = int(args.seed)\n",
        "        # include training-provided metrics if present\n",
        "        if payload.get(\"metrics\") is not None:\n",
        "            out_payload[\"provided_metrics\"] = payload[\"metrics\"]\n",
        "    elif kind == \"folds\":\n",
        "        folds = payload  # list of dicts\n",
        "        fold_metrics = []\n",
        "        n_total = 0\n",
        "        for f in folds:\n",
        "            if not (\"preds\" in f and \"y_test\" in f):\n",
        "                raise ValueError(\"Each fold entry must contain 'preds' and 'y_test'.\")\n",
        "            preds = ensure_array(f[\"preds\"]).flatten()\n",
        "            y_test = ensure_array(f[\"y_test\"]).flatten()\n",
        "            if preds.shape != y_test.shape:\n",
        "                raise ValueError(\"preds and y_test must have same shape within each fold.\")\n",
        "            n_total += len(y_test)\n",
        "            m = compute_metrics(y_test, preds)\n",
        "            fold_metrics.append(m)\n",
        "        # aggregate metrics across folds (mean and percentile)\n",
        "        agg = aggregate_fold_metrics(fold_metrics)\n",
        "        out_metrics[\"MAE_mean\"], out_metrics[\"MAE_lo\"], out_metrics[\"MAE_hi\"] = agg[\"MAE\"]\n",
        "        out_metrics[\"RMSE_mean\"], out_metrics[\"RMSE_lo\"], out_metrics[\"RMSE_hi\"] = agg[\"RMSE\"]\n",
        "        out_metrics[\"DA_mean\"], out_metrics[\"DA_lo\"], out_metrics[\"DA_hi\"] = agg[\"DA\"]\n",
        "        out_metrics[\"n_folds\"] = len(fold_metrics)\n",
        "        out_metrics[\"n_test_total\"] = int(n_total)\n",
        "    else:\n",
        "        raise RuntimeError(\"unexpected structure type\")\n",
        "\n",
        "    # Write out combined JSON\n",
        "    out_path = Path(args.out)\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(out_path, \"w\") as f:\n",
        "        json.dump({\"metrics\": out_metrics, \"meta\": out_payload}, f, indent=2)\n",
        "\n",
        "    print(f\"Wrote metrics to {out_path}\")\n",
        "    print(json.dumps(out_metrics, indent=2))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "oKMpOKicjdsz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7f27bc1-fc72-4789-e1f4-344174f71a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/eval2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGrFRSZ59Tjb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28db4086-cecd-4241-c532-9bd884dffc82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote metrics to outputs/multimodal_lstm_metrics.json\n",
            "{\n",
            "  \"MAE_mean\": 0.011431647198393038,\n",
            "  \"MAE_lo\": 0.010431341032119302,\n",
            "  \"MAE_hi\": 0.012496913065677908,\n",
            "  \"RMSE_mean\": 0.015759469470166664,\n",
            "  \"RMSE_lo\": 0.014196751696569017,\n",
            "  \"RMSE_hi\": 0.017446440392134114,\n",
            "  \"DA_mean\": 49.658769931662874,\n",
            "  \"DA_lo\": 45.3246013667426,\n",
            "  \"DA_hi\": 54.441913439635535,\n",
            "  \"n_test\": 439,\n",
            "  \"bootstrap_samples\": 1000,\n",
            "  \"bootstrap_seed\": 42\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "python src/eval2.py \\\n",
        "  --preds_json outputs/multimodal_lstm_preds.json \\\n",
        "  --out outputs/multimodal_lstm_metrics.json \\\n",
        "  --n_boot 1000 \\\n",
        "  --seed 42\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The above are the results using early fusion mechanism."
      ],
      "metadata": {
        "id": "qXQ1acojhGXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we will use late fusion, and attention-based fusion."
      ],
      "metadata": {
        "id": "pqh-ljWZhGep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# code for late fusion\n",
        "\n",
        "%%writefile src/train_multimodal_late_fusion.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_multimodal_late_fusion.py\n",
        "\n",
        "Late-fusion multimodal LSTM:\n",
        " - numeric features -> numeric LSTM encoder -> numeric MLP head\n",
        " - text embeddings  -> text LSTM encoder    -> text   MLP head\n",
        " - fusion: concat(numeric_repr, text_repr) -> fusion MLP -> scalar prediction\n",
        "\n",
        "Usage mirrors your previous script (features parquet, text_emb parquet, seq_len, emb_agg, emb_fill, etc).\n",
        "\"\"\"\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "import json\n",
        "from typing import Tuple, Optional, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# -----------------------\n",
        "# (Reuse helpers from your script)\n",
        "# -----------------------\n",
        "COMMON_TARGET_NAMES = [\"target\", \"y\", \"ret\", \"returns\", \"target_return\", \"next_return\"]\n",
        "COMMON_DATE_COLS = [\"date\", \"created_utc\", \"timestamp\", \"created\", \"publish_date\", \"published_at\", \"time\"]\n",
        "\n",
        "def detect_date_col(df: pd.DataFrame, prefer: Optional[str] = None) -> Optional[str]:\n",
        "    if prefer and prefer in df.columns:\n",
        "        return prefer\n",
        "    for c in COMMON_DATE_COLS:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    for c in df.columns:\n",
        "        if pd.api.types.is_datetime64_any_dtype(df[c]):\n",
        "            return c\n",
        "    for c in df.columns:\n",
        "        if pd.api.types.is_integer_dtype(df[c]) or pd.api.types.is_float_dtype(df[c]):\n",
        "            series = df[c].dropna()\n",
        "            if len(series) == 0:\n",
        "                continue\n",
        "            v = series.iloc[0]\n",
        "            if isinstance(v, (int, float)) and (1e9 < abs(v) < 1e12):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "def align_embeddings_to_trade_dates(\n",
        "    df_num: pd.DataFrame,\n",
        "    df_text: pd.DataFrame,\n",
        "    emb_agg: str = \"mean\",\n",
        "    emb_fill: str = \"zero\",\n",
        "    emb_ffill_limit: int = 0,\n",
        "    debug: bool = False,\n",
        ") -> Tuple[pd.DataFrame, int]:\n",
        "    # (copy of your align_embeddings_to_trade_dates, adapted minimally)\n",
        "    df_num = df_num.copy()\n",
        "    df_num[\"date\"] = pd.to_datetime(df_num[\"date\"]).dt.normalize()\n",
        "    df_text = df_text.copy()\n",
        "    if \"date\" in df_text.columns:\n",
        "        df_text[\"date\"] = pd.to_datetime(df_text[\"date\"]).dt.normalize()\n",
        "    else:\n",
        "        date_col = detect_date_col(df_text)\n",
        "        if date_col:\n",
        "            df_text[\"date\"] = pd.to_datetime(df_text[date_col]).dt.normalize()\n",
        "        else:\n",
        "            df_text[\"date\"] = pd.NaT\n",
        "\n",
        "    if \"text_emb\" not in df_text.columns:\n",
        "        raise ValueError(\"text embeddings dataframe must contain 'text_emb' column\")\n",
        "\n",
        "    def to_arr(x):\n",
        "        try:\n",
        "            if isinstance(x, (list, tuple, np.ndarray)):\n",
        "                return np.asarray(x, dtype=float)\n",
        "            if isinstance(x, str):\n",
        "                import ast\n",
        "                v = ast.literal_eval(x)\n",
        "                return np.asarray(v, dtype=float)\n",
        "        except Exception:\n",
        "            return None\n",
        "        return None\n",
        "\n",
        "    df_text[\"_emb_arr\"] = df_text[\"text_emb\"].apply(to_arr)\n",
        "    df_text = df_text[~df_text[\"_emb_arr\"].isna()].copy()\n",
        "    if df_text.shape[0] == 0:\n",
        "        raise ValueError(\"No valid embeddings found in text embeddings dataframe (column 'text_emb')\")\n",
        "\n",
        "    emb_dim = df_text[\"_emb_arr\"].iloc[0].shape[0]\n",
        "    df_text = df_text[df_text[\"_emb_arr\"].apply(lambda x: x.shape[0] == emb_dim)].copy()\n",
        "\n",
        "    emb_cols = [f\"emb_{i}\" for i in range(emb_dim)]\n",
        "    emb_df = pd.DataFrame(df_text[\"_emb_arr\"].tolist(), columns=emb_cols, index=df_text[\"date\"])\n",
        "    emb_df.index.name = \"date\"\n",
        "\n",
        "    if emb_agg == \"mean\":\n",
        "        emb_daily = emb_df.groupby(emb_df.index).mean()\n",
        "    elif emb_agg == \"sum\":\n",
        "        emb_daily = emb_df.groupby(emb_df.index).sum()\n",
        "    elif emb_agg == \"max\":\n",
        "        emb_daily = emb_df.groupby(emb_df.index).max()\n",
        "    else:\n",
        "        emb_daily = emb_df.groupby(emb_df.index).mean()\n",
        "\n",
        "    stock_dates = pd.to_datetime(df_num[\"date\"].unique()).normalize()\n",
        "    emb_index = pd.DatetimeIndex(sorted(stock_dates))\n",
        "    emb_daily = emb_daily.reindex(emb_index)\n",
        "\n",
        "    if emb_fill == \"zero\":\n",
        "        emb_filled = emb_daily.fillna(0.0)\n",
        "    elif emb_fill == \"ffill\":\n",
        "        emb_filled = emb_daily.ffill(limit=emb_ffill_limit)\n",
        "    elif emb_fill in (\"hybrid\", \"ffill_then_zero\"):\n",
        "        emb_filled = emb_daily.ffill(limit=emb_ffill_limit).fillna(0.0)\n",
        "    else:\n",
        "        emb_filled = emb_daily.fillna(0.0)\n",
        "\n",
        "    df_dates_order = pd.DataFrame({\"date\": pd.to_datetime(df_num[\"date\"]).dt.normalize()})\n",
        "    emb_for_dates = emb_filled.reindex(df_dates_order[\"date\"].values).fillna(0.0)\n",
        "    emb_list = emb_for_dates.values.tolist()\n",
        "    df_out = df_dates_order.copy()\n",
        "    df_out[\"text_emb\"] = emb_list\n",
        "    return df_out, emb_dim\n",
        "\n",
        "def detect_or_compute_target(df: pd.DataFrame, target_col_arg: Optional[str], debug: bool = False) -> Tuple[pd.DataFrame, str]:\n",
        "    df = df.copy()\n",
        "    if target_col_arg and target_col_arg in df.columns:\n",
        "        if debug:\n",
        "            print(f\"Using provided target column: {target_col_arg}\")\n",
        "        return df, target_col_arg\n",
        "\n",
        "    for c in COMMON_TARGET_NAMES:\n",
        "        if c in df.columns:\n",
        "            if debug:\n",
        "                print(f\"Found existing target column: {c}\")\n",
        "            return df, c\n",
        "\n",
        "    price_col = None\n",
        "    if \"adj_close\" in df.columns:\n",
        "        price_col = \"adj_close\"\n",
        "    elif \"close\" in df.columns:\n",
        "        price_col = \"close\"\n",
        "    elif \"Close\" in df.columns:\n",
        "        price_col = \"Close\"\n",
        "\n",
        "    if price_col:\n",
        "        if debug:\n",
        "            print(f\"Computing target as next-day pct change of price col '{price_col}'\")\n",
        "        df = df.sort_values(\"date\").copy()\n",
        "        df[\"target\"] = df[price_col].pct_change().shift(-1)\n",
        "        return df, \"target\"\n",
        "\n",
        "    raise ValueError(\"No 'target' column found in features parquet. Provide --target_col or include a price column like 'close' or 'adj_close' to compute one.\")\n",
        "\n",
        "# -----------------------\n",
        "# Sequence creation for two modalities\n",
        "# -----------------------\n",
        "def create_sequences_two_modal(\n",
        "    df: pd.DataFrame,\n",
        "    seq_len: int,\n",
        "    numeric_cols: List[str],\n",
        "    emb_col: str,\n",
        "    target_col: str,\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Return (X_num, X_emb, y, df_sorted)\n",
        "     - X_num: (n_samples, seq_len, n_numeric_feats)\n",
        "     - X_emb: (n_samples, seq_len, emb_dim)\n",
        "     - y:     (n_samples,)\n",
        "    \"\"\"\n",
        "    df2 = df.sort_values(\"date\").reset_index(drop=True).copy()\n",
        "    num_vals = df2[numeric_cols].values.astype(float)\n",
        "    emb_vals = np.vstack(df2[emb_col].apply(lambda x: np.asarray(x, dtype=float)).values)\n",
        "    targets = df2[target_col].values.astype(float)\n",
        "\n",
        "    n = len(df2)\n",
        "    seqs_num = []\n",
        "    seqs_emb = []\n",
        "    ys = []\n",
        "    for i in range(n - seq_len):\n",
        "        seq_n = num_vals[i : i + seq_len]\n",
        "        seq_e = emb_vals[i : i + seq_len]\n",
        "        tgt = targets[i + seq_len]\n",
        "        if np.isnan(tgt):\n",
        "            continue\n",
        "        seqs_num.append(seq_n)\n",
        "        seqs_emb.append(seq_e)\n",
        "        ys.append(tgt)\n",
        "    if len(seqs_num) == 0:\n",
        "        return np.zeros((0, seq_len, num_vals.shape[1])), np.zeros((0, seq_len, emb_vals.shape[1])), np.zeros((0,)), df2\n",
        "    X_num = np.stack(seqs_num, axis=0)\n",
        "    X_emb = np.stack(seqs_emb, axis=0)\n",
        "    y = np.array(ys, dtype=float)\n",
        "    return X_num, X_emb, y, df2\n",
        "\n",
        "# -----------------------\n",
        "# Model: Late fusion\n",
        "# -----------------------\n",
        "class LSTMEncoder(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int = 64, num_layers: int = 1, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        self.proj = nn.Sequential(nn.Linear(hidden_size, hidden_size//2), nn.ReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, input_size)\n",
        "        out, (hn, cn) = self.lstm(x)\n",
        "        last = out[:, -1, :]  # (batch, hidden_size)\n",
        "        return self.proj(last)  # (batch, hidden_size//2)\n",
        "\n",
        "class LateFusionModel(nn.Module):\n",
        "    def __init__(self, num_input_size: int, emb_input_size: int, enc_hidden: int = 64, head_hidden: int = 64, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.num_enc = LSTMEncoder(input_size=num_input_size, hidden_size=enc_hidden, num_layers=1, dropout=dropout)\n",
        "        self.emb_enc = LSTMEncoder(input_size=emb_input_size, hidden_size=enc_hidden, num_layers=1, dropout=dropout)\n",
        "        fusion_in = (enc_hidden//2) * 2\n",
        "        self.fusion_head = nn.Sequential(\n",
        "            nn.Linear(fusion_in, head_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(head_hidden, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x_num, x_emb):\n",
        "        # x_num: (batch, seq_len, n_num_feats)\n",
        "        # x_emb: (batch, seq_len, emb_dim)\n",
        "        r_num = self.num_enc(x_num)\n",
        "        r_emb = self.emb_enc(x_emb)\n",
        "        fusion = torch.cat([r_num, r_emb], dim=1)\n",
        "        out = self.fusion_head(fusion)\n",
        "        return out.squeeze(-1)\n",
        "\n",
        "# -----------------------\n",
        "# Training & main\n",
        "# -----------------------\n",
        "def compute_metrics(a_true: np.ndarray, a_pred: np.ndarray):\n",
        "    mae_v = np.mean(np.abs(a_pred - a_true))\n",
        "    rmse_v = np.sqrt(np.mean((a_pred - a_true) ** 2))\n",
        "    if len(a_true) >= 2:\n",
        "        da_v = 100.0 * np.mean(np.sign(a_pred) == np.sign(a_true))\n",
        "    else:\n",
        "        da_v = 0.0\n",
        "    return mae_v, rmse_v, da_v\n",
        "\n",
        "def main():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--features\", required=True)\n",
        "    p.add_argument(\"--text_emb\", required=True)\n",
        "    p.add_argument(\"--seq_len\", type=int, default=10)\n",
        "    p.add_argument(\"--emb_agg\", default=\"mean\", choices=[\"mean\", \"sum\", \"max\"])\n",
        "    p.add_argument(\"--emb_fill\", default=\"zero\", choices=[\"zero\", \"ffill\", \"hybrid\"])\n",
        "    p.add_argument(\"--emb_ffill_limit\", type=int, default=0)\n",
        "    p.add_argument(\"--target_col\", default=None)\n",
        "    p.add_argument(\"--epochs\", type=int, default=10)\n",
        "    p.add_argument(\"--batch_size\", type=int, default=64)\n",
        "    p.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    p.add_argument(\"--device\", default=\"cpu\")\n",
        "    p.add_argument(\"--out\", default=\"outputs/multimodal_late_preds.json\")\n",
        "    p.add_argument(\"--debug\", action=\"store_true\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    device = torch.device(args.device if torch.cuda.is_available() and args.device != \"cpu\" else \"cpu\")\n",
        "    if args.debug:\n",
        "        print(\"Device:\", device)\n",
        "\n",
        "    df_features = pd.read_parquet(args.features)\n",
        "    df_text = pd.read_parquet(args.text_emb)\n",
        "\n",
        "    if \"date\" not in df_features.columns:\n",
        "        date_col = detect_date_col(df_features)\n",
        "        if date_col:\n",
        "            df_features[\"date\"] = pd.to_datetime(df_features[date_col]).dt.normalize()\n",
        "        else:\n",
        "            raise ValueError(\"Features parquet must contain a 'date' column or a detectable datetime column.\")\n",
        "\n",
        "    if \"date\" not in df_text.columns:\n",
        "        date_col = detect_date_col(df_text)\n",
        "        if date_col:\n",
        "            df_text[\"date\"] = pd.to_datetime(df_text[date_col]).dt.normalize()\n",
        "        else:\n",
        "            df_text[\"date\"] = pd.NaT\n",
        "\n",
        "    df_text_aligned, emb_dim = align_embeddings_to_trade_dates(\n",
        "        df_num=df_features,\n",
        "        df_text=df_text,\n",
        "        emb_agg=args.emb_agg,\n",
        "        emb_fill=args.emb_fill,\n",
        "        emb_ffill_limit=args.emb_ffill_limit,\n",
        "        debug=args.debug,\n",
        "    )\n",
        "\n",
        "    # Merge aligned embeddings into features (preserve features order)\n",
        "    df_features = df_features.sort_values(\"date\").reset_index(drop=True)\n",
        "    df_text_aligned = df_text_aligned.reset_index(drop=True)\n",
        "    if len(df_text_aligned) != len(df_features):\n",
        "        df_merged = pd.merge(df_features, df_text_aligned, on=\"date\", how=\"left\", sort=False)\n",
        "    else:\n",
        "        df_merged = pd.concat([df_features.reset_index(drop=True), df_text_aligned[\"text_emb\"].reset_index(drop=True)], axis=1)\n",
        "\n",
        "    if df_merged[\"text_emb\"].isna().any():\n",
        "        if args.debug:\n",
        "            print(\"Filling NaN text_emb with zeros\")\n",
        "        df_merged[\"text_emb\"] = df_merged[\"text_emb\"].apply(lambda x: [0.0]*emb_dim if (pd.isna(x) or x is None) else x)\n",
        "\n",
        "    df_merged, target_col = detect_or_compute_target(df_merged, args.target_col, debug=args.debug)\n",
        "\n",
        "    non_feature_cols = set([\"date\", target_col, \"text_emb\"])\n",
        "    numeric_cols = [c for c in df_merged.columns if c not in non_feature_cols and np.issubdtype(df_merged[c].dtype, np.number)]\n",
        "\n",
        "    if args.debug:\n",
        "        print(\"Numeric feature columns (sample):\", numeric_cols[:10], \" total:\", len(numeric_cols))\n",
        "        print(\"Embedding dim:\", emb_dim)\n",
        "\n",
        "    # create sequences for both modalities\n",
        "    X_num, X_emb, y, df_sorted = create_sequences_two_modal(df_merged[[\"date\"]+numeric_cols+[\"text_emb\", target_col]],\n",
        "                                                            seq_len=args.seq_len, numeric_cols=numeric_cols, emb_col=\"text_emb\", target_col=target_col)\n",
        "    if args.debug:\n",
        "        print(\"X_num shape:\", X_num.shape, \"X_emb shape:\", X_emb.shape, \"y shape:\", y.shape)\n",
        "\n",
        "    if X_num.shape[0] == 0:\n",
        "        raise ValueError(\"No sequences created (dataset too short or NaN targets)\")\n",
        "\n",
        "    # split chronologically\n",
        "    n = X_num.shape[0]\n",
        "    train_n = int(n*0.8)\n",
        "    Xn_tr = X_num[:train_n]\n",
        "    Xe_tr = X_emb[:train_n]\n",
        "    y_tr = y[:train_n]\n",
        "\n",
        "    Xn_te = X_num[train_n:]\n",
        "    Xe_te = X_emb[train_n:]\n",
        "    y_te = y[train_n:]\n",
        "\n",
        "    # build model\n",
        "    num_input_size = Xn_tr.shape[2]\n",
        "    emb_input_size = X_emb.shape[2]\n",
        "    model = LateFusionModel(num_input_size=num_input_size, emb_input_size=emb_input_size, enc_hidden=64, head_hidden=64, dropout=0.1).to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    train_ds = TensorDataset(torch.from_numpy(Xn_tr).float(), torch.from_numpy(Xe_tr).float(), torch.from_numpy(y_tr).float())\n",
        "    test_ds = TensorDataset(torch.from_numpy(Xn_te).float(), torch.from_numpy(Xe_te).float(), torch.from_numpy(y_te).float())\n",
        "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "    if args.debug:\n",
        "        print(f\"Training for {args.epochs} epochs | train samples: {len(train_ds)} | test samples: {len(test_ds)}\")\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(args.epochs):\n",
        "        total_loss = 0.0\n",
        "        for xb_num, xb_emb, yb in train_loader:\n",
        "            xb_num = xb_num.to(device)\n",
        "            xb_emb = xb_emb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            out = model(xb_num, xb_emb)\n",
        "            loss = criterion(out, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item() * xb_num.size(0)\n",
        "        avg_loss = total_loss / len(train_ds)\n",
        "        if args.debug:\n",
        "            print(f\"[Epoch {epoch+1}/{args.epochs}] train loss: {avg_loss:.6f}\")\n",
        "\n",
        "    # eval\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    trues = []\n",
        "    with torch.no_grad():\n",
        "        for xb_num, xb_emb, yb in test_loader:\n",
        "            xb_num = xb_num.to(device)\n",
        "            xb_emb = xb_emb.to(device)\n",
        "            out = model(xb_num, xb_emb)\n",
        "            preds.extend(out.cpu().numpy().tolist())\n",
        "            trues.extend(yb.numpy().tolist())\n",
        "\n",
        "    preds = np.array(preds)\n",
        "    trues = np.array(trues)\n",
        "\n",
        "    mae, rmse, da = compute_metrics(trues, preds)\n",
        "\n",
        "    # bootstrap CI (1000) as in your prior script\n",
        "    n_test_pts = len(trues)\n",
        "    n_boot = 1000\n",
        "    seed = 42\n",
        "    if n_test_pts >= 2:\n",
        "        rng = np.random.default_rng(seed)\n",
        "        mae_bs = np.empty(n_boot, dtype=float)\n",
        "        rmse_bs = np.empty(n_boot, dtype=float)\n",
        "        da_bs = np.empty(n_boot, dtype=float)\n",
        "        for i in range(n_boot):\n",
        "            idx = rng.integers(0, n_test_pts, size=n_test_pts)\n",
        "            t_sample = trues[idx]\n",
        "            p_sample = preds[idx]\n",
        "            m, r, d = compute_metrics(t_sample, p_sample)\n",
        "            mae_bs[i] = m\n",
        "            rmse_bs[i] = r\n",
        "            da_bs[i] = d\n",
        "        mae_lo, mae_hi = np.percentile(mae_bs, [2.5, 97.5]).tolist()\n",
        "        rmse_lo, rmse_hi = np.percentile(rmse_bs, [2.5, 97.5]).tolist()\n",
        "        da_lo, da_hi = np.percentile(da_bs, [2.5, 97.5]).tolist()\n",
        "    else:\n",
        "        mae_lo = mae_hi = float(mae)\n",
        "        rmse_lo = rmse_hi = float(rmse)\n",
        "        da_lo = da_hi = float(da)\n",
        "\n",
        "    out_metrics = {\n",
        "        \"MAE_mean\": float(mae), \"MAE_lo\": float(mae_lo), \"MAE_hi\": float(mae_hi),\n",
        "        \"RMSE_mean\": float(rmse), \"RMSE_lo\": float(rmse_lo), \"RMSE_hi\": float(rmse_hi),\n",
        "        \"DA_mean\": float(da), \"DA_lo\": float(da_lo), \"DA_hi\": float(da_hi),\n",
        "        \"n_test\": int(n_test_pts), \"bootstrap_samples\": int(n_boot) if n_test_pts>=2 else 0, \"bootstrap_seed\": int(seed) if n_test_pts>=2 else None\n",
        "    }\n",
        "\n",
        "    Path(args.out).parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(args.out, \"w\") as f:\n",
        "        json.dump({\"preds\": preds.tolist(), \"y_test\": trues.tolist(), \"metrics\": out_metrics}, f)\n",
        "\n",
        "    print(\"Saved predictions+metrics to\", args.out)\n",
        "    if args.debug:\n",
        "        print(\"Metrics:\", json.dumps(out_metrics, indent=2))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fo5QEuaPhGuA",
        "outputId": "28b2524f-e86a-4a1b-f47f-3039d4512d87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/train_multimodal_late_fusion.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#updated\n",
        "\n",
        "%%bash\n",
        "python src/train_multimodal_late_fusion.py \\\n",
        "  --features data/processed/aapl_features.parquet \\\n",
        "  --text_emb data/processed/text_embeddings.parquet \\\n",
        "  --seq_len 10 --emb_agg mean --emb_fill zero \\\n",
        "  --epochs 30 --batch_size 64 --device cpu --debug \\\n",
        "  --out outputs/multimodal_late_fusion_preds.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBICWkMEhe2u",
        "outputId": "e3c8c1f1-033b-4f5e-f5a1-56a73967f5d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Found existing target column: ret\n",
            "Numeric feature columns (sample): ['Close', 'High', 'Low', 'Open', 'Volume', 'close_next', 'ret_next', 'ret_lag_1', 'ret_lag_2', 'ret_lag_3']  total: 15\n",
            "Embedding dim: 384\n",
            "X_num shape: (2192, 10, 15) X_emb shape: (2192, 10, 384) y shape: (2192,)\n",
            "Training for 30 epochs | train samples: 1753 | test samples: 439\n",
            "[Epoch 1/30] train loss: 0.001099\n",
            "[Epoch 2/30] train loss: 0.000623\n",
            "[Epoch 3/30] train loss: 0.000547\n",
            "[Epoch 4/30] train loss: 0.000533\n",
            "[Epoch 5/30] train loss: 0.000545\n",
            "[Epoch 6/30] train loss: 0.000535\n",
            "[Epoch 7/30] train loss: 0.000489\n",
            "[Epoch 8/30] train loss: 0.000500\n",
            "[Epoch 9/30] train loss: 0.000512\n",
            "[Epoch 10/30] train loss: 0.000499\n",
            "[Epoch 11/30] train loss: 0.000488\n",
            "[Epoch 12/30] train loss: 0.000486\n",
            "[Epoch 13/30] train loss: 0.000494\n",
            "[Epoch 14/30] train loss: 0.000496\n",
            "[Epoch 15/30] train loss: 0.000491\n",
            "[Epoch 16/30] train loss: 0.000478\n",
            "[Epoch 17/30] train loss: 0.000497\n",
            "[Epoch 18/30] train loss: 0.000476\n",
            "[Epoch 19/30] train loss: 0.000461\n",
            "[Epoch 20/30] train loss: 0.000477\n",
            "[Epoch 21/30] train loss: 0.000471\n",
            "[Epoch 22/30] train loss: 0.000471\n",
            "[Epoch 23/30] train loss: 0.000474\n",
            "[Epoch 24/30] train loss: 0.000470\n",
            "[Epoch 25/30] train loss: 0.000472\n",
            "[Epoch 26/30] train loss: 0.000474\n",
            "[Epoch 27/30] train loss: 0.000473\n",
            "[Epoch 28/30] train loss: 0.000472\n",
            "[Epoch 29/30] train loss: 0.000466\n",
            "[Epoch 30/30] train loss: 0.000468\n",
            "Saved predictions+metrics to outputs/multimodal_late_fusion_preds.json\n",
            "Metrics: {\n",
            "  \"MAE_mean\": 0.01146860880872573,\n",
            "  \"MAE_lo\": 0.010493579809380309,\n",
            "  \"MAE_hi\": 0.012453846391022259,\n",
            "  \"RMSE_mean\": 0.01579088061323475,\n",
            "  \"RMSE_lo\": 0.014249447129575708,\n",
            "  \"RMSE_hi\": 0.01733963369917492,\n",
            "  \"DA_mean\": 53.98633257403189,\n",
            "  \"DA_lo\": 49.430523917995444,\n",
            "  \"DA_hi\": 58.31435079726651,\n",
            "  \"n_test\": 439,\n",
            "  \"bootstrap_samples\": 1000,\n",
            "  \"bootstrap_seed\": 42\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python src/eval2.py \\\n",
        "  --preds_json outputs/multimodal_late_fusion_preds.json \\\n",
        "  --out outputs/multimodal_late_fusion_metrics.json \\\n",
        "  --n_boot 1000 \\\n",
        "  --seed 42"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q_8k1YHllor",
        "outputId": "4793ad44-7dcd-4a4e-c3f3-395c2aba076c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote metrics to outputs/multimodal_late_fusion_metrics.json\n",
            "{\n",
            "  \"MAE_mean\": 0.011443764819123018,\n",
            "  \"MAE_lo\": 0.010447056805919487,\n",
            "  \"MAE_hi\": 0.012493952754084603,\n",
            "  \"RMSE_mean\": 0.015729491459482225,\n",
            "  \"RMSE_lo\": 0.01412872916399385,\n",
            "  \"RMSE_hi\": 0.0173640916644696,\n",
            "  \"DA_mean\": 53.80273348519363,\n",
            "  \"DA_lo\": 49.19703872437358,\n",
            "  \"DA_hi\": 58.54783599088837,\n",
            "  \"n_test\": 439,\n",
            "  \"bootstrap_samples\": 1000,\n",
            "  \"bootstrap_seed\": 42\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# code for attention-based fusion\n",
        "\n",
        "%%writefile src/train_multimodal_attention_fusion.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_multimodal_attention_fusion.py\n",
        "\n",
        "Attention-based fusion (cross-attention):\n",
        " - numeric LSTM -> numeric_repr (query)\n",
        " - text LSTM    -> text_seq_repr (keys, values)\n",
        " - cross-attention: query attends to text sequence -> context\n",
        " - fusion: concat(numeric_repr, context) -> MLP -> scalar prediction\n",
        "\"\"\"\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "import json\n",
        "from typing import Tuple, Optional, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# -----------------------\n",
        "# Helpers (same logic as your other scripts)\n",
        "# -----------------------\n",
        "COMMON_TARGET_NAMES = [\"target\", \"y\", \"ret\", \"returns\", \"target_return\", \"next_return\"]\n",
        "COMMON_DATE_COLS = [\"date\", \"created_utc\", \"timestamp\", \"created\", \"publish_date\", \"published_at\", \"time\"]\n",
        "\n",
        "def detect_date_col(df: pd.DataFrame, prefer: Optional[str] = None) -> Optional[str]:\n",
        "    if prefer and prefer in df.columns:\n",
        "        return prefer\n",
        "    for c in COMMON_DATE_COLS:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    for c in df.columns:\n",
        "        if pd.api.types.is_datetime64_any_dtype(df[c]):\n",
        "            return c\n",
        "    for c in df.columns:\n",
        "        if pd.api.types.is_integer_dtype(df[c]) or pd.api.types.is_float_dtype(df[c]):\n",
        "            series = df[c].dropna()\n",
        "            if len(series) == 0:\n",
        "                continue\n",
        "            v = series.iloc[0]\n",
        "            if isinstance(v, (int, float)) and (1e9 < abs(v) < 1e12):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "def align_embeddings_to_trade_dates(\n",
        "    df_num: pd.DataFrame,\n",
        "    df_text: pd.DataFrame,\n",
        "    emb_agg: str = \"mean\",\n",
        "    emb_fill: str = \"zero\",\n",
        "    emb_ffill_limit: int = 0,\n",
        "    debug: bool = False,\n",
        ") -> Tuple[pd.DataFrame, int]:\n",
        "    df_num = df_num.copy()\n",
        "    df_num[\"date\"] = pd.to_datetime(df_num[\"date\"]).dt.normalize()\n",
        "    df_text = df_text.copy()\n",
        "    if \"date\" in df_text.columns:\n",
        "        df_text[\"date\"] = pd.to_datetime(df_text[\"date\"]).dt.normalize()\n",
        "    else:\n",
        "        date_col = detect_date_col(df_text)\n",
        "        if date_col:\n",
        "            df_text[\"date\"] = pd.to_datetime(df_text[date_col]).dt.normalize()\n",
        "        else:\n",
        "            df_text[\"date\"] = pd.NaT\n",
        "\n",
        "    if \"text_emb\" not in df_text.columns:\n",
        "        raise ValueError(\"text embeddings dataframe must contain 'text_emb' column\")\n",
        "\n",
        "    def to_arr(x):\n",
        "        try:\n",
        "            if isinstance(x, (list, tuple, np.ndarray)):\n",
        "                return np.asarray(x, dtype=float)\n",
        "            if isinstance(x, str):\n",
        "                import ast\n",
        "                v = ast.literal_eval(x)\n",
        "                return np.asarray(v, dtype=float)\n",
        "        except Exception:\n",
        "            return None\n",
        "        return None\n",
        "\n",
        "    df_text[\"_emb_arr\"] = df_text[\"text_emb\"].apply(to_arr)\n",
        "    df_text = df_text[~df_text[\"_emb_arr\"].isna()].copy()\n",
        "    if df_text.shape[0] == 0:\n",
        "        raise ValueError(\"No valid embeddings found in text embeddings dataframe (column 'text_emb')\")\n",
        "\n",
        "    emb_dim = df_text[\"_emb_arr\"].iloc[0].shape[0]\n",
        "    df_text = df_text[df_text[\"_emb_arr\"].apply(lambda x: x.shape[0] == emb_dim)].copy()\n",
        "\n",
        "    emb_cols = [f\"emb_{i}\" for i in range(emb_dim)]\n",
        "    emb_df = pd.DataFrame(df_text[\"_emb_arr\"].tolist(), columns=emb_cols, index=df_text[\"date\"])\n",
        "    emb_df.index.name = \"date\"\n",
        "\n",
        "    if emb_agg == \"mean\":\n",
        "        emb_daily = emb_df.groupby(emb_df.index).mean()\n",
        "    elif emb_agg == \"sum\":\n",
        "        emb_daily = emb_df.groupby(emb_df.index).sum()\n",
        "    elif emb_agg == \"max\":\n",
        "        emb_daily = emb_df.groupby(emb_df.index).max()\n",
        "    else:\n",
        "        emb_daily = emb_df.groupby(emb_df.index).mean()\n",
        "\n",
        "    stock_dates = pd.to_datetime(df_num[\"date\"].unique()).normalize()\n",
        "    emb_index = pd.DatetimeIndex(sorted(stock_dates))\n",
        "    emb_daily = emb_daily.reindex(emb_index)\n",
        "\n",
        "    if emb_fill == \"zero\":\n",
        "        emb_filled = emb_daily.fillna(0.0)\n",
        "    elif emb_fill == \"ffill\":\n",
        "        emb_filled = emb_daily.ffill(limit=emb_ffill_limit)\n",
        "    elif emb_fill in (\"hybrid\", \"ffill_then_zero\"):\n",
        "        emb_filled = emb_daily.ffill(limit=emb_ffill_limit).fillna(0.0)\n",
        "    else:\n",
        "        emb_filled = emb_daily.fillna(0.0)\n",
        "\n",
        "    df_dates_order = pd.DataFrame({\"date\": pd.to_datetime(df_num[\"date\"]).dt.normalize()})\n",
        "    emb_for_dates = emb_filled.reindex(df_dates_order[\"date\"].values).fillna(0.0)\n",
        "    emb_list = emb_for_dates.values.tolist()\n",
        "    df_out = df_dates_order.copy()\n",
        "    df_out[\"text_emb\"] = emb_list\n",
        "    return df_out, emb_dim\n",
        "\n",
        "def detect_or_compute_target(df: pd.DataFrame, target_col_arg: Optional[str], debug: bool = False) -> Tuple[pd.DataFrame, str]:\n",
        "    df = df.copy()\n",
        "    if target_col_arg and target_col_arg in df.columns:\n",
        "        if debug:\n",
        "            print(f\"Using provided target column: {target_col_arg}\")\n",
        "        return df, target_col_arg\n",
        "\n",
        "    for c in COMMON_TARGET_NAMES:\n",
        "        if c in df.columns:\n",
        "            if debug:\n",
        "                print(f\"Found existing target column: {c}\")\n",
        "            return df, c\n",
        "\n",
        "    price_col = None\n",
        "    if \"adj_close\" in df.columns:\n",
        "        price_col = \"adj_close\"\n",
        "    elif \"close\" in df.columns:\n",
        "        price_col = \"close\"\n",
        "    elif \"Close\" in df.columns:\n",
        "        price_col = \"Close\"\n",
        "\n",
        "    if price_col:\n",
        "        if debug:\n",
        "            print(f\"Computing target as next-day pct change of price col '{price_col}'\")\n",
        "        df = df.sort_values(\"date\").copy()\n",
        "        df[\"target\"] = df[price_col].pct_change().shift(-1)\n",
        "        return df, \"target\"\n",
        "\n",
        "    raise ValueError(\"No 'target' column found in features parquet. Provide --target_col or include a price column like 'close' or 'adj_close' to compute one.\")\n",
        "\n",
        "def create_sequences_two_modal(\n",
        "    df: pd.DataFrame,\n",
        "    seq_len: int,\n",
        "    numeric_cols: List[str],\n",
        "    emb_col: str,\n",
        "    target_col: str,\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, pd.DataFrame]:\n",
        "    df2 = df.sort_values(\"date\").reset_index(drop=True).copy()\n",
        "    num_vals = df2[numeric_cols].values.astype(float)\n",
        "    emb_vals = np.vstack(df2[emb_col].apply(lambda x: np.asarray(x, dtype=float)).values)\n",
        "    targets = df2[target_col].values.astype(float)\n",
        "\n",
        "    n = len(df2)\n",
        "    seqs_num = []\n",
        "    seqs_emb = []\n",
        "    ys = []\n",
        "    for i in range(n - seq_len):\n",
        "        seq_n = num_vals[i : i + seq_len]\n",
        "        seq_e = emb_vals[i : i + seq_len]\n",
        "        tgt = targets[i + seq_len]\n",
        "        if np.isnan(tgt):\n",
        "            continue\n",
        "        seqs_num.append(seq_n)\n",
        "        seqs_emb.append(seq_e)\n",
        "        ys.append(tgt)\n",
        "    if len(seqs_num) == 0:\n",
        "        return np.zeros((0, seq_len, num_vals.shape[1])), np.zeros((0, seq_len, emb_vals.shape[1])), np.zeros((0,)), df2\n",
        "    X_num = np.stack(seqs_num, axis=0)\n",
        "    X_emb = np.stack(seqs_emb, axis=0)\n",
        "    y = np.array(ys, dtype=float)\n",
        "    return X_num, X_emb, y, df2\n",
        "\n",
        "# -----------------------\n",
        "# Model components\n",
        "# -----------------------\n",
        "class LSTMEncoder(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int = 64, num_layers: int = 1, dropout: float = 0.0, return_seq: bool = False):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        self.return_seq = return_seq\n",
        "        # optional projection to a shared attention dimension\n",
        "        self.proj = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, input_size)\n",
        "        out, (hn, cn) = self.lstm(x)\n",
        "        # out: (batch, seq_len, hidden_size)\n",
        "        if self.return_seq:\n",
        "            # project sequence for attention keys/values\n",
        "            seq_proj = self.proj(out)  # (batch, seq_len, hidden)\n",
        "            return seq_proj  # sequence representation\n",
        "        else:\n",
        "            last = out[:, -1, :]  # (batch, hidden)\n",
        "            return self.proj(last)  # projected numeric representation\n",
        "\n",
        "class AttentionFusionModel(nn.Module):\n",
        "    def __init__(self, num_input_size: int, emb_input_size: int, enc_hidden: int = 64, attn_dim: int = 64, n_heads: int = 4, head_hidden: int = 64, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        # encoders\n",
        "        self.num_enc = LSTMEncoder(input_size=num_input_size, hidden_size=enc_hidden, return_seq=False)\n",
        "        self.emb_enc = LSTMEncoder(input_size=emb_input_size, hidden_size=enc_hidden, return_seq=True)\n",
        "\n",
        "        # ensure query/key/value dims match attention dimension\n",
        "        # project numeric repr (query) to attn_dim and emb seq to attn_dim\n",
        "        self.query_proj = nn.Linear(enc_hidden, attn_dim)\n",
        "        self.kv_proj = nn.Linear(enc_hidden, attn_dim)\n",
        "\n",
        "        # PyTorch MultiheadAttention expects (L, N, E)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=attn_dim, num_heads=n_heads, dropout=dropout, batch_first=False)\n",
        "\n",
        "        fusion_in = enc_hidden + attn_dim  # numeric_repr (enc_hidden) + attended_context (attn_dim)\n",
        "        self.fusion_head = nn.Sequential(\n",
        "            nn.Linear(fusion_in, head_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(head_hidden, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x_num, x_emb):\n",
        "        # x_num: (batch, seq_len, n_num_feats)\n",
        "        # x_emb: (batch, seq_len, emb_dim)\n",
        "        num_repr = self.num_enc(x_num)         # (batch, enc_hidden)\n",
        "        emb_seq = self.emb_enc(x_emb)          # (batch, seq_len, enc_hidden)\n",
        "\n",
        "        # project to attention dim\n",
        "        q = self.query_proj(num_repr)          # (batch, attn_dim)\n",
        "        k = self.kv_proj(emb_seq)              # (batch, seq_len, attn_dim)\n",
        "        v = k                                  # use same proj for values\n",
        "\n",
        "        # prepare for MultiheadAttention: (L, N, E)\n",
        "        # q must be (tgt_len, batch, E). We'll use tgt_len=1 (single query).\n",
        "        q = q.unsqueeze(0)                     # (1, batch, attn_dim)\n",
        "        k = k.permute(1, 0, 2)                 # (seq_len, batch, attn_dim)\n",
        "        v = v.permute(1, 0, 2)                 # (seq_len, batch, attn_dim)\n",
        "\n",
        "        # attention: attn_output: (tgt_len, batch, attn_dim)\n",
        "        attn_out, attn_weights = self.attn(q, k, v, need_weights=False)\n",
        "        attn_out = attn_out.squeeze(0)         # (batch, attn_dim)\n",
        "\n",
        "        # fuse numeric repr and attended text context\n",
        "        fused = torch.cat([num_repr, attn_out], dim=1)  # (batch, enc_hidden + attn_dim)\n",
        "        out = self.fusion_head(fused)                  # (batch, 1) -> squeezed\n",
        "        return out.squeeze(-1)\n",
        "\n",
        "# -----------------------\n",
        "# Metrics\n",
        "# -----------------------\n",
        "def compute_metrics(a_true: np.ndarray, a_pred: np.ndarray):\n",
        "    mae_v = np.mean(np.abs(a_pred - a_true))\n",
        "    rmse_v = np.sqrt(np.mean((a_pred - a_true) ** 2))\n",
        "    if len(a_true) >= 2:\n",
        "        da_v = 100.0 * np.mean(np.sign(a_pred) == np.sign(a_true))\n",
        "    else:\n",
        "        da_v = 0.0\n",
        "    return mae_v, rmse_v, da_v\n",
        "\n",
        "# -----------------------\n",
        "# Main training script\n",
        "# -----------------------\n",
        "def main():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--features\", required=True)\n",
        "    p.add_argument(\"--text_emb\", required=True)\n",
        "    p.add_argument(\"--seq_len\", type=int, default=10)\n",
        "    p.add_argument(\"--emb_agg\", default=\"mean\", choices=[\"mean\", \"sum\", \"max\"])\n",
        "    p.add_argument(\"--emb_fill\", default=\"zero\", choices=[\"zero\", \"ffill\", \"hybrid\"])\n",
        "    p.add_argument(\"--emb_ffill_limit\", type=int, default=0)\n",
        "    p.add_argument(\"--target_col\", default=None)\n",
        "    p.add_argument(\"--epochs\", type=int, default=20)\n",
        "    p.add_argument(\"--batch_size\", type=int, default=64)\n",
        "    p.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    p.add_argument(\"--device\", default=\"cpu\")\n",
        "    p.add_argument(\"--out\", default=\"outputs/multimodal_attention_preds.json\")\n",
        "    p.add_argument(\"--debug\", action=\"store_true\")\n",
        "    p.add_argument(\"--attn_dim\", type=int, default=64)\n",
        "    p.add_argument(\"--n_heads\", type=int, default=4)\n",
        "    args = p.parse_args()\n",
        "\n",
        "    device = torch.device(args.device if torch.cuda.is_available() and args.device != \"cpu\" else \"cpu\")\n",
        "    if args.debug:\n",
        "        print(\"Device:\", device)\n",
        "\n",
        "    df_features = pd.read_parquet(args.features)\n",
        "    df_text = pd.read_parquet(args.text_emb)\n",
        "\n",
        "    if \"date\" not in df_features.columns:\n",
        "        date_col = detect_date_col(df_features)\n",
        "        if date_col:\n",
        "            df_features[\"date\"] = pd.to_datetime(df_features[date_col]).dt.normalize()\n",
        "        else:\n",
        "            raise ValueError(\"Features parquet must contain a 'date' column or a detectable datetime column.\")\n",
        "\n",
        "    if \"date\" not in df_text.columns:\n",
        "        date_col = detect_date_col(df_text)\n",
        "        if date_col:\n",
        "            df_text[\"date\"] = pd.to_datetime(df_text[date_col]).dt.normalize()\n",
        "        else:\n",
        "            df_text[\"date\"] = pd.NaT\n",
        "\n",
        "    df_text_aligned, emb_dim = align_embeddings_to_trade_dates(\n",
        "        df_num=df_features,\n",
        "        df_text=df_text,\n",
        "        emb_agg=args.emb_agg,\n",
        "        emb_fill=args.emb_fill,\n",
        "        emb_ffill_limit=args.emb_ffill_limit,\n",
        "        debug=args.debug,\n",
        "    )\n",
        "\n",
        "    # merge aligned embeddings into features\n",
        "    df_features = df_features.sort_values(\"date\").reset_index(drop=True)\n",
        "    df_text_aligned = df_text_aligned.reset_index(drop=True)\n",
        "    if len(df_text_aligned) != len(df_features):\n",
        "        df_merged = pd.merge(df_features, df_text_aligned, on=\"date\", how=\"left\", sort=False)\n",
        "    else:\n",
        "        df_merged = pd.concat([df_features.reset_index(drop=True), df_text_aligned[\"text_emb\"].reset_index(drop=True)], axis=1)\n",
        "\n",
        "    if df_merged[\"text_emb\"].isna().any():\n",
        "        if args.debug:\n",
        "            print(\"Filling NaN text_emb with zeros\")\n",
        "        df_merged[\"text_emb\"] = df_merged[\"text_emb\"].apply(lambda x: [0.0]*emb_dim if (pd.isna(x) or x is None) else x)\n",
        "\n",
        "    df_merged, target_col = detect_or_compute_target(df_merged, args.target_col, debug=args.debug)\n",
        "\n",
        "    non_feature_cols = set([\"date\", target_col, \"text_emb\"])\n",
        "    numeric_cols = [c for c in df_merged.columns if c not in non_feature_cols and np.issubdtype(df_merged[c].dtype, np.number)]\n",
        "\n",
        "    if args.debug:\n",
        "        print(\"Numeric feature columns (sample):\", numeric_cols[:10], \" total:\", len(numeric_cols))\n",
        "        print(\"Embedding dim detected:\", emb_dim)\n",
        "\n",
        "    X_num, X_emb, y, df_sorted = create_sequences_two_modal(df_merged[[\"date\"]+numeric_cols+[\"text_emb\", target_col]],\n",
        "                                                            seq_len=args.seq_len, numeric_cols=numeric_cols, emb_col=\"text_emb\", target_col=target_col)\n",
        "    if args.debug:\n",
        "        print(\"X_num shape:\", X_num.shape, \"X_emb shape:\", X_emb.shape, \"y shape:\", y.shape)\n",
        "\n",
        "    if X_num.shape[0] == 0:\n",
        "        raise ValueError(\"No sequences created (dataset too short or NaN targets)\")\n",
        "\n",
        "    # chronological split\n",
        "    n = X_num.shape[0]\n",
        "    train_n = int(n * 0.8)\n",
        "    Xn_tr = X_num[:train_n]; Xe_tr = X_emb[:train_n]; y_tr = y[:train_n]\n",
        "    Xn_te = X_num[train_n:]; Xe_te = X_emb[train_n:]; y_te = y[train_n:]\n",
        "\n",
        "    # model\n",
        "    num_input_size = Xn_tr.shape[2]\n",
        "    emb_input_size = Xe_tr.shape[2]\n",
        "    model = AttentionFusionModel(num_input_size=num_input_size, emb_input_size=emb_input_size,\n",
        "                                 enc_hidden=64, attn_dim=args.attn_dim, n_heads=args.n_heads,\n",
        "                                 head_hidden=64, dropout=0.1).to(device)\n",
        "\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # data loaders\n",
        "    train_ds = TensorDataset(torch.from_numpy(Xn_tr).float(), torch.from_numpy(Xe_tr).float(), torch.from_numpy(y_tr).float())\n",
        "    test_ds = TensorDataset(torch.from_numpy(Xn_te).float(), torch.from_numpy(Xe_te).float(), torch.from_numpy(y_te).float())\n",
        "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "    if args.debug:\n",
        "        print(f\"Training for {args.epochs} epochs | train samples: {len(train_ds)} | test samples: {len(test_ds)}\")\n",
        "\n",
        "    # training loop\n",
        "    model.train()\n",
        "    for epoch in range(args.epochs):\n",
        "        total_loss = 0.0\n",
        "        for xb_num, xb_emb, yb in train_loader:\n",
        "            xb_num = xb_num.to(device)\n",
        "            xb_emb = xb_emb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            out = model(xb_num, xb_emb)\n",
        "            loss = criterion(out, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item() * xb_num.size(0)\n",
        "        avg_loss = total_loss / len(train_ds)\n",
        "        if args.debug:\n",
        "            print(f\"[Epoch {epoch+1}/{args.epochs}] train loss: {avg_loss:.6f}\")\n",
        "\n",
        "    # evaluation\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    trues = []\n",
        "    with torch.no_grad():\n",
        "        for xb_num, xb_emb, yb in test_loader:\n",
        "            xb_num = xb_num.to(device)\n",
        "            xb_emb = xb_emb.to(device)\n",
        "            out = model(xb_num, xb_emb)\n",
        "            preds.extend(out.cpu().numpy().tolist())\n",
        "            trues.extend(yb.numpy().tolist())\n",
        "\n",
        "    preds = np.array(preds)\n",
        "    trues = np.array(trues)\n",
        "\n",
        "    mae, rmse, da = compute_metrics(trues, preds)\n",
        "\n",
        "    # bootstrap CI\n",
        "    n_test_pts = len(trues)\n",
        "    n_boot = 1000\n",
        "    seed = 42\n",
        "    if n_test_pts >= 2:\n",
        "        rng = np.random.default_rng(seed)\n",
        "        mae_bs = np.empty(n_boot, dtype=float)\n",
        "        rmse_bs = np.empty(n_boot, dtype=float)\n",
        "        da_bs = np.empty(n_boot, dtype=float)\n",
        "        for i in range(n_boot):\n",
        "            idx = rng.integers(0, n_test_pts, size=n_test_pts)\n",
        "            t_sample = trues[idx]; p_sample = preds[idx]\n",
        "            m, r, d = compute_metrics(t_sample, p_sample)\n",
        "            mae_bs[i] = m; rmse_bs[i] = r; da_bs[i] = d\n",
        "        mae_lo, mae_hi = np.percentile(mae_bs, [2.5, 97.5]).tolist()\n",
        "        rmse_lo, rmse_hi = np.percentile(rmse_bs, [2.5, 97.5]).tolist()\n",
        "        da_lo, da_hi = np.percentile(da_bs, [2.5, 97.5]).tolist()\n",
        "    else:\n",
        "        mae_lo = mae_hi = float(mae)\n",
        "        rmse_lo = rmse_hi = float(rmse)\n",
        "        da_lo = da_hi = float(da)\n",
        "\n",
        "    out_metrics = {\n",
        "        \"MAE_mean\": float(mae), \"MAE_lo\": float(mae_lo), \"MAE_hi\": float(mae_hi),\n",
        "        \"RMSE_mean\": float(rmse), \"RMSE_lo\": float(rmse_lo), \"RMSE_hi\": float(rmse_hi),\n",
        "        \"DA_mean\": float(da), \"DA_lo\": float(da_lo), \"DA_hi\": float(da_hi),\n",
        "        \"n_test\": int(n_test_pts), \"bootstrap_samples\": int(n_boot) if n_test_pts>=2 else 0, \"bootstrap_seed\": int(seed) if n_test_pts>=2 else None\n",
        "    }\n",
        "\n",
        "    Path(args.out).parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(args.out, \"w\") as f:\n",
        "        json.dump({\"preds\": preds.tolist(), \"y_test\": trues.tolist(), \"metrics\": out_metrics}, f)\n",
        "\n",
        "    print(\"Saved predictions+metrics to\", args.out)\n",
        "    if args.debug:\n",
        "        print(\"Metrics:\", json.dumps(out_metrics, indent=2))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rgjgdb_kjgqq",
        "outputId": "059387d0-d724-4977-c05c-c6b7b0ec3173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/train_multimodal_attention_fusion.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#updated\n",
        "\n",
        "%%bash\n",
        "python src/train_multimodal_attention_fusion.py \\\n",
        "  --features data/processed/aapl_features.parquet \\\n",
        "  --text_emb data/processed/text_embeddings.parquet \\\n",
        "  --seq_len 10 --emb_agg mean --emb_fill zero \\\n",
        "  --epochs 30 --batch_size 64 --device cpu --debug \\\n",
        "  --out outputs/multimodal_late_attention_preds.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJKPjHq5kE2q",
        "outputId": "1cdab6ee-5498-46f7-b58b-ce3da1e24b2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Found existing target column: ret\n",
            "Numeric feature columns (sample): ['Close', 'High', 'Low', 'Open', 'Volume', 'close_next', 'ret_next', 'ret_lag_1', 'ret_lag_2', 'ret_lag_3']  total: 15\n",
            "Embedding dim detected: 384\n",
            "X_num shape: (2192, 10, 15) X_emb shape: (2192, 10, 384) y shape: (2192,)\n",
            "Training for 30 epochs | train samples: 1753 | test samples: 439\n",
            "[Epoch 1/30] train loss: 0.000654\n",
            "[Epoch 2/30] train loss: 0.000563\n",
            "[Epoch 3/30] train loss: 0.000539\n",
            "[Epoch 4/30] train loss: 0.000530\n",
            "[Epoch 5/30] train loss: 0.000499\n",
            "[Epoch 6/30] train loss: 0.000488\n",
            "[Epoch 7/30] train loss: 0.000504\n",
            "[Epoch 8/30] train loss: 0.000493\n",
            "[Epoch 9/30] train loss: 0.000501\n",
            "[Epoch 10/30] train loss: 0.000483\n",
            "[Epoch 11/30] train loss: 0.000490\n",
            "[Epoch 12/30] train loss: 0.000473\n",
            "[Epoch 13/30] train loss: 0.000490\n",
            "[Epoch 14/30] train loss: 0.000475\n",
            "[Epoch 15/30] train loss: 0.000472\n",
            "[Epoch 16/30] train loss: 0.000464\n",
            "[Epoch 17/30] train loss: 0.000483\n",
            "[Epoch 18/30] train loss: 0.000497\n",
            "[Epoch 19/30] train loss: 0.000461\n",
            "[Epoch 20/30] train loss: 0.000458\n",
            "[Epoch 21/30] train loss: 0.000459\n",
            "[Epoch 22/30] train loss: 0.000461\n",
            "[Epoch 23/30] train loss: 0.000454\n",
            "[Epoch 24/30] train loss: 0.000460\n",
            "[Epoch 25/30] train loss: 0.000457\n",
            "[Epoch 26/30] train loss: 0.000465\n",
            "[Epoch 27/30] train loss: 0.000451\n",
            "[Epoch 28/30] train loss: 0.000452\n",
            "[Epoch 29/30] train loss: 0.000452\n",
            "[Epoch 30/30] train loss: 0.000458\n",
            "Saved predictions+metrics to outputs/multimodal_late_attention_preds.json\n",
            "Metrics: {\n",
            "  \"MAE_mean\": 0.012991831530685588,\n",
            "  \"MAE_lo\": 0.011922525038870284,\n",
            "  \"MAE_hi\": 0.014071642423027749,\n",
            "  \"RMSE_mean\": 0.017375030765526003,\n",
            "  \"RMSE_lo\": 0.01578540305364318,\n",
            "  \"RMSE_hi\": 0.018910736004196613,\n",
            "  \"DA_mean\": 49.65831435079727,\n",
            "  \"DA_lo\": 44.86902050113895,\n",
            "  \"DA_hi\": 54.21412300683372,\n",
            "  \"n_test\": 439,\n",
            "  \"bootstrap_samples\": 1000,\n",
            "  \"bootstrap_seed\": 42\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python src/train_multimodal_attention_fusion.py \\\n",
        "  --features data/processed/aapl_features.parquet \\\n",
        "  --text_emb data/processed/text_embeddings.parquet \\\n",
        "  --seq_len 10 \\\n",
        "  --emb_agg mean \\\n",
        "  --emb_fill hybrid \\\n",
        "  --emb_ffill_limit 3 \\\n",
        "  --epochs 30 \\\n",
        "  --batch_size 64 \\\n",
        "  --lr 5e-4 \\\n",
        "  --attn_dim 64 \\\n",
        "  --n_heads 4 \\\n",
        "  --device cpu \\\n",
        "  --debug \\\n",
        "  --out outputs/multimodal_attention_fusion_preds.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UAE3GnfmpV1",
        "outputId": "57a25312-1cef-461a-c5d2-95c66adccf07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Found existing target column: ret\n",
            "Numeric feature columns (sample): ['Close', 'High', 'Low', 'Open', 'Volume', 'close_next', 'ret_next', 'ret_lag_1', 'ret_lag_2', 'ret_lag_3']  total: 15\n",
            "Embedding dim detected: 384\n",
            "X_num shape: (2192, 10, 15) X_emb shape: (2192, 10, 384) y shape: (2192,)\n",
            "Training for 30 epochs | train samples: 1753 | test samples: 439\n",
            "[Epoch 1/30] train loss: 0.000519\n",
            "[Epoch 2/30] train loss: 0.000462\n",
            "[Epoch 3/30] train loss: 0.000459\n",
            "[Epoch 4/30] train loss: 0.000454\n",
            "[Epoch 5/30] train loss: 0.000450\n",
            "[Epoch 6/30] train loss: 0.000448\n",
            "[Epoch 7/30] train loss: 0.000450\n",
            "[Epoch 8/30] train loss: 0.000448\n",
            "[Epoch 9/30] train loss: 0.000448\n",
            "[Epoch 10/30] train loss: 0.000448\n",
            "[Epoch 11/30] train loss: 0.000448\n",
            "[Epoch 12/30] train loss: 0.000448\n",
            "[Epoch 13/30] train loss: 0.000448\n",
            "[Epoch 14/30] train loss: 0.000448\n",
            "[Epoch 15/30] train loss: 0.000448\n",
            "[Epoch 16/30] train loss: 0.000448\n",
            "[Epoch 17/30] train loss: 0.000448\n",
            "[Epoch 18/30] train loss: 0.000448\n",
            "[Epoch 19/30] train loss: 0.000448\n",
            "[Epoch 20/30] train loss: 0.000448\n",
            "[Epoch 21/30] train loss: 0.000448\n",
            "[Epoch 22/30] train loss: 0.000448\n",
            "[Epoch 23/30] train loss: 0.000448\n",
            "[Epoch 24/30] train loss: 0.000448\n",
            "[Epoch 25/30] train loss: 0.000448\n",
            "[Epoch 26/30] train loss: 0.000448\n",
            "[Epoch 27/30] train loss: 0.000448\n",
            "[Epoch 28/30] train loss: 0.000448\n",
            "[Epoch 29/30] train loss: 0.000448\n",
            "[Epoch 30/30] train loss: 0.000448\n",
            "Saved predictions+metrics to outputs/multimodal_attention_fusion_preds.json\n",
            "Metrics: {\n",
            "  \"MAE_mean\": 0.01147850582582037,\n",
            "  \"MAE_lo\": 0.010478664166328014,\n",
            "  \"MAE_hi\": 0.012461741062603792,\n",
            "  \"RMSE_mean\": 0.015851840479047856,\n",
            "  \"RMSE_lo\": 0.014285613326345448,\n",
            "  \"RMSE_hi\": 0.0174470633058129,\n",
            "  \"DA_mean\": 49.65831435079727,\n",
            "  \"DA_lo\": 44.86902050113895,\n",
            "  \"DA_hi\": 54.21412300683372,\n",
            "  \"n_test\": 439,\n",
            "  \"bootstrap_samples\": 1000,\n",
            "  \"bootstrap_seed\": 42\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python src/eval2.py \\\n",
        "  --preds_json outputs/multimodal_attention_fusion_preds.json \\\n",
        "  --out outputs/multimodal_attention_fusion_metrics.json \\\n",
        "  --n_boot 1000 \\\n",
        "  --seed 42"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDLJVVaqnHtl",
        "outputId": "8f395b37-5fa6-4a52-973a-92ef4219bef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote metrics to outputs/multimodal_attention_fusion_metrics.json\n",
            "{\n",
            "  \"MAE_mean\": 0.011449459252043995,\n",
            "  \"MAE_lo\": 0.010441709607725804,\n",
            "  \"MAE_hi\": 0.012518504493698167,\n",
            "  \"RMSE_mean\": 0.015783494495048442,\n",
            "  \"RMSE_lo\": 0.014224876911706139,\n",
            "  \"RMSE_hi\": 0.017449858891357153,\n",
            "  \"DA_mean\": 49.658769931662874,\n",
            "  \"DA_lo\": 45.3246013667426,\n",
            "  \"DA_hi\": 54.441913439635535,\n",
            "  \"n_test\": 439,\n",
            "  \"bootstrap_samples\": 1000,\n",
            "  \"bootstrap_seed\": 42\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "BDxX5Yl6zBsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba341c35",
        "outputId": "31f32c49-0e4a-44a9-a2e4-12b1626d9f8e"
      },
      "source": [
        "import json\n",
        "\n",
        "# 1. Define a dictionary named metrics_data to store the extracted metrics\n",
        "metrics_data = {}\n",
        "\n",
        "# 2. Parse the JSON output from cell 0ZszrbKaa1q0 for 'seasonal_naive' and 'ridge_baseline'\n",
        "# The output from cell 0ZszrbKaa1q0 is available in the kernel state as `metrics_json_string`\n",
        "# However, for robustness and to follow the spirit of loading outputs, I will parse the raw string directly.\n",
        "# If the output changes to a file, this part would need adjustment to read from a file.\n",
        "# For now, let's assume the previous cell's output is directly convertible to a dictionary.\n",
        "# The `metrics_data_from_comparison` variable holds the parsed JSON from cell 0ZszrbKaa1q0.\n",
        "metrics_data_from_comparison = json.loads(metrics_json_string)\n",
        "metrics_data['seasonal_naive'] = metrics_data_from_comparison['seasonal_naive']\n",
        "metrics_data['ridge_baseline'] = metrics_data_from_comparison['ridge_baseline']\n",
        "\n",
        "# 3. Load the outputs/multimodal_lstm_metrics.json file for 'multimodal_early_fusion'\n",
        "with open('outputs/multimodal_lstm_metrics.json', 'r') as f:\n",
        "    multimodal_lstm_metrics = json.load(f)\n",
        "metrics_data['multimodal_early_fusion'] = multimodal_lstm_metrics['metrics']\n",
        "\n",
        "# 4. Load the outputs/multimodal_late_fusion_metrics.json file for 'multimodal_late_fusion'\n",
        "with open('outputs/multimodal_late_fusion_metrics.json', 'r') as f:\n",
        "    late_fusion_metrics = json.load(f)\n",
        "metrics_data['multimodal_late_fusion'] = late_fusion_metrics['metrics']\n",
        "\n",
        "# 5. Load the outputs/multimodal_attention_fusion_metrics.json file for 'multimodal_attention_fusion'\n",
        "with open('outputs/multimodal_attention_fusion_metrics.json', 'r') as f:\n",
        "    attention_fusion_metrics = json.load(f)\n",
        "metrics_data['multimodal_attention_fusion'] = attention_fusion_metrics['metrics']\n",
        "\n",
        "print(\"Metrics loaded successfully for all models.\")\n",
        "print(json.dumps(metrics_data, indent=2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics loaded successfully for all models.\n",
            "{\n",
            "  \"seasonal_naive\": {\n",
            "    \"MAE_mean\": 0.017733695857360576,\n",
            "    \"MAE_lo\": 0.015843833773274793,\n",
            "    \"MAE_hi\": 0.019623557941446358,\n",
            "    \"RMSE_mean\": 0.023151427251859143,\n",
            "    \"RMSE_lo\": 0.02059742925307313,\n",
            "    \"RMSE_hi\": 0.025705425250645158,\n",
            "    \"DA_mean\": 49.71014492753623,\n",
            "    \"DA_lo\": 46.96441334605667,\n",
            "    \"DA_hi\": 52.45587650901578\n",
            "  },\n",
            "  \"ridge_baseline\": {\n",
            "    \"MAE_mean\": 0.012199034394008003,\n",
            "    \"MAE_lo\": 0.011022186807270566,\n",
            "    \"MAE_hi\": 0.013375881980745439,\n",
            "    \"RMSE_mean\": 0.016519844699163334,\n",
            "    \"RMSE_lo\": 0.014884440957546752,\n",
            "    \"RMSE_hi\": 0.018155248440779915,\n",
            "    \"DA_mean\": 50.362318840579704,\n",
            "    \"DA_lo\": 46.81180730245322,\n",
            "    \"DA_hi\": 53.91283037870619\n",
            "  },\n",
            "  \"multimodal_early_fusion\": {\n",
            "    \"MAE_mean\": 0.011431647198393038,\n",
            "    \"MAE_lo\": 0.010431341032119302,\n",
            "    \"MAE_hi\": 0.012496913065677908,\n",
            "    \"RMSE_mean\": 0.015759469470166664,\n",
            "    \"RMSE_lo\": 0.014196751696569017,\n",
            "    \"RMSE_hi\": 0.017446440392134114,\n",
            "    \"DA_mean\": 49.658769931662874,\n",
            "    \"DA_lo\": 45.3246013667426,\n",
            "    \"DA_hi\": 54.441913439635535,\n",
            "    \"n_test\": 439,\n",
            "    \"bootstrap_samples\": 1000,\n",
            "    \"bootstrap_seed\": 42\n",
            "  },\n",
            "  \"multimodal_late_fusion\": {\n",
            "    \"MAE_mean\": 0.011443764819123018,\n",
            "    \"MAE_lo\": 0.010447056805919487,\n",
            "    \"MAE_hi\": 0.012493952754084603,\n",
            "    \"RMSE_mean\": 0.015729491459482225,\n",
            "    \"RMSE_lo\": 0.01412872916399385,\n",
            "    \"RMSE_hi\": 0.0173640916644696,\n",
            "    \"DA_mean\": 53.80273348519363,\n",
            "    \"DA_lo\": 49.19703872437358,\n",
            "    \"DA_hi\": 58.54783599088837,\n",
            "    \"n_test\": 439,\n",
            "    \"bootstrap_samples\": 1000,\n",
            "    \"bootstrap_seed\": 42\n",
            "  },\n",
            "  \"multimodal_attention_fusion\": {\n",
            "    \"MAE_mean\": 0.011449459252043995,\n",
            "    \"MAE_lo\": 0.010441709607725804,\n",
            "    \"MAE_hi\": 0.012518504493698167,\n",
            "    \"RMSE_mean\": 0.015783494495048442,\n",
            "    \"RMSE_lo\": 0.014224876911706139,\n",
            "    \"RMSE_hi\": 0.017449858891357153,\n",
            "    \"DA_mean\": 49.658769931662874,\n",
            "    \"DA_lo\": 45.3246013667426,\n",
            "    \"DA_hi\": 54.441913439635535,\n",
            "    \"n_test\": 439,\n",
            "    \"bootstrap_samples\": 1000,\n",
            "    \"bootstrap_seed\": 42\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "8bd3b63c",
        "outputId": "808c7620-298f-484e-c8ae-43ede41a9988"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Initialize lists to store extracted data\n",
        "models = []\n",
        "metrics_type = []\n",
        "mean_values = []\n",
        "lower_cis = []\n",
        "upper_cis = []\n",
        "\n",
        "# Iterate through the metrics_data dictionary to populate the lists\n",
        "for model_name, model_metrics in metrics_data.items():\n",
        "    for metric_prefix in ['MAE', 'RMSE', 'DA']:\n",
        "        models.append(model_name)\n",
        "        metrics_type.append(metric_prefix)\n",
        "        mean_values.append(model_metrics[f'{metric_prefix}_mean'])\n",
        "        lower_cis.append(model_metrics[f'{metric_prefix}_lo'])\n",
        "        upper_cis.append(model_metrics[f'{metric_prefix}_hi'])\n",
        "\n",
        "# Create a DataFrame from the collected data\n",
        "df_all_metrics = pd.DataFrame({\n",
        "    'Model': models,\n",
        "    'Metric': metrics_type,\n",
        "    'Mean': mean_values,\n",
        "    'Lower_CI': lower_cis,\n",
        "    'Upper_CI': upper_cis\n",
        "})\n",
        "\n",
        "# Rename models for better readability in plots with abbreviations\n",
        "df_all_metrics['Model'] = df_all_metrics['Model'].replace({\n",
        "    'seasonal_naive': 'SN',\n",
        "    'ridge_baseline': 'RB',\n",
        "    'multimodal_early_fusion': 'MEF',\n",
        "    'multimodal_late_fusion': 'MLF',\n",
        "    'multimodal_attention_fusion': 'MAF'\n",
        "})\n",
        "\n",
        "print(\"DataFrame created successfully:\")\n",
        "display(df_all_metrics.head(15))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame created successfully:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Model Metric       Mean   Lower_CI   Upper_CI\n",
              "0     SN    MAE   0.017734   0.015844   0.019624\n",
              "1     SN   RMSE   0.023151   0.020597   0.025705\n",
              "2     SN     DA  49.710145  46.964413  52.455877\n",
              "3     RB    MAE   0.012199   0.011022   0.013376\n",
              "4     RB   RMSE   0.016520   0.014884   0.018155\n",
              "5     RB     DA  50.362319  46.811807  53.912830\n",
              "6    MEF    MAE   0.011432   0.010431   0.012497\n",
              "7    MEF   RMSE   0.015759   0.014197   0.017446\n",
              "8    MEF     DA  49.658770  45.324601  54.441913\n",
              "9    MLF    MAE   0.011444   0.010447   0.012494\n",
              "10   MLF   RMSE   0.015729   0.014129   0.017364\n",
              "11   MLF     DA  53.802733  49.197039  58.547836\n",
              "12   MAF    MAE   0.011449   0.010442   0.012519\n",
              "13   MAF   RMSE   0.015783   0.014225   0.017450\n",
              "14   MAF     DA  49.658770  45.324601  54.441913"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-772b51cc-a50f-4a8e-8f85-84ecef2afa8d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Metric</th>\n",
              "      <th>Mean</th>\n",
              "      <th>Lower_CI</th>\n",
              "      <th>Upper_CI</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SN</td>\n",
              "      <td>MAE</td>\n",
              "      <td>0.017734</td>\n",
              "      <td>0.015844</td>\n",
              "      <td>0.019624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SN</td>\n",
              "      <td>RMSE</td>\n",
              "      <td>0.023151</td>\n",
              "      <td>0.020597</td>\n",
              "      <td>0.025705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SN</td>\n",
              "      <td>DA</td>\n",
              "      <td>49.710145</td>\n",
              "      <td>46.964413</td>\n",
              "      <td>52.455877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RB</td>\n",
              "      <td>MAE</td>\n",
              "      <td>0.012199</td>\n",
              "      <td>0.011022</td>\n",
              "      <td>0.013376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RB</td>\n",
              "      <td>RMSE</td>\n",
              "      <td>0.016520</td>\n",
              "      <td>0.014884</td>\n",
              "      <td>0.018155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>RB</td>\n",
              "      <td>DA</td>\n",
              "      <td>50.362319</td>\n",
              "      <td>46.811807</td>\n",
              "      <td>53.912830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>MEF</td>\n",
              "      <td>MAE</td>\n",
              "      <td>0.011432</td>\n",
              "      <td>0.010431</td>\n",
              "      <td>0.012497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>MEF</td>\n",
              "      <td>RMSE</td>\n",
              "      <td>0.015759</td>\n",
              "      <td>0.014197</td>\n",
              "      <td>0.017446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>MEF</td>\n",
              "      <td>DA</td>\n",
              "      <td>49.658770</td>\n",
              "      <td>45.324601</td>\n",
              "      <td>54.441913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>MLF</td>\n",
              "      <td>MAE</td>\n",
              "      <td>0.011444</td>\n",
              "      <td>0.010447</td>\n",
              "      <td>0.012494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>MLF</td>\n",
              "      <td>RMSE</td>\n",
              "      <td>0.015729</td>\n",
              "      <td>0.014129</td>\n",
              "      <td>0.017364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>MLF</td>\n",
              "      <td>DA</td>\n",
              "      <td>53.802733</td>\n",
              "      <td>49.197039</td>\n",
              "      <td>58.547836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>MAF</td>\n",
              "      <td>MAE</td>\n",
              "      <td>0.011449</td>\n",
              "      <td>0.010442</td>\n",
              "      <td>0.012519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>MAF</td>\n",
              "      <td>RMSE</td>\n",
              "      <td>0.015783</td>\n",
              "      <td>0.014225</td>\n",
              "      <td>0.017450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>MAF</td>\n",
              "      <td>DA</td>\n",
              "      <td>49.658770</td>\n",
              "      <td>45.324601</td>\n",
              "      <td>54.441913</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-772b51cc-a50f-4a8e-8f85-84ecef2afa8d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-772b51cc-a50f-4a8e-8f85-84ecef2afa8d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-772b51cc-a50f-4a8e-8f85-84ecef2afa8d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ac0f5343-9680-4ec0-8a49-61491de12661\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ac0f5343-9680-4ec0-8a49-61491de12661')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ac0f5343-9680-4ec0-8a49-61491de12661 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df_all_metrics\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"RB\",\n          \"MAF\",\n          \"MEF\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Metric\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"MAE\",\n          \"RMSE\",\n          \"DA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 24.720304446707786,\n        \"min\": 0.011431647198393038,\n        \"max\": 53.80273348519363,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.011443764819123018,\n          53.80273348519363,\n          0.017733695857360576\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Lower_CI\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 22.8083885841293,\n        \"min\": 0.010431341032119302,\n        \"max\": 49.19703872437358,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.010447056805919487,\n          49.19703872437358,\n          0.015843833773274793\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Upper_CI\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 26.739546749052195,\n        \"min\": 0.012493952754084603,\n        \"max\": 58.54783599088837,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.012493952754084603,\n          58.54783599088837,\n          0.019623557941446358\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7cc64498",
        "outputId": "b5800731-aa3b-4c0d-fbc1-4ef2ff058d14"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Set a consistent style for the plots\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# --- Plotting MAE ---\n",
        "plt.figure(figsize=(8, 6))\n",
        "df_mae = df_all_metrics[df_all_metrics['Metric'] == 'MAE'].copy()\n",
        "\n",
        "# Do not sort for plotting to keep original order from df_all_metrics\n",
        "\n",
        "x_pos = np.arange(len(df_mae['Model']))\n",
        "ax_mae = plt.bar(x_pos, df_mae['Mean'], color='steelblue')\n",
        "\n",
        "# Calculate asymmetric error bars relative to the mean\n",
        "mae_yerr_lower = df_mae['Mean'] - df_mae['Lower_CI']\n",
        "mae_yerr_upper = df_mae['Upper_CI'] - df_mae['Mean']\n",
        "\n",
        "plt.errorbar(x_pos, df_mae['Mean'], yerr=[mae_yerr_lower, mae_yerr_upper], fmt='none', capsize=5, color='black')\n",
        "plt.title('Model Performance: Mean Absolute Error (MAE) with 95% CI')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('MAE Mean Value')\n",
        "plt.xticks(x_pos, df_mae['Model'], rotation=45, ha='right')\n",
        "plt.gca().grid(False) # Turn off grid lines for this plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Plotting RMSE ---\n",
        "plt.figure(figsize=(8, 6))\n",
        "df_rmse = df_all_metrics[df_all_metrics['Metric'] == 'RMSE'].copy()\n",
        "\n",
        "# Do not sort for plotting\n",
        "\n",
        "x_pos = np.arange(len(df_rmse['Model']))\n",
        "ax_rmse = plt.bar(x_pos, df_rmse['Mean'], color='steelblue')\n",
        "\n",
        "# Calculate asymmetric error bars relative to the mean\n",
        "rmse_yerr_lower = df_rmse['Mean'] - df_rmse['Lower_CI']\n",
        "rmse_yerr_upper = df_rmse['Upper_CI'] - df_rmse['Mean']\n",
        "\n",
        "plt.errorbar(x_pos, df_rmse['Mean'], yerr=[rmse_yerr_lower, rmse_yerr_upper], fmt='none', capsize=5, color='black')\n",
        "plt.title('Model Performance: Root Mean Squared Error (RMSE) with 95% CI')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('RMSE Mean Value')\n",
        "plt.xticks(x_pos, df_rmse['Model'], rotation=45, ha='right')\n",
        "plt.gca().grid(False) # Turn off grid lines for this plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Plotting DA ---\n",
        "plt.figure(figsize=(8, 6))\n",
        "df_da = df_all_metrics[df_all_metrics['Metric'] == 'DA'].copy()\n",
        "\n",
        "# Do not sort for plotting\n",
        "\n",
        "x_pos = np.arange(len(df_da['Model']))\n",
        "ax_da = plt.bar(x_pos, df_da['Mean'], color='steelblue')\n",
        "\n",
        "# Calculate asymmetric error bars relative to the mean\n",
        "da_yerr_lower = df_da['Mean'] - df_da['Lower_CI']\n",
        "da_yerr_upper = df_da['Upper_CI'] - df_da['Mean']\n",
        "\n",
        "plt.errorbar(x_pos, df_da['Mean'], yerr=[da_yerr_lower, da_yerr_upper], fmt='none', capsize=5, color='black')\n",
        "plt.title('Model Performance: Directional Accuracy (DA) with 95% CI')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('DA Mean Value (%)')\n",
        "plt.xticks(x_pos, df_da['Model'], rotation=45, ha='right')\n",
        "plt.ylim(min(df_da['Lower_CI']) - 5, max(df_da['Upper_CI']) + 5) # Adjust y-lim for better visibility\n",
        "plt.gca().grid(False) # Turn off grid lines for this plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Summary of performance differences ---\n",
        "summary = \"\"\"\n",
        "Performance Comparison Summary:\n",
        "\n",
        "MAE (Mean Absolute Error):\n",
        "\"\"\"\n",
        "df_mae_sorted = df_mae.sort_values(by='Mean')\n",
        "summary += f\"- Best performing model: {df_mae_sorted.iloc[0]['Model']} with MAE_mean={df_mae_sorted.iloc[0]['Mean']:.4f}\\n\"\n",
        "summary += f\"- Worst performing model: {df_mae_sorted.iloc[-1]['Model']} with MAE_mean={df_mae_sorted.iloc[-1]['Mean']:.4f}\\n\"\n",
        "summary += \"- Multimodal Late Fusion shows the lowest MAE, closely followed by Multimodal Early Fusion and Multimodal Attention Fusion. Ridge Baseline is also strong, while Seasonal Naive is significantly worse.\\n\\n\"\n",
        "\n",
        "summary += \"RMSE (Root Mean Squared Error):\\n\"\n",
        "df_rmse_sorted = df_rmse.sort_values(by='Mean')\n",
        "summary += f\"- Best performing model: {df_rmse_sorted.iloc[0]['Model']} with RMSE_mean={df_rmse_sorted.iloc[0]['Mean']:.4f}\\n\"\n",
        "summary += f\"- Worst performing model: {df_rmse_sorted.iloc[-1]['Model']} with RMSE_mean={df_rmse_sorted.iloc[-1]['Mean']:.4f}\\n\"\n",
        "summary += \"- Multimodal Late Fusion has the lowest RMSE, indicating better handling of larger errors. Multimodal Early Fusion and Multimodal Attention Fusion are very similar. Ridge Baseline is competitive, and Seasonal Naive is the worst.\\n\\n\"\n",
        "\n",
        "summary += \"DA (Directional Accuracy):\\n\"\n",
        "df_da_sorted = df_da.sort_values(by='Mean', ascending=False)\n",
        "summary += f\"- Best performing model: {df_da_sorted.iloc[0]['Model']} with DA_mean={df_da_sorted.iloc[0]['Mean']:.2f}%\\n\"\n",
        "summary += f\"- Worst performing model: {df_da_sorted.iloc[-1]['Model']} with DA_mean={df_da_sorted.iloc[-1]['Mean']:.2f}%\\n\"\n",
        "summary += \"- Multimodal Late Fusion achieves the highest directional accuracy, indicating a better ability to predict the direction of stock price movements. Other models show comparable DA around 50%, with Seasonal Naive slightly underperforming in this metric.\\n\"\n",
        "\n",
        "print(summary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxAAAAJICAYAAADxUwLTAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAY4RJREFUeJzt3XdcVvX///HnxXLhwi2OUgO34t4orlyYW1MaVqg5Gq7c2HCVuXOkpuWeJWji1twj0z6amVZuScMBCIJwfn/44/p6CeYBwQvxcb/dvMl1xnVe57oOh/M8533ex2IYhiEAAAAAMMHB3gUAAAAAeHYQIAAAAACYRoAAAAAAYBoBAgAAAIBpBAgAAAAAphEgAAAAAJhGgAAAAABgGgECAAAAgGkECAAAAACmESCAJ+Dp6alp06Yleb6LFy/K09NTa9asSYWqku/777/Xyy+/rDJlyqhKlSr2Lgf/n4+Pjz766CN7l2EjLdU0bdo0eXp62ruM50pAQIDefPPNp7a8M2fOqHTp0jp9+nSqLsfPz09+fn6mp23ZsmWq1gOkVQQIPPPWrFkjT09PeXp66vDhwwnGG4Yhb29veXp6qkePHnaoMPkOHDhgXTdPT0+VKVNGDRs21KBBg3ThwoUUXdbZs2c1ZMgQFSlSRJ988ok+/vjjFH1/3Ofn52f9PkuWLKlKlSqpadOmGjhwoPbs2WPv8mz8/PPPmjZtmm7fvm3vUlLce++9J09PT33++ef2LuWZc+HCBa1atcpmfxp/UuTBfVX16tXVuXNnffnll7p8+fJ/vueECRPk6emp999/P9HxJUqUkLe3t6ZOnZqSq/JYISEhmjZtmn777bcUf++YmBhNnz5dDRs2VNmyZdWwYUN99dVXunfvns10D/8dePDfL7/8YjPtsmXL5OPjo2rVqmngwIEKDw+3GR8XF6dXXnlFs2bNSlKtd+/e1YIFC9ShQwdVrlxZ5cqVU9OmTfXxxx/rr7/+sk4XH+ZDQ0OT9mHgmeNk7wKAlJIhQwYFBQUlOHN+8OBBXb16VS4uLnaq7Mn5+fmpXLlyunfvnk6ePKnly5dr586dWrdunfLly5ciyzh48KDi4uI0bNgwFS1aNEXeE4nLnz+/PvzwQ0lSZGSkzp07p82bN2vdunVq1qyZPv/8czk7O1un37hxoywWy1Ov8+jRo5o+fbratGmjbNmy2YyzV00pITw8XNu3b5e7u7vWr1+vAQMGPLPrYg/ffvut3N3dVaNGjQTjWrZsqXr16skwDN26dUu//vqrFi5cqG+//VafffaZWrRokWAewzC0fv16ubu7a/v27QoPD5erq2uC6Tp37ix/f3+dP39eRYoUSZV1mzdvns3rf/75R9OnT5e7u7tKlSqVossaOHCgNm7cqHbt2qls2bI6duyYpkyZoitXruiTTz5JMH3834EHPfg5HD58WAEBAfLz81PhwoU1Z84cTZgwweZk0IoVKxQWFqbu3bubrjM0NFRvv/22Tpw4oQYNGqhly5bKnDmz/vrrL23YsEErVqzQ//73v2R8AniWESCQbnh7e2vjxo0aPny4nJz+b9MOCgpSmTJldPPmTfsV94SqVKmil19+WZLUrl07vfDCC/r000/1/fffP/FVlTt37ihz5sz6999/JUlZs2Z94nrjRUZGKlOmTCn2fulF1qxZ1bp1a5thAwYM0KeffqolS5bI3d1dAwcOtI4zE37jv8en5VkO5MHBwYqLi9OYMWP0+uuv69ChQ6pWrZq9y0rAMAzdvXtXGTNmtHcpVjExMQoMDFTnzp0THV+6dOkE2/alS5fUvXt3DR48WMWLF1fJkiVtxh84cEBXr17VwoUL9fbbb2vz5s1q06ZNgveuVauWsmfPrrVr1+q9995LuZV6wNParo8fP64ff/xR7777rnVdunTpopw5c+qbb75R165dE3xOD/4dSMyOHTtUrVo1DRs2TJLk6uqqL7/80hogbt++rcmTJ+vjjz9O0noOGTJEv/32m6ZOnaqmTZvajHv//fc1adIk0++F9IMmTEg3WrRooZs3b9o0A4mOjlZwcLBatWqV6Dx37tzRuHHj5O3trbJly6pp06aaN2+eDMOwmS46OlpjxoxRjRo15OXlpZ49e+rq1auJvmdISIiGDBmiWrVqqWzZsmrRooVWrVqVcisqWc/8Xbx40Tps586devXVV1WxYkV5eXnJ399ff/zxh818H330kby8vHT+/Hm988478vLy0oABA+Tj42O9l6NmzZoJ7u1YvHixWrRoobJly6pOnToaPXp0gmYt8e2B//e//6lr166qUKGCvvzyS2vThnnz5mnx4sVq2LChKlSooO7du+vKlSsyDEMzZsxQvXr1VL58efXq1StB2NuyZYv8/f1Vp04dlS1bVo0aNdKMGTMUGxubaA1nzpyRn5+fKlSooLp16+rrr79O8BnevXtX06ZNU9OmTVWuXDnVqVNHffr00fnz563TxMXFacGCBWrRooXKlSunWrVqaeTIkbp165bNe4WFhens2bMKCwt73Ff3SI6Ojho+fLhKlCihxYsX27zXw/cbxDfbO3jwoAICAlSzZk15e3tbx5vZFqT7zdbee+891ahRQ+XLl1fTpk2tBwPTpk3ThAkTJEkNGza0NpmI3+YSuwfiwoUL6tevn6pVq6YKFSqoY8eO2rFjh8008c0xNmzYoJkzZ6pevXoqV66cXn/9dZ07d85m2sOHD6tfv36qX7++ypYtK29vb40ZM0ZRUVHJ+IT/T2BgoGrVqqUaNWqoePHiCgwMTHS6//p84oWEhGjo0KHWbdPHx0ejRo1SdHS0pEffnxH/HT74O+zj46MePXrop59+Utu2bVW+fHktW7ZMkrR69Wq99tprqlmzpsqWLavmzZtryZIlida9c+dOdevWTV5eXqpUqZLatWtnXcepU6eqTJkyiTYxGTFihKpUqaK7d+8+8rM7cuSIbty4oVq1aj1ymoe5u7tr3LhxiomJSfR3MTAwUCVKlFCNGjVUs2bNR34fzs7OqlatmrZu3fqfyzt16pQ8PT1tpvvf//4nT0/PBMHk7bffVocOHayvH7wH4sCBA2rfvr2k+wfR8b8DD9+7ZmZ/87AjR45IUoIrMs2bN5dhGPrxxx8TnS88PDxBE6d4UVFRyp49u/V19uzZFRkZaX09bdo0eXh4qEmTJo+tL96xY8e0Y8cOtW/fPkF4kO4HrsGDB5t+P6QfXIFAuuHu7q6KFStq/fr11oOpXbt2KSwsTM2bN9d3331nM71hGOrVq5f1j0SpUqX0008/acKECdaDgnjDhg3TunXr1LJlS1WqVEn79++Xv79/ghquX7+ujh07ymKxqGvXrnJzc9OuXbs0bNgwhYeH64033kiRdY0/yM2RI4ek+zc/f/TRR6pTp44GDBigyMhILV26VK+++qrWrl2rQoUKWee9d++e3nrrLVWuXFmDBw9WxowZ1bZtW33//ffavHmzAgIClDlzZutBz7Rp0zR9+nTVqlVLXbp00V9//aWlS5fq119/1dKlS22a2ty8eVPvvPOOWrRoIV9fX+XKlcs6LjAwUDExMfLz89PNmzc1d+5cvf/++6pRo4YOHDigd955R+fOndOiRYs0fvx4jR071jrv2rVrlTlzZr355pvKnDmz9u/fr6lTpyo8PDzBH69bt27p7bffVuPGjdWsWTMFBwfriy++kIeHh3W7iI2NVY8ePbRv3z61aNFCr732miIiIrRnzx6dPn3a2ixg5MiRWrt2rdq2bSs/Pz9dvHhRixcv1smTJ23WffPmzRoyZIjGjh2rtm3bJvt7dXR0VIsWLTRlyhQdOXJE9evX/8/pR48eLTc3N/Xu3Vt37tyRZH5bOHXqlLp27SonJyd16tRJ7u7uOn/+vLZt26YPPvhAjRs31t9//62goCANGTJEOXPmlCS5ubklWsv169fVuXNnRUZGys/PTzlz5tTatWvVq1cvTZ06VY0bN7aZ/uuvv5bFYlH37t0VHh6uuXPnasCAAVq5cqV1mo0bNyoqKkpdunRRjhw5dPz4cS1atEhXr15Ndlv4kJAQHThwQOPGjZN0/wBu4cKFGjFihM1Z2cd9PvHv1b59e4WFhaljx44qVqyYQkJCFBwcrKioqGSdzf7rr7/Uv39/derUSR07dtSLL74oSVq6dKleeukl+fj4yMnJSdu3b9fo0aNlGIa6du1qnX/NmjUaOnSoXnrpJfXo0UNZs2bVb7/9pp9++kmtWrVS69atNWPGDG3YsEHdunWzzhd/sqVJkybKkCHDI+s7evSoLBaLSpcunaT18vLyUpEiRbR3716b4dHR0dq0aZP1huwWLVpo6NChunbtmvLkyZPgfcqUKaOtW7c+spmTJHl4eChbtmw6fPiwGjZsKOl+GHVwcNCpU6es88bFxeno0aPq2LFjou9TvHhx9evXT1OnTlWnTp1UuXJlSVKlSpWs05jZ3yQmPmA+/FnHX7FNrEnQkCFDdOfOHTk6Oqpy5coaNGiQTZOmcuXKadWqVdq9e7cKFSqkb775RuXLl5d0P+QsW7bM5vfLjG3btklSgqtKgAzgGbd69WrDw8PDOH78uLFo0SLDy8vLiIyMNAzDMPr162f4+fkZhmEYDRo0MPz9/a3zbd682fDw8DC++uorm/fr27ev4enpaZw7d84wDMP47bffDA8PDyMgIMBmug8//NDw8PAwpk6dah02dOhQo3bt2kZoaKjNtB988IFRuXJla10XLlwwPDw8jNWrV//nuu3fv9/w8PAwVq1aZfz7779GSEiIsWPHDqNBgwaGp6encfz4cSM8PNyoUqWKMXz4cJt5r127ZlSuXNlm+ODBgw0PDw/jiy++SLCsqVOnGh4eHsa///5rHfbvv/8aZcqUMbp3727ExsZahy9atMhaV7xu3boZHh4extKlS23eN35da9SoYdy+fds6fOLEiYaHh4fh6+trxMTE2HyuZcqUMe7evWsdFv+5PWjEiBFGhQoVbKaLr2Ht2rXWYXfv3jVq165t9O3b1zps1apVhoeHh/HNN98keN+4uDjDMAzj0KFDhoeHh7Fu3Tqb8bt27UowPH4bfNz3GV9jixYtHjk+frtcuHChdViDBg2MwYMHJ1hely5djHv37lmHJ2Vb6Nq1q+Hl5WVcunQp0fU3DMOYO3eu4eHhYVy4cCFBnQ/X9NlnnxkeHh7GoUOHbOrx8fExGjRoYN1+4rfpZs2a2Xx3CxcuNDw8PIzff//dOiyx73327NmGp6enTd3x264Z8+bNM8qXL2+EhYUZhmEYf/31l+Hh4WFs3rzZZjozn8+gQYOMkiVLGsePH0+wnPjpHlVb/Hf44GfboEEDw8PDw9i1a1eC6RP7LLp37240bNjQ+vr27duGl5eX0aFDByMqKuqRdXfq1Mno0KGDzfhNmzYZHh4exv79+xMs50EDBgwwqlWrlmB4/O/53LlzHzlvr169DA8PD+tnbxiGsXHjRsPDw8P4+++/DcMwjLCwMKNcuXKJ/m4ahmEEBgYaHh4exrFjx/6zTn9/f6N9+/bW13369DH69OljlCpVyti5c6dhGIZx4sQJw8PDw9iyZYt1um7duhndunWzvj5+/Pgjf7fN7m8SExwcbHh4eBjff/+9zfClS5caHh4eRsuWLa3Djhw5YvTt29dYuXKlsWXLFmP27NlGtWrVjHLlyhknTpywTnfv3j2jT58+hoeHh+Hh4WF4e3sbp06dMgzj/rYycuTI/6wpMb179zY8PDyMW7dumZo+sb8jSJ9owoR0pVmzZrp79671RrwdO3Y8svnSrl275OjomKDLvu7du8swDO3atUvS/eYAkhJM9/rrr9u8NgxDmzZtko+PjwzDUGhoqPVfnTp1FBYWphMnTiRrvYYOHaqaNWuqbt268vf3V2RkpMaNG6dy5cpp7969un37tlq0aGGzTAcHB1WoUEEHDhxI8H5dunQxtdy9e/cqJiZGr732mhwc/m930aFDB7m6ulo/m3guLi6PPAP/8ssv29xfEX9mzNfX1+aelfLlyysmJkYhISHWYQ+2AQ8PD1doaKiqVKmiyMhI/fnnnzbLyZw5s83ZMhcXF5UrV86m16pNmzYpZ86cNmdg48XfTLtx40ZlzZpVtWvXtvlcy5Qpo8yZM9t8rm3bttXvv//+RFcfHqxfkiIiIh47bceOHeXo6Gh9bXZbCA0N1aFDh9SuXTsVLFgw0fVPqp07d6p8+fI2nRhkyZJFnTp10qVLl3TmzBmb6du2bWtzhj5+vge/pwe/9zt37ig0NFReXl4yDEMnT55MVp2BgYHy9va2nr1+4YUXVKZMGa1bt846jZnPJy4uTlu2bFGDBg0S3Nj64HRJVahQIdWtWzfB8Ac/i7CwMIWGhqpatWq6cOGCtbnbnj17FBERIX9//wRnth+sp3Xr1jp27JhNc73AwEAVKFDgsfeC3Lx506aZTFIktm0HBgaqbNmy1o4bXF1dVb9+/Uc2Y4q/mf/GjRv/uazKlSvr5MmT1itzR44cUb169VSyZElr86HDhw/LYrFYrywkd50et79JjLe3t9zd3TVhwgRt2rRJly5d0oYNGzRp0iQ5OTnZNNOrVKmSpk6dqvbt26thw4by9/fXihUrZLFYNHHiROt0jo6OmjZtmjZt2qTVq1crODjY2pTr+PHjeu+99xQSEqKePXuqTp066tmzp81+NjHxvThlyZIlOR8P0jGaMCFdcXNzU82aNRUUFKSoqCjFxsYm2m5Tun9jX968eRNcBi9evLh1fPz/Dg4OCXr9KFasmM3r0NBQ3b59W8uXL9fy5csTXWZyu7br3bu3qlSpIgcHB+XMmVPFixe3HnT//fffkhIGmngPr5+Tk5Py589varnxXS8+vK4uLi4qXLiw9TOKly9fvkc22yhQoIDN6/gw8ajht27dUuHChSVJf/zxhyZPnqz9+/cn6Jbw4fsO8ufPn+DgLXv27Pr999+tr8+fP68XX3zRJrg87Ny5cwoLC1PNmjUTHR9/03lKiz/gMfMH+8GmaZL5bSH+4MbDwyO5ZSZw+fJlVahQIcHw+G3n8uXLNst7+MA8/sDwwXtrLl++rKlTp2rbtm0J7jt5eDsw4+zZszp58qRat25tc79F9erVtXjxYmvTFjOfT2hoqMLDw/XSSy8luY7/8vB3Gu/IkSOaNm2afvnlF5t27dL934GsWbNaA8HjamrevLnGjBmjdevWqU+fPgoLC9P27dv1xhtvmAo+xkP3iJn18LZ9+/Zt6/0aD34flSpVUnBwsP766y9rE66kLrtKlSq6d++efvnlF+XPn1///vuvqlSpojNnzli7+z58+LBKlChhbQqaHGb2N4nJkCGDZs+erffff199+/aVdH+/OnDgQM2aNeuxHSIULVpUDRs21KZNmxQbG2tzIuHBXvSio6M1fvx49e7dW25ubnr11VeVJ08ezZo1S3PmzNGAAQMSNO99UPw+IyIiIkFPbHi+ESCQ7rRs2VIjRozQ9evXVa9evae204uLi5N0/4x6Yj2ISEr2w648PDweedNi/B/UCRMmJNpm+ME/LNL9P1IPXk1ISf/VW8zDdcR7VC3x63X79m1169ZNrq6u6tevn4oUKaIMGTLoxIkT+uKLL6yf++OWk1RxcXHKlSuXvvjii0THP+pegCcV/6AsM13pPnyWOanbgj097nuPjY3Vm2++aW1jXqxYMWXOnFkhISH66KOPEnzvZsRfZRg7dqzNPTbxgoOD1a5duyS/73951AH5wx0AxEvsd+j8+fN64403VKxYMX300UcqUKCAnJ2dtXPnTi1YsCDJn0X27NnVoEEDBQYGqk+fPtq4caOio6Pl6+v72Hlz5MiR7OeC/PHHH8qVK5f1oDR+ufPnz9f8+fMTTB8YGKh+/frZDItfdvw9OY9StmxZZciQQYcOHVLBggWVK1cuvfjii6pSpYqWLFmi6OhoHTlyRI0aNUrWusR7kt+pl156SUFBQTpz5oxu3bqlEiVKKGPGjBo7dqyqVq362Pnz58+vmJgYRUZGPvJ+kAULFsjR0VHdunXTlStXdOTIEW3dulWFChXSwIED1ahRI129evWRJ5XiTwCcPn2ah4vCBgEC6U7jxo01atQo/fLLL//ZvZy7u7v27duX4Ga8+CYx7u7u1v/j4uJ0/vx5mzPxDzedcXNzU5YsWRQXF5ekHkqeVPxZ+ly5cqX4cuPPEv/555/W5Uj3z2pdvHjxqaznwYMHdfPmTU2fPt3mj+qDvdckVZEiRXTs2DHFxMTY3AT+8DT79u1TpUqVnlo3mrGxsQoKClKmTJmS1azC7LYQP93jnuqblGY4BQsWtHmgVLz435OHrzg8zunTp/X3339r/PjxeuWVV6zDk/uwPcMwFBgYqOrVq+vVV19NMP6rr75SYGCg2rVrZ+rzcXNzk6ura6K9Wz3owSsrD57MeNyD1R60bds2RUdHa+bMmTaf48PNE+Ovkv7xxx+PDaCtW7fWu+++q+PHjyswMFClS5c2dTWlWLFiCgwMtF71MOvo0aM6f/68TUgJDAyUh4eHevfunWD65cuXKygoKEGAuHjxohwcHBJcmXiYi4uLypcvr8OHD6tgwYLWg9/KlSsrOjpa69at0/Xr1x97oJ7azwexWCw2n/vOnTtN/w25ePGiMmTI8MirFf/8849mzpypKVOmyMnJSf/8848kKW/evJJkfYZQSEjIIwNEgwYNNHv2bK1bt44AARvcA4F0J0uWLAoICFDfvn3l4+PzyOnq1aun2NhYLV682Gb4ggULZLFYVK9ePet0khJc5l24cKHNa0dHRzVt2lTBwcGJHnik1pM569atK1dXV82ePVsxMTEputxatWrJ2dlZ3333nU3TgVWrViksLOw/exlJKfFnqh9cfnR09CO7sDSjSZMmunHjRoLv/sHlNGvWTLGxsfrqq68STHPv3j2bs7Ap0Y1rbGysPv30U509e1Z+fn6PPKP4X8xuC25ubqpatapWr16d4ED2wc85vkcYM+vl7e2t48eP6+jRo9Zhd+7c0YoVK+Tu7q4SJUokaV0S+94Nw9C3336bpPeJd+TIEV26dElt27bVyy+/nOBf8+bNdeDAAYWEhJj6fBwcHNSoUSNt375dv/76a4LlxU8Xf1B/6NAh67g7d+7o+++/N117/FnuBz+LsLAwrV692ma6OnXqKEuWLJo9e3aCrlgfbvpTr1495cyZU3PnztWhQ4dMXX2QpIoVK8owjCQ9OOzSpUv66KOP5OzsrLfeekuSdOXKFR06dCjR7+Lll19W27Ztde7cOR07dszmvU6cOKESJUqYCi+VK1fW8ePHdeDAAWsgd3NzU/Hixa1drT7uoDj+d+BpPI09KipKU6ZMUZ48eWy6d01sH37q1Clt27ZNtWvXfuTVvIkTJ6pq1arWv2HxveLFh/qzZ89KknLnzv3Imry8vFS3bl2tXLlSW7ZsSTA+vokUnj9cgUC69KgmRA/y8fFR9erVNWnSJF26dEmenp7as2ePtm7dqtdff936h79UqVJq2bKllixZorCwMHl5eWn//v0J+qyXpP79++vAgQPq2LGjOnTooBIlSujWrVs6ceKE9u3bp4MHD6b4urq6uiogIECDBg1S27Zt1bx5c7m5ueny5cvauXOnKlWqpJEjRybrvd3c3NSjRw9Nnz5db7/9tnx8fPTXX39pyZIlKleunOmDjifh5eWl7Nmz66OPPpKfn58sFot++OGHZLfDlqRXXnlF33//vcaOHavjx4+rcuXKioyM1L59+9SlSxc1atRI1apVU6dOnTR79mz99ttvql27tpydnfX3339r48aNGjZsmPWhTkntxjUsLEw//PCDpPsHDfFPoj5//rxatGiR7IdkJWVbGD58uLp06aI2bdqoU6dOKlSokC5duqQdO3ZYaytTpowkadKkSWrevLmcnZ3VoEGDRM94+vv7a/369XrnnXfk5+en7Nmz6/vvv9fFixc1bdq0JDebK1asmIoUKaLx48crJCRErq6uCg4OTvaBXGBgoBwdHR/ZNa6Pj48mTZqkDRs26M033zT1+Xz44Yfas2eP/Pz81LFjRxUvXlzXrl3Txo0btWTJEmXLlk21a9dWwYIFNWzYMP35559ydHTU6tWrlTNnTtNXIeK3vZ49e6pz586KiIjQypUrlStXLl27ds06naurq4YMGaLhw4erffv2atmypbJly6ZTp04pKirK5kDP2dlZLVq00KJFi6zdB5tRuXJl5ciRQ/v27Uv0/qCTJ09afz9v376tX3/9VZs2bZLFYtGECROsD0cLDAyUYRjWblYf5u3tLScnJwUGBlrvrYmJidGhQ4dMdwJRpUoVzZo1S1euXLEJClWqVNHy5cvl7u7+2PvBihQpomzZsmnZsmXKkiWLMmfOrPLly9tckU2u9957T3nz5lWJEiUUHh6u1atX68KFC5ozZ47NCYT3339fGTNmlJeXl3LlyqUzZ85oxYoVypgxowYMGJDoex8/flwbNmyw6RygUKFCKlu2rIYMGaL27dtr5cqVqlChgvVq+6NMmDBB3bt3V58+fdSgQQPVrFlTmTJl0rlz57Rhwwb9888/PAviOUSAwHPLwcFBM2fO1NSpU7VhwwatWbNG7u7uGjRokLp3724z7ZgxY5QzZ04FBgZq69atql69uubMmZPgDHzu3Lm1cuVKzZgxQ5s3b9bSpUuVI0cOlShR4pE7+pTQqlUr5c2bV3PmzNG8efMUHR2tfPnyqUqVKk/cM1Dfvn3l5uamRYsWaezYscqePbs6duyoDz/88JHNf1JSzpw5NWvWLI0fP16TJ09WtmzZ5Ovrq5o1a1rPZiaVo6Ojvv76a82cOVNBQUHatGmTcuTIoUqVKtncp/Lxxx+rbNmyWrZsmSZNmiRHR0e5u7vL19fXpi/4pLp69aoGDRok6X4vLnnz5lXFihUVEBCg2rVrJ/t9JfPbQsmSJbVixQpNmTJFS5cu1d27d1WwYEE1a9bMOk358uX13nvvadmyZfrpp58UFxenrVu3JhogcufOrWXLlunzzz/XokWLdPfuXXl6emrWrFmPfZ5FYpydnTVr1ix9+umnmj17tjJkyKDGjRura9euSe6TPiYmRhs3bpSXl9cjb5j18PBQoUKFtG7dOr355pumPp98+fJZpwkMDFR4eLjy5cunevXqWZu9OTs7a/r06Ro9erT17PLrr7+ubNmyaciQIabqL1asmKZOnarJkydr/Pjxyp07t7p06SI3Nzeb59VI93tIy5Url+bMmaOvvvpKTk5OKlasWKLPoGndurUWLVqkmjVrWpu1PI6Li4tatWqljRs36sMPP0wwPigoSEFBQXJycpKrq6uKFi2q119/XZ07d7ZpfhUYGKiCBQsmeNpyvGzZsqlSpUrasGGDPvroIzk5OWnfvn26efOmqRNE0v2TD46OjsqYMaPNcuIDhJkmOc7Ozho3bpy+/PJLBQQE6N69exo7dmyKBIiyZctqzZo1Wr58uTJmzKjKlStr4sSJKlWqlM10jRo1UmBgoBYsWKDw8HDlzJlTjRs3Vp8+fRJtqmYYhj799FN17do1QVOvSZMmaejQofriiy9UpkyZRO8Fepibm5uWLVumJUuWWHuKiomJkbu7u3x8fPTaa6892QeBZ5LFeJLTeAAA4Jl06tQptW7dOsF9Jo9z4cIFNWvWTF9//fUjeylLDe+++64sFotmzJjx1JYJIHHcAwEAwHNoxYoVypw5s5o0aZKk+QoXLqx27dppzpw5qVRZQmfPntWOHTuS3bwPQMriCgQAAM+Rbdu26cyZM5o6daq6du1quikVAMQjQAAA8Bzx8fHR9evXVadOHU2YMCFZPX4BeL4RIAAAAACYxj0QAAAAAEwjQAAAAAAwjedASDp69KgMw3gqfdoDAAAAaU1MTIwsFou8vLweOy0BQvcfusKtIAAAAHheJeVYmAAhWa88lCtXzs6VAAAAAE/fr7/+anpa7oEAAAAAYBoBAgAAAIBpBAgAAAAAphEgAAAAAJhGgAAAAABgGgECAAAAgGkECAAAAACmESAAAAAAmEaAAAAAAGAaAQIAAACAaQQIAAAAAKYRIAAAAACYRoAAAAAAYBoBAgAAAIBpBAgAAAAAphEgAAAAAJhGgAAAAABgGgECAAAAgGkECAAAAACmESAAAAAAmEaAAAAAAGAaAQIAAACAaQQIAAAAAKYRIAAAAACYRoAAAAAAYBoBAgAAAIBpBAgAAAAAphEgAAAAAJhGgAAAAABgGgECAAAAgGkECAAAAACmESAAAAAAmEaAAAAAAGAaAQIAAACAaQQIAAAAAKYRIAAAAACYRoAAAAAAYBoBAgAAAIBpBAgAAAAAphEgAAAAAJhGgAAAAABgGgECAAAAgGkECAAAAACmESAAAAAAmEaAAAAAAGAaAQIAAACAaQQIAAAAAKYRIAAAAACYRoAAAAAAYBoBAgAAAIBpBAgAAAAAphEgAAAAAJhGgAAAAABgGgECAAAAgGkECAAAAACmESAAAAAAmEaAAAAAAGAaAQIAAACAaQQIAAAAAKYRIAAAAACYRoAAAAAAYBoBAgAAAIBpBAgAAAAAphEgAAAAAJiWJgPE2rVr9corr6hcuXKqXr263n77bUVFRVnHb9u2Tb6+vipXrpyaNm2q1atX27FaAAAA4PnhZO8CHjZz5kx9/fXX6tmzpypWrKgbN25o3759io2NlSQdPnxYffr0Ufv27TV06FDt379fw4YNU5YsWfTyyy/buXoAAAAgfbMYhmHYu4h4f/75p1q1aqWvvvpK3t7eiU7z1ltvKSIiQsuWLbMO69+/v3777Tdt2LAhWcv99ddfJUnlypVL1vwAAADAsywpx8NpqgnTmjVrVKhQoUeGh+joaB04cCDBlYbmzZvr7Nmzunjx4tMoEwAAAHhupakAcezYMXl4eOirr75SzZo1VbZsWXXu3FnHjh2TJJ0/f14xMTEqVqyYzXzFixeXdP8KBgAAAIDUk6bugbh27Zr+97//6fTp0xo1apQyZcqkWbNmqXv37tq0aZNu3bolScqWLZvNfPGv48cDAAAASB1pKkAYhqE7d+5oypQpKlmypCSpQoUK8vHx0aJFi1SnTh07VwgAAAA839JUE6Zs2bIpR44c1vAgSTly5FDp0qV15swZZc+eXZIUFhZmM9/t27clyToeAAAAQOpIUwGiRIkSjxx39+5dFSlSRM7OzgnudYh//fC9EQAAAABSVpoKEA0aNNDNmzf122+/WYfduHFDJ06cUJkyZeTi4qLq1asrODjYZr4NGzaoePHiKlSo0NMuGQAAAHiupKl7IBo1aqRy5cqpX79++uCDD5QhQwbNmTNHLi4uevXVVyVJvXr10muvvaaAgAA1a9ZMBw4cUFBQkCZNmmTn6gEAAID0L009SE6SQkNDNXbsWG3fvl0xMTGqUqWKhgwZYtO8aevWrZo8ebL++usvFSxYUP7+/mrfvn2yl8mD5AAAAPA8S8rxcJoLEPZAgAAAAMDz7Jl9EjUAAACAtI0AAQAAAMA0AgQAAAAA0wgQAAAAAEwjQAAAAAAwjQABAAAAwDQCBAAAAADTCBAAAAAATCNAAAAAADCNAAEAAADANAIEAAAAANMIEAAAAABMI0AAAAAAMI0AAQAAAMA0AgQAAAAA0wgQAAAAAEwjQAAAAAAwjQABAAAAwDQCBAAAAADTCBAAAAAATCNAAAAAADCNAAEAAADANCd7FwAAAJ5vV65c0ZUrV5I8X4ECBVSgQIFUqAjAfyFAAAAAu5o9e7ZGjx6d5PlGjRqlgICAlC8IwH8iQAAAALvq0aOHfH19bYZFRkaqTp06kqTdu3crU6ZMCebj6gNgHwQIAABgV4k1RYqIiLD+XLFiRWXJkuVplwXgEbiJGgAAAIBpBAgAAAAAphEgAAAAAJhGgAAAAABgGgECAAAAgGkECAAAAACmESAAAAAAmEaAAAAAAGAaAQIAAACAaQQIAAAAAKYRIAAAAACYRoAAAAAAYBoBAgAAAIBpBAgAAAAAphEgAAAAAJhGgAAAAABgGgECAAAAgGkECAAAAACmESAAAAAAmEaAAAAAAGAaAQIAAACAaQQIAAAAAKYRIAAAAACYRoAAAAAAYBoBAgAAAIBpBAgAAAAAphEgAAAAAJhGgAAAAABgGgECAAAAgGkECAAAAACmESAAAAAAmEaAAAAAAGAaAQIAAACAaQQIAAAAAKYRIAAAAACYRoAAAAAAYBoBAgAAAIBpBAgAAAAAphEgAAAAAJhGgAAAAABgGgECAAAAgGkECAAAAACmESAAAAAAmEaAAAAAAGAaAQIAAACAaQQIAAAAAKYRIAAAAACYRoAAAAAAYBoBAgAAAIBpBAgAAAAAphEgAAAAAJhGgAAAAABgGgECAAAAgGkECAAAAACmOSVnplOnTunIkSM6e/asbty4IYvFopw5c6pYsWKqVKmSSpUqldJ1AgAAAEgDTAeIf//9V0uWLNH333+vy5cvyzAMOTs7K3v27DIMQ7dv31ZMTIwsFosKFCigNm3aqEuXLsqdO3dq1g8AAADgKTIVID7//HMtWbJEWbJk0csvv6xatWqpTJkyypcvn810ISEhOnHihPbs2aMVK1Zo/vz56tatm/r3758qxQMAAAB4ukwFiMOHD+vzzz9Xw4YNZbFYHjldvnz5lC9fPvn4+Gj48OHaunWr5s6dm2LFAgAAALAvUwFi+fLlSX5ji8WiRo0aqVGjRkmeFwAAAEDaRC9MAAAAAExLVi9MDzt48KACAwMVEhKi3Llzq3nz5qpTp05KvDUAAACANOSJr0AsW7ZMPXr0UExMjEqWLKlbt26pR48emj9/fkrUBwAAACANMX0FIjw8XK6urgmGL1y4UBMnTpSPj4912JdffqkFCxaoe/fuKVMlAAAAgDTB9BWIJk2aaOXKlaamtVgs/9lb06OsWbNGnp6eCf598cUX1mn8/PwSnebs2bNJXh4AAACApDF9BSIgIEATJkzQ0qVLNWzYMFWuXFnS/QP6/v376+WXX1bevHn1119/acuWLfrggw+SXdTcuXOVNWtW6+uHnzdRqVIlDR482GZYoUKFkr08AAAAAOaYDhBNmjRR/fr1NXfuXL399tuqX7++Bg8erFdffVUvvPCC1q9fr5MnTyp37tz66quvVL9+/WQXVaZMGbm5uT1yfLZs2VSxYsVkvz8AAACA5ElSL0wuLi5699131a5dO02YMEHNmjVT9+7d9c4776hWrVqpVSMAAACANCJZvTDly5dPEydO1Lx587Rjxw69/PLLCgoKSrGiWrZsqVKlSqlhw4aaPXu2YmNjbcYfPHhQFStWVLly5dStWzcdOnQoxZYNAAAA4NGSdAXi8uXL2r17tyIjI1WhQgVVqlRJq1at0qpVqzRu3DgtWrRIw4cPV9myZZNVTJ48edS3b19VqFBBFotF27Zt0+TJkxUSEqKRI0dKkqpWrarWrVvrhRde0D///KN58+bpzTff1HfffScvL69kLRcAAACAORbDMAwzE27fvl3vv/++8uTJo2zZsunUqVN6/fXXrTczh4eHa8aMGVq8eLFatGihAQMGKFeuXE9c4Pjx47Vw4ULt2LFDefPmTTD+zp07atmypYoXL66vv/46Wcv49ddfJUnlypV7oloBAEDKiIiIsHYfHx4erixZsti5IiB9S8rxsOkmTF988YWaNm2qLVu2aM2aNRo3bpwWLFigkJAQSZKrq6sGDx6sH374Qf/++6+aNGmSzPJtNWvWTLGxsfrtt98SHZ85c2Z5e3vrxIkTKbI8AAAAAI9mOkBcvXpVlSpVsr6uVKmSDMOwBoh4L774oubMmaMvv/wy5aoEAAAAkCaYvgeicuXK+u677/TSSy8pW7ZsmjVrlrJnz66XXnop0em9vb1TpMANGzbI0dFRpUuXTnT8nTt3tGPHDpofAQAAAE+B6QDxySefaPDgwerWrZsMw1CRIkU0ZcoUZcqUKcWKeeutt1S9enV5enpKkrZu3aoVK1botddeU548eXT48GHNnTtXjRs3lru7u/755x998803unbtmqZMmZJidQAAAABInOkAkS9fPi1YsEB3797V3bt3lS1bthQv5sUXX9Tq1at19epVxcXF6YUXXtDQoUPl5+cn6X4vTTExMZo0aZJu3rypTJkyycvLS6NHj1b58uVTvB4AAAAAtkz3wpSe0QsTAABpC70wAU9XivfCNHv2bEVERCS5kPDwcM2ePTvJ8wEAAABIm0wFiKCgINWvX18BAQE6cOBAgidDPygmJkZ79+7ViBEjVL9+/RR9QjUAAAAA+zJ1D8S6desUGBio+fPna9myZXJxcdFLL72kQoUKKXv27DIMQ7du3dLFixf1xx9/6N69e/Lw8NCIESPk6+ub2usAAAAA4CkxFSAsFot8fX3l6+urkydPasuWLfrll1907Ngx3bx5U5KUI0cOFStWTO+8844aNmyoMmXKpGbdAIBn0JUrV3TlypUkz1egQAEVKFAgFSoCkFaxv0i7TPfCFK906dKPfCYDAAD/Zfbs2Ro9enSS5xs1apQCAgJSviAAaRb7i7QryQECAIDk6tGjR4KmrZGRkapTp44kaffu3Yk+X4izicDzh/1F2kWAAAA8NYk1LXiwl7+KFSvSXScASewv0jJTvTABAAAAgESAAAAAAJAEBAgAAAAAphEgAAAAAJj2RDdRh4SE6NChQ/r333/VtGlT5c+fX7GxsQoLC1PWrFnl6OiYUnUCAAAASAOSFSAMw9C4ceO0ePFi3bt3TxaLRR4eHsqfP7/u3LkjHx8f9evXT2+88UYKlwsAAADAnpLVhGnu3Ln69ttv1b17d33zzTcyDMM6LmvWrGrSpIk2bdqUYkUCAAAASBuSdQVi5cqVeuWVV/Thhx/qxo0bCcZ7enpq165dT1wcgGfXlStXdOXKlSTPl1i/3wAAIO1IVoC4cuWKvLy8Hjk+U6ZMCg8PT3ZRAJ59s2fP1ujRo5M836hRoxQQEJDyBQEAgBSRrACRK1eu/zyzeOLECc4gAs+5Hj16yNfX12ZYZGSk6tSpI0navXu3MmXKlGA+9h0AAKRtyQoQjRs31rJly9S2bVu5urpKkiwWi6T7BwVr167VW2+9lXJVAnjmJNYUKSIiwvpzxYoVlSVLlqddFpCuxcYZcnSw2LsMmMB3hWdZsgJEv379dODAAbVu3VpVqlSRxWLR119/rSlTpuiXX35RqVKl1LNnz5SuFQAA/AdHB4vGrT2qC9ef/WbEMXcjrT9/8M0eOWdIeMXyWVU4t6s+avPopuBAWpesAJE1a1atWLFC8+fPV3BwsDJkyKBDhw6pSJEi6t27t95++21lzJgxpWsFAACPceF6uM5cvW3vMp7Yvego689nQ8Lk5BJjx2oAPCjZD5LLmDGj3n33Xb377rspWQ8AAACANCxZz4EAAAAA8HxK1hWIIUOGPHYai8WiMWPGJOftAQAAAKRRyQoQBw4cSDAsLi5O165dU2xsrNzc3BLtnhEAAADAsy1ZAWLbtm2JDo+JidHy5cu1cOFCzZ8//4kKAwAAAJD2pOg9EM7OzurWrZtq166tTz75JCXfGgAAAEAakCo3UZcsWVKHDh1KjbcGAAAAYEepEiD27t3LPRAAAABAOpSseyCmT5+e6PCwsDAdOnRIJ0+elL+//xMVBgAAACDtSdEAkT17dhUuXFijR49Wx44dn6gwAAAAAGlPsgLEqVOnUroOAAAAAM8AnkQNAAAAwDRTVyAuX76crDcvWLBgsuYDAAAAkDaZChA+Pj6yWCxJfvPffvstyfMAAAAASLtMBYgxY8YkK0AAAAAASF9MBYi2bdumdh0AAAAAngHcRA0AAADAtGR14xrvyJEjOnnypMLCwhQXF2czzmKxqHfv3k9UHAAAAIC0JVkB4ubNm+rRo4eOHz8uwzBksVhkGIYkWX8mQAAAAADpT7KaME2YMEG///67Jk6cqC1btsgwDM2bN0/BwcHq3LmzSpUqpZ9++imlawUAAABgZ8kKELt27VKnTp3UvHlzZcmS5f4bOTioaNGiGjVqlNzd3TVmzJgULRQAAACA/SUrQNy+fVslSpSQJGuAiIiIsI6vXbu2du/enQLlAQAAAEhLkhUg8ubNq+vXr0uSXFxclCtXLp06dco6PiQkhOdGAAAAAOlQsm6irlq1qvbu3atevXpJkpo1a6Z58+bJ0dFRcXFxWrhwoerWrZuihQIAAACwv2QFiDfeeEN79+5VdHS0XFxc1LdvX505c0ZTpkyRdD9gDB8+PEULRdp15coVXblyJcnzFShQQAUKFEiFigAAAJBaTAeItm3bqnXr1mrevLk8PT3l6elpHZc9e3YtWLBAt2/floODg1xdXVOlWKRNs2fP1ujRo5M836hRoxQQEJDyBQEAACDVmA4Q//77r8aOHasJEyaoRo0aatWqlRo3bmy9iVqSsmXLlipFIm3r0aOHfH19bYZFRkaqTp06kqTdu3crU6ZMCebj6gMAAMCzx3SA2Llzpw4ePKigoCAFBwdrz549CggIUIMGDdSqVSvVq1dPTk5P9GBrPKMSa4r0YK9cFStWtAmaAAAAeHYl6Yi/WrVqqlatmkaOHKldu3YpKChI27dv18aNG5UtWza9/PLLatWqlapUqZJa9QIAAACwo2RdMnBycpKPj498fHwUGRmpzZs3KygoSKtXr9aKFStUoEABtWzZUh9++GFK1wsAAADAjpL1HIgHZcqUSb6+vpozZ4527twpHx8fXb58WV9//XVK1AcAAAAgDUmRmxZ++eUXBQUFaePGjbp+/boyZMigBg0apMRbAwAAAEhDkh0gzp49q3Xr1mnDhg26ePGiLBaLqlevrg8//FBNmjShK1cAAAAgHUpSgLhy5YqCgoIUFBSk06dPyzAMlS5dWoMGDVKLFi2UN2/e1KoTAAAAQBpgOkB07dpVR48eVVxcnNzd3eXv7y9fX18VL148NesDAAAAkIaYDhBnz55Vx44d1apVK1WuXDk1awIAAACQRpkOELt37+ZBcQAAAMBzznQ3roQHAAAAAE/8HAgAAAAAzw8CBAAAAADTCBAAAAAATCNAAAAAADDtie+MjoiI0O3bt2UYRoJxBQsWfNK3BwAAAJCGJCtA3L17V9OnT9eqVat08+bNR07322+/JbcuAAAAAGlQsgJEQECAvv/+ezVq1EiVK1dW9uzZU7ouAAAAAGlQsgLE5s2b1aFDB3388ccpXQ8AAACANCxZN1FbLBaVLl06pWsBAAAAkMYlK0A0bNhQe/fuTelaAAAAAKRxyQoQ7777ri5evKgRI0bof//7n0JDQ3Xz5s0E/wAAAACkL8m6B6JJkyaSpJMnT2rVqlWPnI5emAAAAID0JVkBonfv3rJYLCldCwAAAIA0LlkBom/fvildBwAAAIBnQLLugQAAAADwfErWFYh4R44c0cmTJxUWFqa4uDibcRaLRb17936i4gAAAACkLckKEDdv3lSPHj10/PhxGYYhi8UiwzAkyfozAQIAAABIf5LVhGnChAn6/fffNXHiRG3ZskWGYWjevHkKDg5W586dVapUKf30008pXSsAAAAAO0tWgNi1a5c6deqk5s2bK0uWLPffyMFBRYsW1ahRo+Tu7q4xY8akaKEAAAAA7C9ZAeL27dsqUaKEJFkDREREhHV87dq1tXv37hQoDwAAAEBakqwAkTdvXl2/fl2S5OLioly5cunUqVPW8SEhITwnAgAAAEiHknUTddWqVbV371716tVLktSsWTPNmzdPjo6OiouL08KFC1W3bt0ULRQAAACA/SUrQLzxxhvau3evoqOj5eLior59++rMmTOaMmWKpPsBY/jw4SlaKAAAAAD7S1aA8PT0lKenp/V19uzZtWDBAt2+fVsODg5ydXVNsQIBAAAApB1P9CC5h2XLli0l3w4AAABAGpOsm6gl6fLlyxo5cqSaNm2qatWq6dChQ5Kk0NBQffrppzp58mSKFQkAAAAgbUhWgDhz5ozatGmjH3/8UYUKFVJYWJju3bsnSXJzc9ORI0e0aNGiFC0UAAAAgP0lqwnT559/rqxZs2rFihWSpFq1atmM9/b21o8//vjk1QEAAABIU5J1BeLQoUPq0qWL3NzcEn3eQ8GCBRUSEvLExQEAAABIW5IVIAzDUMaMGR85PjQ0VC4uLskuCsB9sXGGvUtAEvB9AQCeB8lqwlS6dGnt3LlTXbt2TTDu3r17Wr9+vSpUqJDk912zZo2GDBmSYPg777yjAQMGWF+vXLlSc+fO1eXLl/Xiiy/qgw8+UIMGDZK8PCCtc3SwaNzao7pwPdzepaSImLuR1p8/+GaPnDNksmM1Katwbld91MbL3mUAAJDqkhUg/P391bNnT40aNUotWrSQJP3777/au3evZs2apT///FMjR45MdlFz585V1qxZra/z5ctn/Xn9+vUaMWKEevbsqRo1amjDhg3q06ePFi9erIoVKyZ7mUBadeF6uM5cvW3vMlLEvego689nQ8Lk5BJjx2oAAEByJCtAeHt7a+zYsRozZoz1RuqBAwfKMAy5urpq/Pjxqlq1arKLKlOmjNzc3BIdN3XqVLVo0ULvv/++JKlGjRo6ffq0ZsyYoa+//jrZywQAAADweMl+kNwrr7yiJk2aaO/evfr7778VFxenIkWKqE6dOqn2JOoLFy7o77//1sCBA22GN2/eXBMmTFB0dDT3XgAAAACp6ImeRJ05c2Y1atQopWqxatmypW7cuKGCBQuqY8eOevvtt+Xo6Kg///xTkvTiiy/aTF+8eHHFxMTowoULKl68eIrXAwAAAOC+JwoQKS1Pnjzq27evKlSoIIvFom3btmny5MkKCQnRyJEjdevWLUlStmzZbOaLfx0/HgAAAEDqMB0gKlWqlKQ3tlgsOnLkSJLmqVu3rurWrWt9XadOHWXIkEELFy5Uz549k/ReAAAAAFKe6QBx584dZcyYUbVq1VL27NlTsyYbzZo10/z58/Xbb79ZlxsWFqY8efJYp7l9+34PNU+zLgAAAOB5ZDpAtGjRQtu2bdNPP/2kunXrqmXLlmrYsKEyZMiQmvXZKFasmCTpzz//tP4c/9rZ2VmFCxd+arUAAAAAzyPTT6KeOHGi9u7dqzFjxig2NlaDBg1SrVq1NHjwYP3000+Ki4tLlQI3bNggR0dHlS5dWoULF9YLL7ygjRs3JpimZs2a9MAEAAAApLIk3USdKVMmtWrVSq1atdKNGze0YcMGrV+/Xv7+/sqZM6eaNWumrl272lwdSIq33npL1atXl6enpyRp69atWrFihV577TVrk6W+fftqwIABKlKkiKpXr64NGzbo+PHjWrRoUbKWCQAAAMC8ZPfClDNnTnXt2lVdu3bV+fPnNXz4cC1ZskQ5c+ZUnz59kvWeL774olavXq2rV68qLi5OL7zwgoYOHSo/Pz/rNC1btlRkZKS+/vprzZkzRy+++KKmT58uLy+v5K4KAAAAAJOeqBvXn3/+WevXr9fGjRsVGhqqSpUqqUaNGsl+v+HDh5uarkOHDurQoUOyl5MWxcYZcnSw2LsMmMT3BQAAnldJDhC///67goKCtH79el2+fFmenp5644031LJlSxUoUCA1anwuODpYNG7tUV24Hm7vUlJEzN1I688ffLNHzhky2bGalFU4t6s+asMVL9gPAfbZwvcFe2MbfHY8K9+V6QAxa9YsrV+/XmfOnFGhQoWs90KUKFEiNet7rly4Hq4zV2/bu4wUcS86yvrz2ZAwObnE2LEaIH3hhMOzgxMOSAvS0z6D/UXaYDpATJ48WRkzZlTjxo2t9xv89NNP+umnnxKd3mKx6I033kiRIgEAtjjhACAp0ss+g/1F2pCkJkxRUVHatGmTNm3a9NhpCRAAAABA+mM6QGzdujU16wAAAADwDDAdINzd3VOzDgAAAADPANNPogYAAAAAAgQAAAAA0wgQAAAAAEx7oidRAwAAPKm7YaG6GxZqMyw2Jtr6c9iVP+Xo7JJgvgxZ3ZQhq1uq1wfAFgECAADY1YVDP+rPHUsfOf7QvEGJDi9Wv4tK+HRNrbIAPEKKB4iYmBjt3LlT69at09SpU1P67QEAQDpTuGoz5S1ZPcnzcfUBsI8UCxAHDx5UYGCgNm3apFu3bilTpvTzaHEAAJB6aIoEPFueKECcOnVKgYGBWr9+vUJCQpQ7d241bdpUPj4+qlmzZkrVCAAAACCNSHKAuHz5soKCghQYGKgzZ87Izc1N1atX148//qgRI0aoSZMmqVEnAAAAgDTAdIBYtmyZAgMD9fPPPytr1qxq3LixhgwZoho1aujChQvasGFDatYJAAAAIA0wHSACAgJUqFAhTZs2Td7e3nJ2draOs1gsqVIcAAAAgLTF9IPkypYtq4sXLyogIEDjx4/X0aNHU7MuAAAAAGmQ6SsQq1at0rlz5/TDDz9o/fr1WrRokQoWLKjmzZurXLlyqVkjAAAAgDTC9BUISSpatKj69eun4OBgLV++XA0aNNCaNWv03nvvyWKxKDg4WD///LMMw0itegEAAADYUbK7ca1QoYIqVKigoUOHavfu3QoMDNTWrVu1YcMG5ciRQ/Xr19fYsWNTslYAAAAAdpakKxCJcXR0lLe3t7744gvt3btX48aNU5kyZRQYGJgS9QEAAABIQ1LsSdSSlClTJrVu3VqtW7dWaGhoSr41AAAAgDQgyQEiOjpaP/zwg/bs2aPz588rIiJCWbJkUdGiRVW3bl21bNlSLi4ucnPjkfQAAABAepOkAPH777/r3Xff1eXLl2UYhrJmzarMmTMrNDRUJ0+e1MaNGzVr1izNnDlTxYsXT62aAQAAANiJ6QARERGhXr16KTQ0VB988IFat26tfPnyWceHhITo+++/18yZM9WzZ0/98MMPypw5c6oUDQAAAMA+TN9EvWbNGl25ckWzZ8+Wv7+/TXiQpHz58qlHjx6aOXOmLl68qLVr16Z4sQAAAADsy3SA2LFjh2rXrq3q1av/53Q1a9ZUrVq1tG3bticuDgAAAEDaYjpAnD59WtWqVTM1bY0aNXT69OlkFwUAAAAgbTIdIG7duqU8efKYmjZ37ty6detWsosCAAAAkDaZDhDR0dFycjJ3z7Wjo6NiYmKSXRQAAACAtClJ3bheunRJJ06ceOx0Fy9eTHZBAAAAANKuJAWIKVOmaMqUKY+dzjAMWSyWZBcFAAAAIG0yHSDGjh2bmnUAAAAAeAaYDhBt2rRJzToAAAAAPANM30QNAAAAAAQIAAAAAKYRIAAAAACYRoAAAAAAYBoBAgAAAIBpBAgAAAAAphEgAAAAAJhGgAAAAABgGgECAAAAgGkECAAAAACmESAAAAAAmEaAAAAAAGAaAQIAAACAaQQIAAAAAKYRIAAAAACYRoAAAAAAYBoBAgAAAIBpBAgAAAAAphEgAAAAAJjmZO8C8Oy7Gxaqu2GhNsNiY6KtP4dd+VOOzi4J5suQ1U0Zsrqlen0AAABIOQQIPLELh37UnzuWPnL8oXmDEh1erH4XlfDpmlplAQAAIBUQIPDECldtprwlqyd5Pq4+AAAAPHsIEHhiNEVCYmjaBgBA+kSAAJAqaNoGAED6RIAAkCpo2gYAQPpEgACQKmiKBABA+sRzIAAAAACYRoAAAAAAYBoBAgAAAIBpBAgAAAAAphEgAAAAAJhGgAAAAABgGgECAAAAgGkECAAAAACmESAAAAAAmEaAAAAAAGAaAQIAAACAaQQIAAAAAKYRIAAAAACYRoAAAAAAYBoBAgAAAIBpBAgAAAAAphEgAAAAAJhGgAAAAABgGgECAAAAgGkECAAAAACmESAAAAAAmEaAAAAAAGAaAQIAAACAaQQIAAAAAKYRIAAAAACYRoAAAAAAYJqTvQsAADw/7oaF6m5YqM2w2Jho689hV/6Uo7NLgvkyZHVThqxuqV4fgLSD/UXaRYAAADw1Fw79qD93LH3k+EPzBiU6vFj9Lirh0zW1ygKQBrG/SLvSbICIiIhQs2bNFBISolWrVqlcuXKSJD8/Px08eDDB9Bs2bFDx4sWfdpkAgCQoXLWZ8pasnuT5OJsIPH/YX6RdaTZAfPXVV4qNjU10XKVKlTR48GCbYYUKFXoaZQEAngBNCwCYxf4i7UqTAeLs2bNasmSJBg8erFGjRiUYny1bNlWsWPHpFwYAAAA859JkL0yffvqpOnfurBdffNHepQAAAAB4QJoLEBs3btTp06fVu3fvR05z8OBBVaxYUeXKlVO3bt106NChp1ghAAAA8PxKU02YIiMjNW7cOH3wwQdydXVNdJqqVauqdevWeuGFF/TPP/9o3rx5evPNN/Xdd9/Jy8vrKVcMAAAAPF/SVICYOXOmcuXKpXbt2j1ymn79+tm8rl+/vlq2bKmvvvpKX3/9dWqXCAAAADzX0kwTpkuXLmn+/Pnq16+fwsLCdPv2bd25c0eSdOfOHUVERCQ6X+bMmeXt7a0TJ048zXIBAACA51KauQJx8eJFxcTEyN/fP8G41157TRUqVNCKFSvsUBkAAACAeGkmQJQqVUrffvutzbDffvtNY8eO1ejRo60PknvYnTt3tGPHjkeOBwAAAJBy0kyAyJYtm6pXT/xpg2XKlFGZMmV0+PBhzZ07V40bN5a7u7v++ecfffPNN7p27ZqmTJnylCsGAAAAnj9pJkCYkSdPHsXExGjSpEm6efOmMmXKJC8vL40ePVrly5e3d3kAAABAupemA0T16tX1+++/W18XLVpU8+bNs2NFAAAAwPMtzfTCBAAAACDtI0AAAAAAMI0AAQAAAMA0AgQAAAAA0wgQAAAAAEwjQAAAAAAwjQABAAAAwDQCBAAAAADTCBAAAAAATCNAAAAAADCNAAEAAADANAIEAAAAANMIEAAAAABMI0AAAAAAMI0AAQAAAMA0AgQAAAAA0wgQAAAAAEwjQAAAAAAwjQABAAAAwDQCBAAAAADTCBAAAAAATCNAAAAAADCNAAEAAADANAIEAAAAANMIEAAAAABMI0AAAAAAMI0AAQAAAMA0AgQAAAAA0wgQAAAAAEwjQAAAAAAwjQABAAAAwDQCBAAAAADTCBAAAAAATCNAAAAAADCNAAEAAADANAIEAAAAANMIEAAAAABMI0AAAAAAMI0AAQAAAMA0AgQAAAAA0wgQAAAAAEwjQAAAAAAwjQABAAAAwDQCBAAAAADTCBAAAAAATCNAAAAAADCNAAEAAADANAIEAAAAANMIEAAAAABMI0AAAAAAMI0AAQAAAMA0AgQAAAAA0wgQAAAAAEwjQAAAAAAwjQABAAAAwDQCBAAAAADTCBAAAAAATCNAAAAAADCNAAEAAADANAIEAAAAANMIEAAAAABMI0AAAAAAMI0AAQAAAMA0AgQAAAAA0wgQAAAAAEwjQAAAAAAwjQABAAAAwDQCBAAAAADTCBAAAAAATCNAAAAAADCNAAEAAADANAIEAAAAANMIEAAAAABMI0AAAAAAMI0AAQAAAMA0AgQAAAAA0wgQAAAAAEwjQAAAAAAwjQABAAAAwDQCBAAAAADTCBAAAAAATCNAAAAAADCNAAEAAADANAIEAAAAANMIEAAAAABMI0AAAAAAMI0AAQAAAMA0i2EYhr2LsLeff/5ZhmHIxcXFrnX8GxalmNg4u9aAx3N2dFCurBmf2vLYLp4NbBdIzNPeLiS2jWcB2wUSY4/t4kHR0dGyWCyqVKnSY6d1egr1pHkWi8XeJUiSXTcapF1sF0gM2wUehW0DiWG7wONYLBbTx8RcgQAAAABgGvdAAAAAADCNAAEAAADANAIEAAAAANMIEAAAAABMI0AAAAAAMI0AAQAAAMA0AgQAAAAA0wgQAAAAAEwjQAAAAAAwjQABAAAAwDQCBAAAAADTCBAAAAAATCNAwLTo6GidOXPG3mUAAIB0Ji4uToZh2LsMmESAgCmxsbHq27evRo0apV9//dXe5SCNYuePeFFRUTp27Ji9y0Aac+fOHQUFBSkqKsrepSANCQ8P18iRI/XXX3/ZuxSYRICAKY6OjqpXr56uX7+umTNn6vjx4/YuCXYWHR2tX375RVu2bFFoaKhiY2NlsVjsXRbSgPDwcPn6+mrbtm2KiYmxdzlII8LDw9WoUSPt2bPH3qUgDQkPD1erVq30xx9/KFeuXPYuByZZDE4Z4jFiY2Pl6OgoSVq9erW++uoreXp6qmfPnipfvrydq4M9hIeHq2fPnrp8+bIuX76sYsWKqXfv3mrWrJkcHDgv8TwLDw9XmzZtlC9fPn355ZfKmzevvUtCGhAeHq5XXnlFhQoV0vjx45UvXz57l4Q0IP5kQ5EiRTRhwgT2F88Q/tLjkeLi4hQXFydHR0fFxsZKktq1a6devXrp999/16xZs7gS8RyK3+FnyJBBI0aM0Pr165UlSxbNmjVLd+/etXd5sKP4bcPd3V0TJ05U3rx5FRcXZ++yYGcRERF65ZVXVLRoUWt4iP+bEo9zmc+f8PBwdezYUYULF7aebIjfX7DfSPsIEEhURESEhg0bprFjx+qvv/7Sv//+ax3Xvn17+fv769SpUzRnes5ERESoXbt2evHFFzVu3DjVrVtXxYsXt7ZdvXTpkmJjY/kj8ByKiopS165d5ezsrJkzZypfvny6d++eHBwcFB0draNHj9q7RNhBdHS0OnXqpMjISA0fPlz58uVTTEyMHB0dFR0drYkTJ0oSzR+fQwMGDNCff/6pjh07ys3NTZKs+4v27dsrODjYzhXivxAgkEBcXJyGDRumtWvX6rvvvpOfn5969OihGTNmaN++fZKkTp06aeDAgTp16pRmzZqlX375xb5FI9XFxsaqR48eOnfunLp166Y8efJYm7bdvXtXOXLk0PTp09WtWzcFBAToxo0bcnBwIEQ8J/73v/8pNDRU2bJl08GDByVJTk5OioqKUuPGjTVlyhRFR0fbuUo8bS4uLsqfP7+cnJy0ceNG3bp1S87OzoqKilK7du20adMm3bp1y95lwg4CAgJUoEABzZo1Szt37rQOb9u2rZycnFSxYkX7FYfHIkAgAQcHB7Vp00blypVTmTJlVKFCBdWvX1/ffvut3nvvPfn6+mrOnDmqXLmyevTooQsXLmj+/PlciUjnHB0d1b59e+XJk0crVqzQkSNHZLFYFBMTo9GjRytz5szKkyePsmbNqh9++EFdu3bVrVu3uCcinYu/SbpKlSr69NNPde/ePc2YMUOHDh2SdP+Kpbu7u8aMGSMXFxd7loqnLP7kwdy5c1WhQgUtXbpUq1evVmhoqDp06KAsWbJo4cKFyp49u50rxdN279495c+fX8uWLdONGzc0adIkbd++XS1btpSrq6smT57MfTJpHDdRw8owDBmGYT3g27Vrl6ZPny5HR0f169dP5cqV05EjR7R06VKdPn1aV69eVbNmzfTzzz/LMAwVKVJEQ4YMUalSpey8JkhN69at0/jx41WpUiX5+fnpk08+UZYsWTRhwgQVKVJEkjRz5kxNmTJFvXv3Vp8+fWiekE6Fh4frww8/1Kuvvqr69etLkrZv366pU6fKyclJ165dk7u7u7788ksOBp5TD3bC0a9fPx09elRxcXEqUKCA5s2bR3h4DhmGIYvFonv37snJyUlXr15Vhw4ddO3aNb300kuaM2eOChQoYO8y8RicGoQkKTIyUtOmTdO8efN08+ZNSVK9evXUp08f3b17V59//rmOHDkib29vzZo1S4sWLdKYMWOsgePq1av6448/lCNHDruuB1JWfFetO3fu1N9//y1J8vX11cCBA/Xzzz+rR48ekqTZs2erSJEiunfvniTp7bffVvbs2XXz5k3CQzoV3/ViZGSkypcvb70JtkGDBnrvvfcUHR2t8PBwtW/f3hoeaM72/HmwE46pU6eqevXqunHjhqpVqyYnJydJbBfPg7t37+r3339XdHS0LBaLDMOQk5OTYmJilD9/fq1Zs0bu7u6KiorS2bNnrfNxjjvtcrJ3AbC/8PBwvfXWW7p7964KFiyobt26WcfVq1dPDg4O+vLLLzVt2jTdvXtXTZo0UcGCBfXKK6+oefPmioiI0IYNG1SvXj3OGqQj4eHh6t27t06fPq0bN26oYMGCatu2rfr06aNXXnlFGTJkUEBAgHLnzq2//vpLFStWtP5BOH/+vHLlyqWiRYtK+r8zTkgfwsPD1bp1a73wwgsaP3683NzcbP7Q169fXxaLRZMnT9bSpUvl5uYmb29vOTg4sC2kY/FnlB8WHyIcHR31xRdfKCYmRkFBQXJ1dZWfn5+yZs3KdpGORUdHq0OHDtbg2LdvXxUsWFAuLi5ydnZWTEyM8uTJoyVLlqh9+/YaN26cBg0apHr16lnDBttG2sMViOfcnTt39Oqrrypjxoz65JNPNHnyZGXKlMnmkfJ16tTR+++/r7i4OM2ZM0dbtmyxzm+xWJQzZ0517dpVhQsXttdqIIXFHyA6ODho4MCBmj17trJkyaL58+dr9erVkqRmzZpp5MiROn36tGbNmmXtZScmJkYLFy5UVFSUfHx8JNHDSnryYH/+48aNs3a9aLFYFBsbqz/++EOS5O3trffee08xMTGaMWOGduzYIUnWAwKkL9HR0WrXrp3mzJmT6PgHr0RMmTJFXl5eWrZsmb777juFh4ezXaRjV65c0dWrV3Xt2jXt379fLVu21IgRI6w3Tjs7O0uS8uXLp1WrVunWrVsaP368du/eTXhIwwgQz7l58+YpU6ZMGjlypMqWLWu9ydHBwcHml7ZevXrq16+f4uLiNGvWLG3btk3S//3iI/24c+eOfH195eHhoXHjxql169by9vbW/PnzlTlzZmtPXJLUokULDR48WL/++qtmz56tffv2aeLEifrhhx80Y8YMFSpUyI5rgpQWExOjN998U+Hh4dauWmNjY61dLzZr1kwbN2609rZUv359a4iYPXu2Nm/eLIlAmR7FxMTopZde0uTJk/Xdd98lOk1iIWLVqlWaM2eONUQg/SlatKg++OADSVKvXr3Uv39/7d69W++++6769++vjRs3WqfNly+fVq5cqYiICH300Ufav3+/vcrGYxAgnnMnT55U3rx5Vbx4cZud99atW/X5559r9OjRWrJkiaT7BwP9+vWTg4ODxo8fb9PtGtKPuXPn6vLly6pdu7by5csnR0dHRUVFKU+ePGrQoIHu3bunsLAw6/S+vr4aPHiwTp48qd69e2vt2rVasmQJN9OnQ5GRkfL09FRYWJjWrFkj6f5B4d27d9WuXTtlzZpV7du3l4uLi/Vscv369fX+++8rJCRES5Ys0Z07d+y5CkhhkZGRWrBggeLi4hQQEKDOnTvrs88++88QEb9tTJkyRS+99JI2b95MF7/pVPx9cVWrVlXx4sW1c+dOvfnmm5o9e7bef/99HTx4UMOGDZO/v7/27t2ry5cvK3/+/FqyZImyZ88ud3d3O68BHoV7IJ5j9+7dU1xcnKKiohQeHi5XV1ddu3ZNI0eO1J49e6wPgTIMQ8ePH9e4ceNUv359xcXF6ZtvvlGxYsXsvQpIBZ06ddK5c+c0YcIEZcuWTb6+vsqYMaMk6dq1azp69KhatGghT09PlS1bVl27dpWvr6+yZs2qTz75RDNnzpSnp6ed1wIpKTIyUsuXL1e7du00ePBgubi46NNPP1XGjBnVvn17tW3bVq6urpoyZYry588vSTZtl729vTVq1CgVK1ZMmTNntvPaIKVER0erS5cuOn36tP755x/16tVLH374oQzD0GeffSZJ8vPzs04fvz1YLBZduXJF9+7d0+zZsxUSEmJ9kBieffH7C19fX+v3WqJECdWuXVvLli3TqVOnVLZsWZUtW1Zt2rRRy5YttWvXLh08eFAlS5ZU69at1aVLF61bt87agxfSHrpxfc798ssvevXVV9WwYUO5urpq3759unXrlho3bmztYWfixIn66aefNH36dHl7e0u638yFA4H069q1a/rss8+0detWffbZZ/L19dX06dM1a9Ys1alTRzlz5tTRo0d18eJFubi4qGjRoho1apQ8PT2VKVMme5ePFBQdHa2OHTvq9OnTeuONN9SrVy9ZLBZNnDhRS5cuVdasWeXp6anPP/88QScKd+7c0d69e9WoUSM7VY/UdPHiRbVq1UpRUVEqV66cKlWqpD59+kiSvvzySy1ZskTDhg2Tn5+fTVv28+fPa8yYMYqJidG0adP4W5KOxO8vzpw5Iz8/P73zzjvWEHHz5k21atVK5cuX14wZMyRJgwYN0p49ezR58mQdOXJE27Zt07lz5xQUFKQ8efLYc1XwGFyBeM5VrFhRCxcu1NChQxUaGqrq1aurU6dOqlq1qnWnPmTIEG3btk0hISHW+djhp2958uTRsGHDJEnDhg3Tjz/+qD179mjcuHFq2LChMmXKpNu3b+vChQsKCgrS0aNHlSVLFsJDOvTPP//o3LlzMgxDhw8f1owZM9SnTx/1799fGTNm1HfffacaNWokCA8REREaN26cdu/erfLlyytv3rx2WgOkBsMwVKhQIQ0dOtR6L922bdtksVjUu3dvmysRhmHotddekySdO3dOEydO1J49e7R8+XL+lqQz8fuL2NhY/fzzz5o9e7Z69OghNzc3ZcyYUU2aNNGPP/6ow4cPa9GiRdq3b58+//xzVa1aVVWrVlXHjh0liStSzwACBFS1alV9//33CgsLszY/iBcXF6c///xTefLksXbJiedDnjx5NHz4cDk5OSk4OFht27ZVy5YtJd1v/pYtWzaVKVNGZcqUUVRUlLWZE9KPxx0k9ujRQ9HR0Zo+fbqyZ89uba4SHh6uCRMm6IcfftDSpUsJD+nMgw+HK1WqlNzd3dWmTRsdP35cQUFBkqTevXurf//+slgsGjNmjBwcHNSsWTN9/vnn2rNnj1auXKmSJUvaczWQwh7eX2TMmFE7duyQxWKRv7+/3Nzc1KFDB61Zs0b+/v7KmjWrJk6cqFq1alnfg+Dw7CBAQJKUJUsWZcmSRdL93jTie1cKCwtTcHCw9UZrPF9y586tQYMGyTAMrVmzRpUrV5avr6+cnJysN0JaLBbCQzpk9iDxgw8+UFxcnE2b9/Hjx2vdunVatmyZSpcubbd1QMqKjo6Wi4uLTbv0smXLKn/+/Jo7d65Wr14tR0dHbdiwQZJsrkSMGzdO8+fP161bt7RkyRLCQzrzX/uL+O3hnXfeUcmSJfXaa6/p22+/lb+/v+rUqWPPsvEECBBIID48HDt2TMuWLdOWLVu0aNEi5c6d286VwR7y5s2rjz76SLGxsdZmTb6+vnS5mE4l5yCxf//+kqTx48dr+fLlunTpkpYuXUp4SEfu3Lmjtm3bKl++fOrevbs8PDyszdb69++vN954Q6tXr9bgwYMVERFh7ZozfvuIjY3Vtm3btHjxYsJDOmJ2fxG/Pbz77ruqX7++vv32W507d06SbfjAs4MAgQRiYmLk7++vsLAwGYahRYsW0avOcy7+nghHR0cNGjRITk5Oat68ub3LQgp70oPEe/fuafv27XTjmw7NnTtXf//9t/7++29lz55dFy9elL+/v6pXry43NzdVq1ZNwcHB6tixoz7++GONHDlSGzdulKOjo3r06KH+/fvrvffeU65cuey9KkghSd1fBAcHy9HRUQMHDlTnzp313XffqVevXsqZM6ed1wTJQYBAAs7Oznr//fd17NgxNWnSJMF9EXg+5cmTR4MGDZKLiwuBMp1KiYPE999/n4PEdKhjx476559/FBwcLFdXV7Vp00ZDhgxRzZo11bBhQ7311ltq2rSpVq5cqQ4dOujjjz/Wxx9/rMWLF8vZ2Vn9+vXjqmU6k5z9RVBQkBwcHOTh4aHixYsrIiKCAPGMohtXPBKPkEdi7t27Jycnzj2kR1evXtX06dMVHBysxo0bq1SpUvryyy+tB4m1atVS06ZNNWLECHXo0EGS9PHHH2vt2rV64403OEhM5/755x9NmDBBwcHBmjlzpooVK6bly5dr2bJl8vDw0MWLF+Xp6alPPvnE2gXnuHHj1LlzZ73wwgv2LR4pLjn7i9GjR+vHH39UmzZt1L17d7pqfYYRIAAAVhwk4r9cu3ZNn376qbZu3apJkyapcePGunHjhiZNmqRTp07Jzc1NY8eOVfbs2eXg4GDvcpHKkrO/+Oyzz/Taa6+pcOHCdq4eT4IAAQCwwUEi/suD28fHH3+stm3bKjY2VlevXpWjoyPNXp8z7C+eTwQIAEACHCTivzz4tPqPP/5Ybdq0sXdJsCP2F88fGjIDABKIf5CgxWLRqFGjZLFY1KZNG7m7u9u7NKQBDz6tfuTIkXJyclKrVq3sXBXshf3F84cAAQBIFAeJ+C8Pdu88cOBAOTo60r3zc4z9xfOFAAEAeCQOEvFf6N4ZD2J/8fwgQAAA/hMHifgv+fLl0yeffEL3zpDE/uJ5wU3UAABTeAYIALPYX6RvBAgAAAAAptEhLwAAAADTCBAAAAAATCNAAAAAADCNAAEAAADANAIEAAAAANMIEAAAAABMI0AAANI8T09PTZs2LcnzXbx4UZ6enlqzZk0qVAUAzycCBADAtDVr1sjT01Oenp46fPhwgvGGYcjb21uenp7q0aOHHSoEAKQ2AgQAIMkyZMigoKCgBMMPHjyoq1evysXFxQ5VAQCeBgIEACDJvL29tXHjRt27d89meFBQkMqUKaM8efLYqTIAQGojQAAAkqxFixa6efOm9uzZYx0WHR2t4OBgtWrVKsH0d+7c0bhx4+Tt7a2yZcuqadOmmjdvngzDsJkuOjpaY8aMUY0aNeTl5aWePXvq6tWridYQEhKiIUOGqFatWipbtqxatGihVatWpeyKAgAScLJ3AQCAZ4+7u7sqVqyo9evXy9vbW5K0a9cuhYWFqXnz5vruu++s0xqGoV69eunAgQNq3769SpUqpZ9++kkTJkxQSEiIhg4dap122LBhWrdunVq2bKlKlSpp//798vf3T7D869evq2PHjrJYLOratavc3Ny0a9cuDRs2TOHh4XrjjTdS/TMAgOcVVyAAAMnSqlUrbdmyRVFRUZKkwMBAVa1aVfny5bOZbuvWrdq/f7/ee+89ffrpp+ratatmzZqlpk2b6ttvv9X58+clSadOndK6dev06quvauLEierataumTZuml156KcGyJ02apNjYWK1du1a9e/dWly5dNHPmTLVo0ULTp0+31gQASHkECABAsjRr1kx3797V9u3bFR4erh07diTafGnXrl1ydHSUn5+fzfDu3bvLMAzt2rVLkrRz505JSjDd66+/bvPaMAxt2rRJPj4+MgxDoaGh1n916tRRWFiYTpw4kZKrCgB4AE2YAADJ4ubmppo1ayooKEhRUVGKjY1V06ZNE0x36dIl5c2bV66urjbDixcvbh0f/7+Dg4OKFCliM12xYsVsXoeGhur27dtavny5li9fnmhtoaGhyV4vAMB/I0AAAJKtZcuWGjFihK5fv6569eopW7Zsqb7MuLg4SZKvr6/atGmT6DSenp6pXgcAPK8IEACAZGvcuLFGjRqlX375RZMmTUp0Gnd3d+3bt0/h4eE2VyH+/PNP6/j4/+Pi4nT+/Hmbqw7x08Vzc3NTlixZFBcXp1q1aqX0KgEAHoN7IAAAyZYlSxYFBASob9++8vHxSXSaevXqKTY2VosXL7YZvmDBAlksFtWrV886nSSbHpwkaeHChTavHR0d1bRpUwUHB+v06dMJlkfzJQBIXVyBAAA8kUc1I4rn4+Oj6tWra9KkSbp06ZI8PT21Z88ebd26Va+//rr1nodSpUqpZcuWWrJkicLCwuTl5aX9+/fr3LlzCd6zf//+OnDggDp27KgOHTqoRIkSunXrlk6cOKF9+/bp4MGDqbKuAAACBAAglTk4OGjmzJmaOnWqNmzYoDVr1sjd3V2DBg1S9+7dbaYdM2aMcubMqcDAQG3dulXVq1fXnDlzrM+aiJc7d26tXLlSM2bM0ObNm7V06VLlyJFDJUqU0IABA57m6gHAc8diPPwYUAAAAAB4BO6BAAAAAGAaAQIAAACAaQQIAAAAAKYRIAAAAACYRoAAAAAAYBoBAgAAAIBpBAgAAAAAphEgAAAAAJhGgAAAAABgGgECAAAAgGkECAAAAACmESAAAAAAmEaAAAAAAGDa/wORML9yRjL8yAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performance Comparison Summary:\n",
            "\n",
            "MAE (Mean Absolute Error):\n",
            "- Best performing model: MEF with MAE_mean=0.0114\n",
            "- Worst performing model: SN with MAE_mean=0.0177\n",
            "- Multimodal Late Fusion shows the lowest MAE, closely followed by Multimodal Early Fusion and Multimodal Attention Fusion. Ridge Baseline is also strong, while Seasonal Naive is significantly worse.\n",
            "\n",
            "RMSE (Root Mean Squared Error):\n",
            "- Best performing model: MLF with RMSE_mean=0.0157\n",
            "- Worst performing model: SN with RMSE_mean=0.0232\n",
            "- Multimodal Late Fusion has the lowest RMSE, indicating better handling of larger errors. Multimodal Early Fusion and Multimodal Attention Fusion are very similar. Ridge Baseline is competitive, and Seasonal Naive is the worst.\n",
            "\n",
            "DA (Directional Accuracy):\n",
            "- Best performing model: MLF with DA_mean=53.80%\n",
            "- Worst performing model: MAF with DA_mean=49.66%\n",
            "- Multimodal Late Fusion achieves the highest directional accuracy, indicating a better ability to predict the direction of stock price movements. Other models show comparable DA around 50%, with Seasonal Naive slightly underperforming in this metric.\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}