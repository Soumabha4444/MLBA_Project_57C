{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soumabha4444/MLBA_Project_57C/blob/main/Stock_Forecasting_Multimodal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spdTxIH1TPbt",
        "outputId": "9e43553b-7a70-4ac3-ea5c-41fd3ae610cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mlba-stock-forecasting\n"
          ]
        }
      ],
      "source": [
        "# create folders exactly as in the repo\n",
        "!mkdir -p mlba-stock-forecasting/{src,data/raw,data/processed,outputs,results}\n",
        "%cd mlba-stock-forecasting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr3BtVhxWZEr",
        "outputId": "13e77022-a27e-412a-9143-b8ded70dfe58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.12/dist-packages (0.2.66)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.32.4)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.0.12)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.5.0)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.12/dist-packages (from yfinance) (3.18.3)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.13.5)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.13.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (5.29.5)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.8)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (2025.10.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (2.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.23)\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance numpy pandas scikit-learn torch tqdm scipy matplotlib seaborn joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQW7r44X01t1",
        "outputId": "0d3ae35d-ed4a-4977-e63e-8ac48f26b321"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uYAuTEWWggC",
        "outputId": "0ba02622-ec68-4947-aa78-990225de1f57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/data.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/data.py\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import os\n",
        "\n",
        "def download_price(ticker, start, end, out_csv):\n",
        "    df = yf.download(ticker, start=start, end=end, progress=False)\n",
        "    df = df.reset_index().rename(columns={'Date':'date'})\n",
        "    df['ticker'] = ticker\n",
        "    os.makedirs(os.path.dirname(out_csv), exist_ok=True)\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(f\"Saved prices to {out_csv}\")\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    download_price(\"AAPL\", \"2008-01-01\", \"2016-10-31\", \"data/raw/aapl.csv\")\n",
        "\n",
        "# This downloads real AAPL stock data and saves it as data/raw/aapl.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYTYBTecXiB3",
        "outputId": "e0aa3332-028c-4f15-e544-925768f05314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/features.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/features.py\n",
        "import argparse\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def create_features(df, lags=(1,2,3,5,10,20), roll_mean_win=5, roll_std_win=20):\n",
        "    # normalize date column\n",
        "    if 'Date' in df.columns and 'date' not in df.columns:\n",
        "        df = df.rename(columns={'Date':'date'})\n",
        "    if 'date' not in df.columns:\n",
        "        raise ValueError(\"Input dataframe must contain a 'date' or 'Date' column.\")\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "\n",
        "    # standardize some common column names (map spaces -> underscores)\n",
        "    to_check = ['Open','High','Low','Close','Adj Close','Adj_Close','Volume']\n",
        "    for col in to_check:\n",
        "        if col in df.columns:\n",
        "            std_name = col.replace(' ', '_')\n",
        "            df[std_name] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # prefer adjusted close if Close missing\n",
        "    if 'Close' not in df.columns and 'Adj_Close' in df.columns:\n",
        "        df['Close'] = df['Adj_Close']\n",
        "\n",
        "    # accept lowercase variant\n",
        "    if 'close' in df.columns and 'Close' not in df.columns:\n",
        "        df['Close'] = pd.to_numeric(df['close'], errors='coerce')\n",
        "\n",
        "    # sort and drop rows missing Close\n",
        "    df = df.sort_values('date').copy().reset_index(drop=True)\n",
        "    df = df.dropna(subset=['Close']).reset_index(drop=True)\n",
        "\n",
        "    # create price-based target: next-day close (kept for reference)\n",
        "    df['close_next'] = df['Close'].shift(-1)\n",
        "\n",
        "    # next-day log return target (preferred for modeling)\n",
        "    # ret_next = log(close_next / Close)\n",
        "    df['ret_next'] = np.log(df['close_next'] / df['Close'])\n",
        "\n",
        "    # log return (today)\n",
        "    df['ret'] = np.log(df['Close']).diff()\n",
        "\n",
        "    # lag features\n",
        "    for l in lags:\n",
        "        df[f'ret_lag_{l}'] = df['ret'].shift(l)\n",
        "\n",
        "    # rolling stats\n",
        "    df[f'ret_roll_{roll_mean_win}_mean'] = df['ret'].rolling(roll_mean_win).mean()\n",
        "    df[f'ret_roll_{roll_std_win}_std'] = df['ret'].rolling(roll_std_win).std()\n",
        "\n",
        "    # list of columns required for modelling\n",
        "    cols_needed = ['ret_next'] + [f'ret_lag_{l}' for l in lags] + [f'ret_roll_{roll_mean_win}_mean', f'ret_roll_{roll_std_win}_std']\n",
        "\n",
        "    df = df.dropna(subset=cols_needed).reset_index(drop=True)\n",
        "\n",
        "    if df.empty:\n",
        "        raise ValueError(\"No rows remain after feature creation. Check input data length and column names.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def save_sample(df, out_csv, n=200):\n",
        "    os.makedirs(os.path.dirname(out_csv), exist_ok=True)\n",
        "    df.head(n).to_csv(out_csv, index=False)\n",
        "    print(\"Saved sample slice to\", out_csv)\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--input\", dest=\"infile\", required=False, default=\"data/raw/aapl.csv\",\n",
        "                        help=\"Input raw CSV file (has Date/Close/Adj Close etc.)\")\n",
        "    parser.add_argument(\"--out\", dest=\"outfile\", required=False, default=\"data/processed/aapl_features.parquet\")\n",
        "    parser.add_argument(\"--sample_out\", dest=\"sample_out\", default=\"data/sample_slice.csv\")\n",
        "    parser.add_argument(\"--sample_n\", type=int, default=200)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if not os.path.exists(args.infile):\n",
        "        raise SystemExit(f\"Input file not found: {args.infile}\")\n",
        "    df = pd.read_csv(args.infile)\n",
        "    df_feats = create_features(df)\n",
        "    os.makedirs(os.path.dirname(args.outfile), exist_ok=True)\n",
        "    df_feats.to_parquet(args.outfile, index=False)\n",
        "    save_sample(df_feats, args.sample_out, n=args.sample_n)\n",
        "    print(f\"Saved features to {args.outfile} with {len(df_feats)} rows\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yko0UqXNXzUZ",
        "outputId": "32d6480f-a53e-43db-d707-0f8068f9aa94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/train_baseline.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/train_baseline.py\n",
        "import argparse, os, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import math\n",
        "from scipy import stats\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    y_true = np.array(y_true); y_pred = np.array(y_pred)\n",
        "    return float(math.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
        "\n",
        "def directional_accuracy_returns(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Directional accuracy for returns: sign(pred) == sign(true)\n",
        "    Returns percentage in [0,100].\n",
        "    \"\"\"\n",
        "    y_true = np.array(y_true, dtype=float)\n",
        "    y_pred = np.array(y_pred, dtype=float)\n",
        "    n = min(len(y_true), len(y_pred))\n",
        "    if n == 0:\n",
        "        return 0.0\n",
        "    signs_pred = np.sign(y_pred[:n])\n",
        "    signs_true = np.sign(y_true[:n])\n",
        "    acc = np.mean(signs_pred == signs_true)\n",
        "    return float(100.0 * acc)\n",
        "\n",
        "def rolling_origin(df, initial_train_days=800, test_days=60, step_days=60):\n",
        "    dates = sorted(df['date'].unique())\n",
        "    folds=[]\n",
        "    start_idx=0\n",
        "    while True:\n",
        "        train_end = start_idx + initial_train_days - 1\n",
        "        test_start = train_end + 1\n",
        "        test_end = test_start + test_days - 1\n",
        "        if test_end >= len(dates): break\n",
        "        folds.append((dates[0], dates[train_end], dates[test_start], dates[test_end]))\n",
        "        start_idx += step_days\n",
        "    return folds\n",
        "\n",
        "def run_fold(df, train_end_date, test_start, test_end, features):\n",
        "    train_df = df[df['date']<=train_end_date]\n",
        "    test_df = df[(df['date']>=test_start) & (df['date']<=test_end)].copy().reset_index(drop=True)\n",
        "\n",
        "    X_train = train_df[features].values\n",
        "    y_train = train_df['ret_next'].values   # now predicting next-day log return\n",
        "    X_test = test_df[features].values\n",
        "    y_test = test_df['ret_next'].values\n",
        "\n",
        "    # Keep current_close for reference (not used for DA on returns)\n",
        "    current_close = test_df['Close'].values if 'Close' in test_df.columns else np.full_like(y_test, np.nan, dtype=float)\n",
        "\n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "    X_train = scaler.transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    model = Ridge(alpha=1.0)\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "\n",
        "    # compute metrics for this fold (on returns)\n",
        "    mae_val = float(mean_absolute_error(y_test, preds))\n",
        "    rmse_val = float(rmse(y_test, preds))\n",
        "    da_val = directional_accuracy_returns(y_test, preds)\n",
        "\n",
        "    # convert arrays to plain python lists (floats / None) for JSON\n",
        "    def as_pylist(arr, cast=float):\n",
        "        out = []\n",
        "        for x in list(arr):\n",
        "            if pd.isna(x) or (isinstance(x, float) and (np.isinf(x) or np.isnan(x))):\n",
        "                out.append(None)\n",
        "            else:\n",
        "                out.append(cast(x))\n",
        "        return out\n",
        "\n",
        "    return {\n",
        "        \"y_true\": as_pylist(y_test, float),\n",
        "        \"y_pred\": as_pylist(preds, float),\n",
        "        \"current_close\": as_pylist(current_close, float),\n",
        "        \"metrics\": {\n",
        "            \"MAE\": mae_val,\n",
        "            \"RMSE\": rmse_val,\n",
        "            \"DA_percent\": da_val\n",
        "        }\n",
        "    }\n",
        "\n",
        "def t_ci(x, alpha=0.95):\n",
        "    x = np.array(x, dtype=float); n=len(x)\n",
        "    if n == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    mean = float(np.mean(x))\n",
        "    if n == 1:\n",
        "        return mean, mean, mean\n",
        "    se = float(np.std(x, ddof=1)/math.sqrt(n))\n",
        "    t = stats.t.ppf((1+alpha)/2., n-1)\n",
        "    return mean, mean - t*se, mean + t*se\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--features\", required=False, default=\"data/processed/aapl_features.parquet\")\n",
        "    parser.add_argument(\"--out\", required=False, default=\"outputs/baseline_preds.json\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    df = pd.read_parquet(args.features)\n",
        "    df = df.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "    # pick feature columns (lags & rolling stats)\n",
        "    features = [c for c in df.columns if c.startswith('ret_lag_') or c.startswith('ret_roll_')]\n",
        "\n",
        "    folds = rolling_origin(df)\n",
        "    os.makedirs(os.path.dirname(args.out), exist_ok=True)\n",
        "    all_preds=[]\n",
        "\n",
        "    for idx,(train_start, train_end, test_start, test_end) in enumerate(folds):\n",
        "        print(f\"Fold {idx}: train_end={train_end}, test_start={test_start}, test_end={test_end}\")\n",
        "        rec = run_fold(df, train_end, test_start, test_end, features)\n",
        "        # stringify timestamps\n",
        "        def to_str(d):\n",
        "            try:\n",
        "                return d.isoformat()\n",
        "            except Exception:\n",
        "                return str(d)\n",
        "        out_rec = {\n",
        "            \"fold\": int(idx),\n",
        "            \"model\": \"ridge_baseline\",\n",
        "            \"train_start\": to_str(train_start),\n",
        "            \"train_end\": to_str(train_end),\n",
        "            \"test_start\": to_str(test_start),\n",
        "            \"test_end\": to_str(test_end),\n",
        "            \"y_true\": rec[\"y_true\"],\n",
        "            \"y_pred\": rec[\"y_pred\"],\n",
        "            \"current_close\": rec[\"current_close\"],\n",
        "            \"metrics\": rec[\"metrics\"]\n",
        "        }\n",
        "        all_preds.append(out_rec)\n",
        "\n",
        "    # compute summary across folds\n",
        "    maes = [r[\"metrics\"][\"MAE\"] for r in all_preds]\n",
        "    rmses = [r[\"metrics\"][\"RMSE\"] for r in all_preds]\n",
        "    das = [r[\"metrics\"][\"DA_percent\"] for r in all_preds]\n",
        "\n",
        "    summary = {\n",
        "        \"MAE\": { \"mean\": t_ci(maes)[0], \"lo\": t_ci(maes)[1], \"hi\": t_ci(maes)[2] },\n",
        "        \"RMSE\": { \"mean\": t_ci(rmses)[0], \"lo\": t_ci(rmses)[1], \"hi\": t_ci(rmses)[2] },\n",
        "        \"DA_percent\": { \"mean\": t_ci(das)[0], \"lo\": t_ci(das)[1], \"hi\": t_ci(das)[2] }\n",
        "    }\n",
        "\n",
        "    out_blob = {\n",
        "        \"per_fold\": all_preds,\n",
        "        \"summary\": summary\n",
        "    }\n",
        "\n",
        "    with open(args.out,\"w\") as f:\n",
        "        json.dump(out_blob,f, indent=2)\n",
        "    print(\"Saved preds and metrics to\", args.out)\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAfz_SWFX45p",
        "outputId": "55396d73-959a-49b7-c43f-d1d9b448dd3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/eval.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/eval.py\n",
        "import json, argparse, os, math, sys\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from scipy import stats\n",
        "\n",
        "def t_confidence_interval(x, confidence=0.95):\n",
        "    x = np.array(x, dtype=float)\n",
        "    n = len(x)\n",
        "    if n == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    mean = float(np.mean(x))\n",
        "    if n == 1:\n",
        "        return mean, mean, mean\n",
        "    se = float(np.std(x, ddof=1) / math.sqrt(n))\n",
        "    t = stats.t.ppf((1 + confidence) / 2.0, n - 1)\n",
        "    return mean, mean - t * se, mean + t * se\n",
        "\n",
        "def directional_accuracy_filtered(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute directional accuracy (%) for returns.\n",
        "    Filters out non-finite entries and aligns arrays.\n",
        "    \"\"\"\n",
        "    # convert to numpy arrays and attempt to coerce to float\n",
        "    y_t = np.array([_safe_float(x) for x in y_true], dtype=float)\n",
        "    y_p = np.array([_safe_float(x) for x in y_pred], dtype=float)\n",
        "\n",
        "    # mask finite entries in both\n",
        "    mask = np.isfinite(y_t) & np.isfinite(y_p)\n",
        "    if mask.sum() == 0:\n",
        "        return 0.0\n",
        "    y_t = y_t[mask]\n",
        "    y_p = y_p[mask]\n",
        "    signs_true = np.sign(y_t)\n",
        "    signs_pred = np.sign(y_p)\n",
        "    acc = np.mean(signs_true == signs_pred)\n",
        "    return float(100.0 * acc)\n",
        "\n",
        "def _safe_float(x):\n",
        "    try:\n",
        "        v = float(x)\n",
        "        if math.isfinite(v):\n",
        "            return v\n",
        "    except Exception:\n",
        "        pass\n",
        "    return float('nan')\n",
        "\n",
        "def load_preds(path):\n",
        "    blob = json.load(open(path))\n",
        "    if isinstance(blob, dict) and \"per_fold\" in blob:\n",
        "        records = blob[\"per_fold\"]\n",
        "    elif isinstance(blob, list):\n",
        "        records = blob\n",
        "    else:\n",
        "        raise ValueError(\"Unrecognized preds_json structure. Expect list or dict with 'per_fold'.\")\n",
        "    return records\n",
        "\n",
        "def t_confidence_interval_list(x):\n",
        "    return t_confidence_interval(x)\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--preds_json\", required=True)\n",
        "    parser.add_argument(\"--out\", required=True)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    records = load_preds(args.preds_json)\n",
        "\n",
        "    maes, rmses, das = [], [], []\n",
        "    for rec in records:\n",
        "        y_true = rec.get(\"y_true\", [])\n",
        "        y_pred = rec.get(\"y_pred\", [])\n",
        "        # compute per-fold metrics, making sure to coerce/filter invalid entries\n",
        "        # MAE/RMSE: only use aligned finite pairs\n",
        "        y_t = np.array([_safe_float(x) for x in y_true], dtype=float)\n",
        "        y_p = np.array([_safe_float(x) for x in y_pred], dtype=float)\n",
        "        mask = np.isfinite(y_t) & np.isfinite(y_p)\n",
        "        if mask.sum() == 0:\n",
        "            continue\n",
        "        y_t_f = y_t[mask]\n",
        "        y_p_f = y_p[mask]\n",
        "        maes.append(float(mean_absolute_error(y_t_f, y_p_f)))\n",
        "        rmses.append(float(math.sqrt(mean_squared_error(y_t_f, y_p_f))))\n",
        "        das.append(directional_accuracy_filtered(y_t_f, y_p_f))\n",
        "\n",
        "    mae_mean, mae_lo, mae_hi = t_confidence_interval_list(maes)\n",
        "    rmse_mean, rmse_lo, rmse_hi = t_confidence_interval_list(rmses)\n",
        "    da_mean, da_lo, da_hi = t_confidence_interval_list(das)\n",
        "\n",
        "    out = {\n",
        "        \"MAE_mean\": mae_mean, \"MAE_lo\": mae_lo, \"MAE_hi\": mae_hi,\n",
        "        \"RMSE_mean\": rmse_mean, \"RMSE_lo\": rmse_lo, \"RMSE_hi\": rmse_hi,\n",
        "        \"DA_mean\": da_mean, \"DA_lo\": da_lo, \"DA_hi\": da_hi\n",
        "    }\n",
        "\n",
        "    os.makedirs(os.path.dirname(args.out), exist_ok=True)\n",
        "    with open(args.out, \"w\") as f:\n",
        "        json.dump(out, f, indent=2)\n",
        "    print(\"Saved metrics to\", args.out)\n",
        "    print(json.dumps(out, indent=2))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "Ge-e0YHOYANH",
        "outputId": "ed9aa44f-3b0d-4dad-c22c-a150414af2cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mlba-stock-forecasting/src/data.py:8: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=start, end=end, progress=False)\n",
            "Saved prices to data/raw/aapl.csv\n",
            "COLUMNS: ['date', 'Close', 'High', 'Low', 'Open', 'Volume', 'ticker']\n",
            "\n",
            "FIRST 6 ROWS:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         date               Close               High                 Low  \\\n",
              "0         NaN                AAPL               AAPL                AAPL   \n",
              "1  2008-01-02   5.849120140075684  6.011829717984964   5.780374892495661   \n",
              "2  2008-01-03   5.851821422576904  5.925670996791083   5.784576425208865   \n",
              "3  2008-01-04   5.405121326446533   5.79388187846163   5.370298399997362   \n",
              "4  2008-01-07   5.332773208618164   5.51169317331155  5.1103238843778405   \n",
              "5  2008-01-08  5.1409454345703125  5.477471471304045  5.1274364720363845   \n",
              "\n",
              "                Open      Volume ticker  \n",
              "0               AAPL        AAPL    NaN  \n",
              "1  5.982109918767337  1079178800   AAPL  \n",
              "2  5.866231408423027   842066400   AAPL  \n",
              "3  5.747350839715102  1455832000   AAPL  \n",
              "4  5.441145493849881  2072193200   AAPL  \n",
              "5  5.407823999678407  1523816000   AAPL  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-11fad13e-cb88-415e-8776-389ef81ed646\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Open</th>\n",
              "      <th>Volume</th>\n",
              "      <th>ticker</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2008-01-02</td>\n",
              "      <td>5.849120140075684</td>\n",
              "      <td>6.011829717984964</td>\n",
              "      <td>5.780374892495661</td>\n",
              "      <td>5.982109918767337</td>\n",
              "      <td>1079178800</td>\n",
              "      <td>AAPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2008-01-03</td>\n",
              "      <td>5.851821422576904</td>\n",
              "      <td>5.925670996791083</td>\n",
              "      <td>5.784576425208865</td>\n",
              "      <td>5.866231408423027</td>\n",
              "      <td>842066400</td>\n",
              "      <td>AAPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2008-01-04</td>\n",
              "      <td>5.405121326446533</td>\n",
              "      <td>5.79388187846163</td>\n",
              "      <td>5.370298399997362</td>\n",
              "      <td>5.747350839715102</td>\n",
              "      <td>1455832000</td>\n",
              "      <td>AAPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2008-01-07</td>\n",
              "      <td>5.332773208618164</td>\n",
              "      <td>5.51169317331155</td>\n",
              "      <td>5.1103238843778405</td>\n",
              "      <td>5.441145493849881</td>\n",
              "      <td>2072193200</td>\n",
              "      <td>AAPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2008-01-08</td>\n",
              "      <td>5.1409454345703125</td>\n",
              "      <td>5.477471471304045</td>\n",
              "      <td>5.1274364720363845</td>\n",
              "      <td>5.407823999678407</td>\n",
              "      <td>1523816000</td>\n",
              "      <td>AAPL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11fad13e-cb88-415e-8776-389ef81ed646')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-11fad13e-cb88-415e-8776-389ef81ed646 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-11fad13e-cb88-415e-8776-389ef81ed646');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5d89f5d8-d58d-4d12-8247-7e794d3510dc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5d89f5d8-d58d-4d12-8247-7e794d3510dc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5d89f5d8-d58d-4d12-8247-7e794d3510dc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"\\\\nDTYPES:\\\\n\\\", df\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2008-01-02 00:00:00\",\n        \"max\": \"2008-01-08 00:00:00\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2008-01-03\",\n          \"2008-01-08\",\n          \"2008-01-04\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Close\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"AAPL\",\n          \"5.849120140075684\",\n          \"5.1409454345703125\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"High\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"AAPL\",\n          \"6.011829717984964\",\n          \"5.477471471304045\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Low\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"AAPL\",\n          \"5.780374892495661\",\n          \"5.1274364720363845\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Open\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"AAPL\",\n          \"5.982109918767337\",\n          \"5.407823999678407\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Volume\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"AAPL\",\n          \"1079178800\",\n          \"1523816000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ticker\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"AAPL\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DTYPES:\n",
            " date      object\n",
            "Close     object\n",
            "High      object\n",
            "Low       object\n",
            "Open      object\n",
            "Volume    object\n",
            "ticker    object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "!python src/data.py\n",
        "# inspect downloaded CSV without parse_dates\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"data/raw/aapl.csv\")\n",
        "print(\"COLUMNS:\", df.columns.tolist())\n",
        "print(\"\\nFIRST 6 ROWS:\")\n",
        "display(df.head(6))\n",
        "print(\"\\nDTYPES:\\n\", df.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wv9LEdDGZClK",
        "outputId": "3fa91e09-18c4-4773-c882-3ff906a958a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (18.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyarrow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ouga1vMawwN",
        "outputId": "989fd3e1-b906-41ce-e762-7ffaf98cad20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved sample slice to data/sample_slice.csv\n",
            "Saved features to data/processed/aapl_features.parquet with 2202 rows\n"
          ]
        }
      ],
      "source": [
        "!python src/features.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_NnZeoUbIRr",
        "outputId": "4d4297e7-54b1-4bf1-f1f3-bd96201b8491"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0: train_end=2011-04-04 00:00:00, test_start=2011-04-05 00:00:00, test_end=2011-06-29 00:00:00\n",
            "Fold 1: train_end=2011-06-29 00:00:00, test_start=2011-06-30 00:00:00, test_end=2011-09-23 00:00:00\n",
            "Fold 2: train_end=2011-09-23 00:00:00, test_start=2011-09-26 00:00:00, test_end=2011-12-19 00:00:00\n",
            "Fold 3: train_end=2011-12-19 00:00:00, test_start=2011-12-20 00:00:00, test_end=2012-03-16 00:00:00\n",
            "Fold 4: train_end=2012-03-16 00:00:00, test_start=2012-03-19 00:00:00, test_end=2012-06-12 00:00:00\n",
            "Fold 5: train_end=2012-06-12 00:00:00, test_start=2012-06-13 00:00:00, test_end=2012-09-06 00:00:00\n",
            "Fold 6: train_end=2012-09-06 00:00:00, test_start=2012-09-07 00:00:00, test_end=2012-12-04 00:00:00\n",
            "Fold 7: train_end=2012-12-04 00:00:00, test_start=2012-12-05 00:00:00, test_end=2013-03-04 00:00:00\n",
            "Fold 8: train_end=2013-03-04 00:00:00, test_start=2013-03-05 00:00:00, test_end=2013-05-29 00:00:00\n",
            "Fold 9: train_end=2013-05-29 00:00:00, test_start=2013-05-30 00:00:00, test_end=2013-08-22 00:00:00\n",
            "Fold 10: train_end=2013-08-22 00:00:00, test_start=2013-08-23 00:00:00, test_end=2013-11-15 00:00:00\n",
            "Fold 11: train_end=2013-11-15 00:00:00, test_start=2013-11-18 00:00:00, test_end=2014-02-13 00:00:00\n",
            "Fold 12: train_end=2014-02-13 00:00:00, test_start=2014-02-14 00:00:00, test_end=2014-05-12 00:00:00\n",
            "Fold 13: train_end=2014-05-12 00:00:00, test_start=2014-05-13 00:00:00, test_end=2014-08-06 00:00:00\n",
            "Fold 14: train_end=2014-08-06 00:00:00, test_start=2014-08-07 00:00:00, test_end=2014-10-30 00:00:00\n",
            "Fold 15: train_end=2014-10-30 00:00:00, test_start=2014-10-31 00:00:00, test_end=2015-01-28 00:00:00\n",
            "Fold 16: train_end=2015-01-28 00:00:00, test_start=2015-01-29 00:00:00, test_end=2015-04-24 00:00:00\n",
            "Fold 17: train_end=2015-04-24 00:00:00, test_start=2015-04-27 00:00:00, test_end=2015-07-21 00:00:00\n",
            "Fold 18: train_end=2015-07-21 00:00:00, test_start=2015-07-22 00:00:00, test_end=2015-10-14 00:00:00\n",
            "Fold 19: train_end=2015-10-14 00:00:00, test_start=2015-10-15 00:00:00, test_end=2016-01-11 00:00:00\n",
            "Fold 20: train_end=2016-01-11 00:00:00, test_start=2016-01-12 00:00:00, test_end=2016-04-07 00:00:00\n",
            "Fold 21: train_end=2016-04-07 00:00:00, test_start=2016-04-08 00:00:00, test_end=2016-07-01 00:00:00\n",
            "Fold 22: train_end=2016-07-01 00:00:00, test_start=2016-07-05 00:00:00, test_end=2016-09-27 00:00:00\n",
            "Saved preds and metrics to outputs/baseline_preds.json\n"
          ]
        }
      ],
      "source": [
        "!python src/train_baseline.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/eval.py --preds_json outputs/baseline_preds.json --out outputs/baseline_metrics.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6gbo3iIV7nd",
        "outputId": "a3d97e62-e02a-404a-f386-41fd97d51ea1"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved metrics to outputs/baseline_metrics.json\n",
            "{\n",
            "  \"MAE_mean\": 0.012199044427849321,\n",
            "  \"MAE_lo\": 0.011022202108742588,\n",
            "  \"MAE_hi\": 0.013375886746956054,\n",
            "  \"RMSE_mean\": 0.016519847123619435,\n",
            "  \"RMSE_lo\": 0.014884446166904753,\n",
            "  \"RMSE_hi\": 0.018155248080334117,\n",
            "  \"DA_mean\": 50.362318840579704,\n",
            "  \"DA_lo\": 46.81180730245322,\n",
            "  \"DA_hi\": 53.91283037870619\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python - <<'PY'\n",
        "import pandas as pd, numpy as np, json, math\n",
        "from scipy import stats\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# load processed features and preds\n",
        "df = pd.read_parquet(\"data/processed/aapl_features.parquet\")\n",
        "blob = json.load(open(\"outputs/baseline_preds.json\"))\n",
        "\n",
        "# support both formats: list OR {\"per_fold\": [...], \"summary\": {...}}\n",
        "if isinstance(blob, dict) and \"per_fold\" in blob:\n",
        "    all_preds = blob[\"per_fold\"]\n",
        "else:\n",
        "    all_preds = blob\n",
        "\n",
        "# helper: compute RMSE\n",
        "def rmse(y_true, y_pred):\n",
        "    y_true = np.array(y_true); y_pred = np.array(y_pred)\n",
        "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
        "\n",
        "# reconstruct folds used in train_baseline.py (same rolling_origin logic)\n",
        "dates = sorted(df['date'].unique())\n",
        "initial_train_days=800; test_days=60; step_days=60\n",
        "folds=[]\n",
        "start_idx=0\n",
        "while True:\n",
        "    train_end = start_idx + initial_train_days - 1\n",
        "    test_start = train_end + 1\n",
        "    test_end = test_start + test_days - 1\n",
        "    if test_end >= len(dates): break\n",
        "    folds.append((dates[0], dates[train_end], dates[test_start], dates[test_end]))\n",
        "    start_idx += step_days\n",
        "\n",
        "naive_maes=[]; naive_rmses=[]; naive_das=[]\n",
        "ridge_maes=[]; ridge_rmses=[]; ridge_das=[]\n",
        "# load ridge per-fold results (make sure fold keys are ints)\n",
        "ridge_by_fold = {}\n",
        "for rec in all_preds:\n",
        "    fold_key = rec.get('fold')\n",
        "    if fold_key is None:\n",
        "        continue\n",
        "    ridge_by_fold[int(fold_key)] = rec\n",
        "\n",
        "for i,(train_start, train_end, test_start, test_end) in enumerate(folds):\n",
        "    test_mask = (df['date']>=test_start)&(df['date']<=test_end)\n",
        "    test_df = df[test_mask].copy().reset_index(drop=True)\n",
        "    # find global indices in original df\n",
        "    global_idx = df.index[(df['date']>=test_start) & (df['date']<=test_end)].tolist()\n",
        "\n",
        "    # Handle cases where global_idx might be too early for a previous day's close\n",
        "    # The 'ret_next' column is available, so this price-scale path is likely not taken.\n",
        "    # However, for robustness, ensuring prev_close is handled correctly.\n",
        "    if global_idx and min(global_idx) > 0:\n",
        "      prev_idx_for_close_comparison = [idx-1 for idx in global_idx]\n",
        "      prev_close_for_comparison = df.loc[prev_idx_for_close_comparison, 'Close'].values\n",
        "    else:\n",
        "      prev_close_for_comparison = np.array([])\n",
        "\n",
        "    # Decide whether we are working in returns or price scale\n",
        "    if 'ret_next' in df.columns:\n",
        "        # return-scale naive: predict previous day's return (ret_lag_1) if present else zero\n",
        "        y_true = df.loc[global_idx, 'ret_next'].values\n",
        "        if 'ret_lag_1' in df.columns:\n",
        "            naive_pred = df.loc[global_idx, 'ret_lag_1'].values\n",
        "            if len(naive_pred) != len(y_true):\n",
        "                naive_pred = naive_pred[:len(y_true)]\n",
        "        else:\n",
        "            naive_pred = np.zeros_like(y_true)\n",
        "\n",
        "        # Ensure y_true and naive_pred are not empty before computing metrics\n",
        "        if len(y_true) > 0 and len(naive_pred) > 0:\n",
        "            naive_maes.append(float(mean_absolute_error(y_true, naive_pred)))\n",
        "            naive_rmses.append(rmse(y_true, naive_pred))\n",
        "            da_naive = np.mean(np.sign(naive_pred) == np.sign(y_true))\n",
        "            naive_das.append(float(100.0 * da_naive))\n",
        "        else:\n",
        "            naive_maes.append(0.0)\n",
        "            naive_rmses.append(0.0)\n",
        "            naive_das.append(0.0)\n",
        "    else:\n",
        "        # This block is likely dead code since 'ret_next' is expected\n",
        "        # price-scale naive: previous day's Close predicts next-day close\n",
        "        y_true = df.loc[global_idx, 'close_next'].values\n",
        "        naive_pred = prev_close_for_comparison # Assuming prev_close_for_comparison here is Close_t for close_next_t+1\n",
        "        # Ensure y_true and naive_pred have the same length for MAE/RMSE\n",
        "        min_len = min(len(y_true), len(naive_pred))\n",
        "        if min_len == 0:\n",
        "            naive_maes.append(0.0)\n",
        "            naive_rmses.append(0.0)\n",
        "            naive_das.append(0.0)\n",
        "        else:\n",
        "            naive_maes.append(float(mean_absolute_error(y_true[:min_len], naive_pred[:min_len])))\n",
        "            naive_rmses.append(rmse(y_true[:min_len], naive_pred[:min_len]))\n",
        "            # For directional accuracy, compare (y_true - prev_close) with (naive_pred - prev_close)\n",
        "            if len(prev_close_for_comparison) >= min_len:\n",
        "                da_naive = np.mean(np.sign(naive_pred[:min_len] - prev_close_for_comparison[:min_len]) == np.sign(y_true[:min_len] - prev_close_for_comparison[:min_len]))\n",
        "            else:\n",
        "                da_naive = 0.0 # Not enough data for comparison\n",
        "            naive_das.append(float(100.0 * da_naive))\n",
        "\n",
        "    # ridge per-fold\n",
        "    ridge_rec = ridge_by_fold.get(i)\n",
        "    if ridge_rec is None:\n",
        "        # fold missing in preds JSON\n",
        "        continue\n",
        "\n",
        "    ridge_y = np.array(ridge_rec.get('y_true', []))\n",
        "    ridge_p = np.array(ridge_rec.get('y_pred', []))\n",
        "    # filter finite and align\n",
        "    mask = np.isfinite(ridge_y) & np.isfinite(ridge_p)\n",
        "    ridge_y = ridge_y[mask]; ridge_p = ridge_p[mask]\n",
        "    if len(ridge_y) == 0:\n",
        "        continue\n",
        "\n",
        "    ridge_maes.append(float(mean_absolute_error(ridge_y, ridge_p)))\n",
        "    ridge_rmses.append(rmse(ridge_y, ridge_p))\n",
        "\n",
        "    # Directional accuracy for Ridge model (always on returns as per train_baseline.py)\n",
        "    da_ridge = np.mean(np.sign(ridge_p) == np.sign(ridge_y))\n",
        "    ridge_das.append(float(100.0 * da_ridge))\n",
        "\n",
        "def t_ci(x,alpha=0.95):\n",
        "    x = np.array(x); n=len(x)\n",
        "    mean = float(np.mean(x)) if n>0 else 0.0\n",
        "    se = float(np.std(x, ddof=1)/math.sqrt(n)) if n>1 else 0.0\n",
        "    t = stats.t.ppf((1+alpha)/2., n-1) if n>1 else 0.0\n",
        "    return mean, mean - t*se, mean + t*se\n",
        "\n",
        "out = {\n",
        " \"seasonal_naive\": {\n",
        "   \"MAE_mean\": t_ci(naive_maes)[0], \"MAE_lo\": t_ci(naive_maes)[1], \"MAE_hi\": t_ci(naive_maes)[2],\n",
        "   \"RMSE_mean\": t_ci(naive_rmses)[0], \"RMSE_lo\": t_ci(naive_rmses)[1], \"RMSE_hi\": t_ci(naive_rmses)[2],\n",
        "   \"DA_mean\": t_ci(naive_das)[0], \"DA_lo\": t_ci(naive_das)[1], \"DA_hi\": t_ci(naive_das)[2]\n",
        " },\n",
        " \"ridge_baseline\": {\n",
        "   \"MAE_mean\": t_ci(ridge_maes)[0], \"MAE_lo\": t_ci(ridge_maes)[1], \"MAE_hi\": t_ci(ridge_maes)[2],\n",
        "   \"RMSE_mean\": t_ci(ridge_rmses)[0], \"RMSE_lo\": t_ci(ridge_rmses)[1], \"RMSE_hi\": t_ci(ridge_rmses)[2],\n",
        "   \"DA_mean\": t_ci(ridge_das)[0], \"DA_lo\": t_ci(ridge_das)[1], \"DA_hi\": t_ci(ridge_das)[2]\n",
        " }\n",
        "}\n",
        "\n",
        "# Directly print the 'out' dictionary as pretty-formatted JSON\n",
        "print(json.dumps(out, indent=2))\n",
        "PY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZszrbKaa1q0",
        "outputId": "003518b2-5ffe-4c8d-a221-4ed3a2a02394"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"seasonal_naive\": {\n",
            "    \"MAE_mean\": 0.017733725954873138,\n",
            "    \"MAE_lo\": 0.015843874602422075,\n",
            "    \"MAE_hi\": 0.0196235773073242,\n",
            "    \"RMSE_mean\": 0.023151456202389058,\n",
            "    \"RMSE_lo\": 0.0205974700969722,\n",
            "    \"RMSE_hi\": 0.025705442307805915,\n",
            "    \"DA_mean\": 49.71014492753623,\n",
            "    \"DA_lo\": 46.96441334605667,\n",
            "    \"DA_hi\": 52.45587650901578\n",
            "  },\n",
            "  \"ridge_baseline\": {\n",
            "    \"MAE_mean\": 0.012199044427849321,\n",
            "    \"MAE_lo\": 0.011022202108742588,\n",
            "    \"MAE_hi\": 0.013375886746956054,\n",
            "    \"RMSE_mean\": 0.016519847123619435,\n",
            "    \"RMSE_lo\": 0.014884446166904753,\n",
            "    \"RMSE_hi\": 0.018155248080334117,\n",
            "    \"DA_mean\": 50.362318840579704,\n",
            "    \"DA_lo\": 46.81180730245322,\n",
            "    \"DA_hi\": 53.91283037870619\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72440d20",
        "outputId": "7c899bf1-0e3a-47a4-c741-4bade4917c25"
      },
      "source": [
        "%%writefile src/train_lstm.py\n",
        "#!/usr/bin/env python3\n",
        "# save as src/train_lstm.py\n",
        "import argparse, os, json, math\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from scipy import stats\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ---------------------------\n",
        "# PyTorch LSTM model\n",
        "# ---------------------------\n",
        "class SeqDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        # X: (#samples, seq_len, n_features), y: (#samples,)\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = y.astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class LSTMRegressor(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=64, num_layers=1, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
        "                            batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, F)\n",
        "        out, _ = self.lstm(x)           # out: (B, T, hidden)\n",
        "        out = out[:, -1, :]             # take last timestep\n",
        "        out = self.fc(out)              # (B,1)\n",
        "        return out.squeeze(1)          # (B,)\n",
        "\n",
        "# ---------------------------\n",
        "# Utilities\n",
        "# ---------------------------\n",
        "def rmse(y_true, y_pred):\n",
        "    y_true = np.array(y_true); y_pred = np.array(y_pred)\n",
        "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
        "\n",
        "def t_ci(x, alpha=0.95):\n",
        "    x = np.array(x, dtype=float); n=len(x)\n",
        "    if n == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    mean = float(np.mean(x))\n",
        "    if n == 1:\n",
        "        return mean, mean, mean\n",
        "    se = float(np.std(x, ddof=1)/math.sqrt(n))\n",
        "    t = stats.t.ppf((1+alpha)/2., n-1)\n",
        "    return mean, mean - t*se, mean + t*se\n",
        "\n",
        "def directional_accuracy_percent(y_true, y_pred):\n",
        "    y_true = np.array(y_true, dtype=float)\n",
        "    y_pred = np.array(y_pred, dtype=float)\n",
        "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
        "    if mask.sum() == 0:\n",
        "        return 0.0\n",
        "    y_true = y_true[mask]; y_pred = y_pred[mask]\n",
        "    return float(100.0 * np.mean(np.sign(y_true) == np.sign(y_pred)))\n",
        "\n",
        "# ---------------------------\n",
        "# Build sliding sequences\n",
        "# ---------------------------\n",
        "def build_sequences(df, features, seq_len):\n",
        "    \"\"\"\n",
        "    Given sorted df, returns:\n",
        "      indices: list of indices i for which sequence X[i] ends at row i (so target is at row i)\n",
        "      X_seq: np.array (#samples, seq_len, n_features)\n",
        "      y: np.array (#samples,) -> df.loc[i, 'ret_next']\n",
        "    Only rows where full seq_len history exists are included.\n",
        "    \"\"\"\n",
        "    n = len(df)\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "    idx_list = []\n",
        "    arr = df[features].values\n",
        "    targets = df['ret_next'].values\n",
        "    for end_idx in range(seq_len - 1, n):\n",
        "        start_idx = end_idx - (seq_len - 1)\n",
        "        # ensure target exists and finite\n",
        "        if not np.isfinite(targets[end_idx]):\n",
        "            continue\n",
        "        seq = arr[start_idx:end_idx+1]  # shape (seq_len, n_features)\n",
        "        if np.any(~np.isfinite(seq)):\n",
        "            continue\n",
        "        X_list.append(seq)\n",
        "        y_list.append(targets[end_idx])\n",
        "        idx_list.append(end_idx)\n",
        "    if len(X_list) == 0:\n",
        "        return np.empty((0, seq_len, len(features))), np.empty((0,)), []\n",
        "    return np.stack(X_list), np.array(y_list), idx_list\n",
        "\n",
        "# ---------------------------\n",
        "# Train per-fold LSTM\n",
        "# ---------------------------\n",
        "def train_epoch(model, loader, opt, loss_fn, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for Xb, yb in loader:\n",
        "        Xb = Xb.to(device); yb = yb.to(device)\n",
        "        opt.zero_grad()\n",
        "        out = model(Xb)\n",
        "        loss = loss_fn(out, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += float(loss.item()) * Xb.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def predict(model, loader, device):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    trues = []\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in loader:\n",
        "            Xb = Xb.to(device)\n",
        "            out = model(Xb).cpu().numpy()\n",
        "            preds.append(out)\n",
        "            trues.append(yb.numpy())\n",
        "    if len(preds) == 0:\n",
        "        return np.array([]), np.array([])\n",
        "    return np.concatenate(trues), np.concatenate(preds)\n",
        "\n",
        "# ---------------------------\n",
        "# Main: rolling folds\n",
        "# ---------------------------\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--features\", default=\"data/processed/aapl_features.parquet\")\n",
        "    parser.add_argument(\"--out\", default=\"outputs/lstm_preds.json\")\n",
        "    parser.add_argument(\"--seq_len\", type=int, default=10)\n",
        "    parser.add_argument(\"--hidden\", type=int, default=64)\n",
        "    parser.add_argument(\"--epochs\", type=int, default=20)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=64)\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    parser.add_argument(\"--device\", default=\"cpu\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    df = pd.read_parquet(args.features).sort_values('date').reset_index(drop=True)\n",
        "    # choose features to feed LSTM per-timestep\n",
        "    features = [c for c in df.columns if c.startswith('ret_lag_') or c.startswith('ret_roll_') or c in ['ret']]\n",
        "    if len(features) == 0:\n",
        "        raise SystemExit(\"No features found for LSTM. Check your processed parquet columns.\")\n",
        "\n",
        "    # rolling-origin folds (same as baseline)\n",
        "    dates = sorted(df['date'].unique())\n",
        "    initial_train_days=800; test_days=60; step_days=60\n",
        "    folds=[]\n",
        "    start_idx=0\n",
        "    while True:\n",
        "        train_end = start_idx + initial_train_days - 1\n",
        "        test_start = train_end + 1\n",
        "        test_end = test_start + test_days - 1\n",
        "        if test_end >= len(dates): break\n",
        "        folds.append((dates[0], dates[train_end], dates[test_start], dates[test_end]))\n",
        "        start_idx += step_days\n",
        "\n",
        "    device = torch.device(args.device if torch.cuda.is_available() or args.device=='cpu' else 'cpu')\n",
        "    out_recs = []\n",
        "\n",
        "    for idx, (train_start, train_end, test_start, test_end) in enumerate(folds):\n",
        "        print(f\"[Fold {idx}] train_end={train_end}, test_start={test_start}, test_end={test_end}\")\n",
        "        # split df by date\n",
        "        train_mask = df['date'] <= train_end\n",
        "        test_mask = (df['date'] >= test_start) & (df['date'] <= test_end)\n",
        "        df_train = df[train_mask].reset_index(drop=True)\n",
        "        df_test = df[test_mask].reset_index(drop=True)\n",
        "\n",
        "        # Build sequences from the full train portion to scale properly and then build for test using scaler\n",
        "        X_train_seq, y_train_seq, idxs_train = build_sequences(df_train, features, args.seq_len)\n",
        "        # for test sequences we need context that may include rows inside train - so build sequences on concatenated df up to test_end\n",
        "        df_upto_test = df[df['date'] <= test_end].reset_index(drop=True)\n",
        "        X_all_seq, y_all_seq, idxs_all = build_sequences(df_upto_test, features, args.seq_len)\n",
        "\n",
        "        # identify which sequences correspond to test indices (those whose end idx is in df_upto_test and date between test_start/test_end)\n",
        "        test_end_positions = []\n",
        "        # map idxs_all (which are indices in df_upto_test) to dates; need to pick those with date in test range\n",
        "        dates_all = df_upto_test['date'].values\n",
        "        for pos, end_idx in enumerate(idxs_all):\n",
        "            dt = dates_all[end_idx]\n",
        "            if (dt >= np.datetime64(test_start)) and (dt <= np.datetime64(test_end)):\n",
        "                test_end_positions.append(pos)\n",
        "\n",
        "        # now prepare X_test_seq and y_test_seq\n",
        "        if len(test_end_positions) == 0:\n",
        "            print(f\"  no test sequences for fold {idx}, skipping\")\n",
        "            continue\n",
        "        X_test_seq = X_all_seq[test_end_positions]\n",
        "        y_test_seq = y_all_seq[test_end_positions]\n",
        "\n",
        "        # scale features: fit scaler on flattened training sequences (all timesteps)\n",
        "        n_feats = len(features)\n",
        "        if X_train_seq.shape[0] == 0:\n",
        "            print(\"  no train sequences for this fold; skipping\")\n",
        "            continue\n",
        "        # flatten to (num_samples * seq_len, n_feats)\n",
        "        flat_train = X_train_seq.reshape(-1, n_feats)\n",
        "        scaler = StandardScaler().fit(flat_train)\n",
        "        # apply scaler\n",
        "        X_train_seq_scaled = scaler.transform(flat_train).reshape(X_train_seq.shape)\n",
        "        X_test_seq_scaled = scaler.transform(X_test_seq.reshape(-1, n_feats)).reshape(X_test_seq.shape)\n",
        "\n",
        "        # build dataloaders\n",
        "        train_ds = SeqDataset(X_train_seq_scaled, y_train_seq)\n",
        "        test_ds = SeqDataset(X_test_seq_scaled, y_test_seq)\n",
        "        train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True)\n",
        "        test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "        # model\n",
        "        model = LSTMRegressor(input_size=n_feats, hidden_size=args.hidden).to(device)\n",
        "        loss_fn = nn.MSELoss()\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "        # training loop\n",
        "        for ep in range(args.epochs):\n",
        "            tr_loss = train_epoch(model, train_loader, opt, loss_fn, device)\n",
        "            if (ep+1) % 5 == 0 or ep == 0 or ep == args.epochs-1:\n",
        "                print(f\"   Epoch {ep+1}/{args.epochs}, train_loss={tr_loss:.6f}\")\n",
        "\n",
        "        # predict on test\n",
        "        y_true, y_pred = predict(model, test_loader, device)\n",
        "        # convert to plain floats\n",
        "        y_true_list = [float(x) for x in y_true.tolist()]\n",
        "        y_pred_list = [float(x) for x in y_pred.tolist()]\n",
        "\n",
        "        mae_val = float(mean_absolute_error(y_true, y_pred))\n",
        "        rmse_val = rmse(y_true, y_pred)\n",
        "        da_val = directional_accuracy_percent(y_true, y_pred)\n",
        "\n",
        "        # record\n",
        "        out_rec = {\n",
        "            \"fold\": int(idx),\n",
        "            \"model\": \"lstm_numerical\",\n",
        "            \"train_start\": pd.to_datetime(str(train_start)).isoformat(),\n",
        "            \"train_end\": pd.to_datetime(str(train_end)).isoformat(),\n",
        "            \"test_start\": pd.to_datetime(str(test_start)).isoformat(),\n",
        "            \"test_end\": pd.to_datetime(str(test_end)).isoformat(),\n",
        "            \"y_true\": y_true_list,\n",
        "            \"y_pred\": y_pred_list,\n",
        "            \"metrics\": {\n",
        "                \"MAE\": mae_val,\n",
        "                \"RMSE\": rmse_val,\n",
        "                \"DA_percent\": da_val\n",
        "            }\n",
        "        }\n",
        "        out_recs.append(out_rec)\n",
        "\n",
        "    # summary across folds\n",
        "    maes = [r[\"metrics\"][\"MAE\"] for r in out_recs]\n",
        "    rmses = [r[\"metrics\"][\"RMSE\"] for r in out_recs]\n",
        "    das = [r[\"metrics\"][\"DA_percent\"] for r in out_recs]\n",
        "    summary = {\n",
        "        \"MAE\": {\"mean\": t_ci(maes)[0], \"lo\": t_ci(maes)[1], \"hi\": t_ci(maes)[2]},\n",
        "        \"RMSE\": {\"mean\": t_ci(rmses)[0], \"lo\": t_ci(rmses)[1], \"hi\": t_ci(rmses)[2]},\n",
        "        \"DA_percent\": {\"mean\": t_ci(das)[0], \"lo\": t_ci(das)[1], \"hi\": t_ci(das)[2]}\n",
        "    }\n",
        "\n",
        "    os.makedirs(os.path.dirname(args.out), exist_ok=True)\n",
        "    out_blob = {\"per_fold\": out_recs, \"summary\": summary}\n",
        "    with open(args.out, \"w\") as f:\n",
        "        json.dump(out_blob, f, indent=2)\n",
        "    print(\"Saved LSTM preds+metrics to\", args.out)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/train_lstm.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/features.py --input data/raw/aapl.csv --out data/processed/aapl_features.parquet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSTASdMfjdAf",
        "outputId": "238463d8-b640-472c-e56a-3e6249634b30"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved sample slice to data/sample_slice.csv\n",
            "Saved features to data/processed/aapl_features.parquet with 2202 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/train_lstm.py --features data/processed/aapl_features.parquet --out outputs/lstm_preds.json --seq_len 10 --epochs 20 --hidden 64 --batch_size 64 --lr 1e-3 --device cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQJwz3zSjqJ0",
        "outputId": "0e2d0a35-8d83-4daa-989d-b020e57c0f3b"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] train_end=2011-04-04 00:00:00, test_start=2011-04-05 00:00:00, test_end=2011-06-29 00:00:00\n",
            "   Epoch 1/20, train_loss=0.004478\n",
            "   Epoch 5/20, train_loss=0.000617\n",
            "   Epoch 10/20, train_loss=0.000595\n",
            "   Epoch 15/20, train_loss=0.000547\n",
            "   Epoch 20/20, train_loss=0.000501\n",
            "[Fold 1] train_end=2011-06-29 00:00:00, test_start=2011-06-30 00:00:00, test_end=2011-09-23 00:00:00\n",
            "   Epoch 1/20, train_loss=0.003430\n",
            "   Epoch 5/20, train_loss=0.000576\n",
            "   Epoch 10/20, train_loss=0.000520\n",
            "   Epoch 15/20, train_loss=0.000504\n",
            "   Epoch 20/20, train_loss=0.000468\n",
            "[Fold 2] train_end=2011-09-23 00:00:00, test_start=2011-09-26 00:00:00, test_end=2011-12-19 00:00:00\n",
            "   Epoch 1/20, train_loss=0.002116\n",
            "   Epoch 5/20, train_loss=0.000578\n",
            "   Epoch 10/20, train_loss=0.000522\n",
            "   Epoch 15/20, train_loss=0.000490\n",
            "   Epoch 20/20, train_loss=0.000454\n",
            "[Fold 3] train_end=2011-12-19 00:00:00, test_start=2011-12-20 00:00:00, test_end=2012-03-16 00:00:00\n",
            "   Epoch 1/20, train_loss=0.002965\n",
            "   Epoch 5/20, train_loss=0.000563\n",
            "   Epoch 10/20, train_loss=0.000541\n",
            "   Epoch 15/20, train_loss=0.000501\n",
            "   Epoch 20/20, train_loss=0.000472\n",
            "[Fold 4] train_end=2012-03-16 00:00:00, test_start=2012-03-19 00:00:00, test_end=2012-06-12 00:00:00\n",
            "   Epoch 1/20, train_loss=0.002209\n",
            "   Epoch 5/20, train_loss=0.000529\n",
            "   Epoch 10/20, train_loss=0.000478\n",
            "   Epoch 15/20, train_loss=0.000465\n",
            "   Epoch 20/20, train_loss=0.000439\n",
            "[Fold 5] train_end=2012-06-12 00:00:00, test_start=2012-06-13 00:00:00, test_end=2012-09-06 00:00:00\n",
            "   Epoch 1/20, train_loss=0.001684\n",
            "   Epoch 5/20, train_loss=0.000533\n",
            "   Epoch 10/20, train_loss=0.000475\n",
            "   Epoch 15/20, train_loss=0.000490\n",
            "   Epoch 20/20, train_loss=0.000464\n",
            "[Fold 6] train_end=2012-09-06 00:00:00, test_start=2012-09-07 00:00:00, test_end=2012-12-04 00:00:00\n",
            "   Epoch 1/20, train_loss=0.001697\n",
            "   Epoch 5/20, train_loss=0.000503\n",
            "   Epoch 10/20, train_loss=0.000467\n",
            "   Epoch 15/20, train_loss=0.000465\n",
            "   Epoch 20/20, train_loss=0.000407\n",
            "[Fold 7] train_end=2012-12-04 00:00:00, test_start=2012-12-05 00:00:00, test_end=2013-03-04 00:00:00\n",
            "   Epoch 1/20, train_loss=0.001014\n",
            "   Epoch 5/20, train_loss=0.000479\n",
            "   Epoch 10/20, train_loss=0.000462\n",
            "   Epoch 15/20, train_loss=0.000440\n",
            "   Epoch 20/20, train_loss=0.000402\n",
            "[Fold 8] train_end=2013-03-04 00:00:00, test_start=2013-03-05 00:00:00, test_end=2013-05-29 00:00:00\n",
            "   Epoch 1/20, train_loss=0.001491\n",
            "   Epoch 5/20, train_loss=0.000521\n",
            "   Epoch 10/20, train_loss=0.000480\n",
            "   Epoch 15/20, train_loss=0.000447\n",
            "   Epoch 20/20, train_loss=0.000409\n",
            "[Fold 9] train_end=2013-05-29 00:00:00, test_start=2013-05-30 00:00:00, test_end=2013-08-22 00:00:00\n",
            "   Epoch 1/20, train_loss=0.000741\n",
            "   Epoch 5/20, train_loss=0.000503\n",
            "   Epoch 10/20, train_loss=0.000450\n",
            "   Epoch 15/20, train_loss=0.000423\n",
            "   Epoch 20/20, train_loss=0.000407\n",
            "[Fold 10] train_end=2013-08-22 00:00:00, test_start=2013-08-23 00:00:00, test_end=2013-11-15 00:00:00\n",
            "   Epoch 1/20, train_loss=0.002224\n",
            "   Epoch 5/20, train_loss=0.000511\n",
            "   Epoch 10/20, train_loss=0.000465\n",
            "   Epoch 15/20, train_loss=0.000425\n",
            "   Epoch 20/20, train_loss=0.000402\n",
            "[Fold 11] train_end=2013-11-15 00:00:00, test_start=2013-11-18 00:00:00, test_end=2014-02-13 00:00:00\n",
            "   Epoch 1/20, train_loss=0.003406\n",
            "   Epoch 5/20, train_loss=0.000484\n",
            "   Epoch 10/20, train_loss=0.000440\n",
            "   Epoch 15/20, train_loss=0.000438\n",
            "   Epoch 20/20, train_loss=0.000397\n",
            "[Fold 12] train_end=2014-02-13 00:00:00, test_start=2014-02-14 00:00:00, test_end=2014-05-12 00:00:00\n",
            "   Epoch 1/20, train_loss=0.001249\n",
            "   Epoch 5/20, train_loss=0.000469\n",
            "   Epoch 10/20, train_loss=0.000431\n",
            "   Epoch 15/20, train_loss=0.000401\n",
            "   Epoch 20/20, train_loss=0.000389\n",
            "[Fold 13] train_end=2014-05-12 00:00:00, test_start=2014-05-13 00:00:00, test_end=2014-08-06 00:00:00\n",
            "   Epoch 1/20, train_loss=0.001557\n",
            "   Epoch 5/20, train_loss=0.000483\n",
            "   Epoch 10/20, train_loss=0.000428\n",
            "   Epoch 15/20, train_loss=0.000437\n",
            "   Epoch 20/20, train_loss=0.000391\n",
            "[Fold 14] train_end=2014-08-06 00:00:00, test_start=2014-08-07 00:00:00, test_end=2014-10-30 00:00:00\n",
            "   Epoch 1/20, train_loss=0.001149\n",
            "   Epoch 5/20, train_loss=0.000432\n",
            "   Epoch 10/20, train_loss=0.000402\n",
            "   Epoch 15/20, train_loss=0.000380\n",
            "   Epoch 20/20, train_loss=0.000360\n",
            "[Fold 15] train_end=2014-10-30 00:00:00, test_start=2014-10-31 00:00:00, test_end=2015-01-28 00:00:00\n",
            "   Epoch 1/20, train_loss=0.000684\n",
            "   Epoch 5/20, train_loss=0.000431\n",
            "   Epoch 10/20, train_loss=0.000401\n",
            "   Epoch 15/20, train_loss=0.000395\n",
            "   Epoch 20/20, train_loss=0.000368\n",
            "[Fold 16] train_end=2015-01-28 00:00:00, test_start=2015-01-29 00:00:00, test_end=2015-04-24 00:00:00\n",
            "   Epoch 1/20, train_loss=0.000753\n",
            "   Epoch 5/20, train_loss=0.000428\n",
            "   Epoch 10/20, train_loss=0.000390\n",
            "   Epoch 15/20, train_loss=0.000377\n",
            "   Epoch 20/20, train_loss=0.000347\n",
            "[Fold 17] train_end=2015-04-24 00:00:00, test_start=2015-04-27 00:00:00, test_end=2015-07-21 00:00:00\n",
            "   Epoch 1/20, train_loss=0.000708\n",
            "   Epoch 5/20, train_loss=0.000422\n",
            "   Epoch 10/20, train_loss=0.000385\n",
            "   Epoch 15/20, train_loss=0.000380\n",
            "   Epoch 20/20, train_loss=0.000363\n",
            "[Fold 18] train_end=2015-07-21 00:00:00, test_start=2015-07-22 00:00:00, test_end=2015-10-14 00:00:00\n",
            "   Epoch 1/20, train_loss=0.001717\n",
            "   Epoch 5/20, train_loss=0.000419\n",
            "   Epoch 10/20, train_loss=0.000385\n",
            "   Epoch 15/20, train_loss=0.000371\n",
            "   Epoch 20/20, train_loss=0.000351\n",
            "[Fold 19] train_end=2015-10-14 00:00:00, test_start=2015-10-15 00:00:00, test_end=2016-01-11 00:00:00\n",
            "   Epoch 1/20, train_loss=0.003236\n",
            "   Epoch 5/20, train_loss=0.000418\n",
            "   Epoch 10/20, train_loss=0.000390\n",
            "   Epoch 15/20, train_loss=0.000375\n",
            "   Epoch 20/20, train_loss=0.000365\n",
            "[Fold 20] train_end=2016-01-11 00:00:00, test_start=2016-01-12 00:00:00, test_end=2016-04-07 00:00:00\n",
            "   Epoch 1/20, train_loss=0.001508\n",
            "   Epoch 5/20, train_loss=0.000422\n",
            "   Epoch 10/20, train_loss=0.000399\n",
            "   Epoch 15/20, train_loss=0.000368\n",
            "   Epoch 20/20, train_loss=0.000352\n",
            "[Fold 21] train_end=2016-04-07 00:00:00, test_start=2016-04-08 00:00:00, test_end=2016-07-01 00:00:00\n",
            "   Epoch 1/20, train_loss=0.000640\n",
            "   Epoch 5/20, train_loss=0.000410\n",
            "   Epoch 10/20, train_loss=0.000375\n",
            "   Epoch 15/20, train_loss=0.000378\n",
            "   Epoch 20/20, train_loss=0.000360\n",
            "[Fold 22] train_end=2016-07-01 00:00:00, test_start=2016-07-05 00:00:00, test_end=2016-09-27 00:00:00\n",
            "   Epoch 1/20, train_loss=0.001173\n",
            "   Epoch 5/20, train_loss=0.000394\n",
            "   Epoch 10/20, train_loss=0.000378\n",
            "   Epoch 15/20, train_loss=0.000366\n",
            "   Epoch 20/20, train_loss=0.000341\n",
            "Saved LSTM preds+metrics to outputs/lstm_preds.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/eval.py --preds_json outputs/lstm_preds.json --out outputs/lstm_metrics.json\n",
        "!cat outputs/lstm_metrics.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP9EF2YIkKXw",
        "outputId": "bde68130-4139-484d-a46b-261aad1f8a40"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved metrics to outputs/lstm_metrics.json\n",
            "{\n",
            "  \"MAE_mean\": 0.012866363107648635,\n",
            "  \"MAE_lo\": 0.011597102762979476,\n",
            "  \"MAE_hi\": 0.014135623452317794,\n",
            "  \"RMSE_mean\": 0.017185575751011716,\n",
            "  \"RMSE_lo\": 0.015485040282239262,\n",
            "  \"RMSE_hi\": 0.01888611121978417,\n",
            "  \"DA_mean\": 50.14492753623189,\n",
            "  \"DA_lo\": 46.44490355990235,\n",
            "  \"DA_hi\": 53.84495151256143\n",
            "}\n",
            "{\n",
            "  \"MAE_mean\": 0.012866363107648635,\n",
            "  \"MAE_lo\": 0.011597102762979476,\n",
            "  \"MAE_hi\": 0.014135623452317794,\n",
            "  \"RMSE_mean\": 0.017185575751011716,\n",
            "  \"RMSE_lo\": 0.015485040282239262,\n",
            "  \"RMSE_hi\": 0.01888611121978417,\n",
            "  \"DA_mean\": 50.14492753623189,\n",
            "  \"DA_lo\": 46.44490355990235,\n",
            "  \"DA_hi\": 53.84495151256143\n",
            "}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/build_text_embeddings.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "build_text_embeddings.py\n",
        "\n",
        "Robust SBERT embedding builder for a news CSV.\n",
        "\n",
        "Features:\n",
        " - Detects date column (configurable with --date_col). Falls back to common names.\n",
        " - If date column is an integer (epoch), converts to datetime (configurable unit).\n",
        " - Allows specifying text column name (--text_col).\n",
        " - Computes SBERT embeddings for entire dataset in batches.\n",
        " - Saves parquet with columns: date (datetime normalized to date), text_emb (list of floats), optionally other meta.\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "from typing import Optional, List\n",
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# sentence-transformers\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception as e:\n",
        "    print(\"ERROR: sentence_transformers not available. Install with `pip install sentence-transformers`.\", file=sys.stderr)\n",
        "    raise\n",
        "\n",
        "COMMON_DATE_COLS = [\"date\", \"created_utc\", \"timestamp\", \"created\", \"publish_date\", \"published_at\", \"time\"]\n",
        "\n",
        "def detect_date_col(df: pd.DataFrame, prefer: Optional[str] = None) -> Optional[str]:\n",
        "    if prefer and prefer in df.columns:\n",
        "        return prefer\n",
        "    for c in COMMON_DATE_COLS:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    # otherwise try to find a datetime-like column\n",
        "    for c in df.columns:\n",
        "        if pd.api.types.is_datetime64_any_dtype(df[c]):\n",
        "            return c\n",
        "    # try numeric columns that look like epoch (very large ints)\n",
        "    for c in df.columns:\n",
        "        if pd.api.types.is_integer_dtype(df[c]) or pd.api.types.is_float_dtype(df[c]):\n",
        "            series = df[c].dropna()\n",
        "            if len(series) == 0:\n",
        "                continue\n",
        "            v = series.iloc[0]\n",
        "            # heuristic: epoch seconds are > 1e9 (since 2001) and < 1e11\n",
        "            if isinstance(v, (int, float)) and (1e9 < abs(v) < 1e12):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "def parse_dates_column(series: pd.Series, unit: str = \"s\") -> pd.Series:\n",
        "    # If already datetime, return normalized datetimes\n",
        "    if pd.api.types.is_datetime64_any_dtype(series):\n",
        "        return pd.to_datetime(series).dt.normalize()\n",
        "    # If numeric, treat as epoch\n",
        "    if pd.api.types.is_integer_dtype(series) or pd.api.types.is_float_dtype(series):\n",
        "        # use pandas to_datetime with unit\n",
        "        return pd.to_datetime(series, unit=unit, errors=\"coerce\").dt.normalize()\n",
        "    # otherwise try parsing strings\n",
        "    return pd.to_datetime(series, errors=\"coerce\").dt.normalize()\n",
        "\n",
        "def main():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--news_csv\", required=True, help=\"Input CSV of news (must contain text column)\")\n",
        "    p.add_argument(\"--out\", required=True, help=\"Output parquet path (will include columns 'date','text_emb')\")\n",
        "    p.add_argument(\"--text_col\", default=None, help=\"Column name containing the text (title/body). If not set, tries common choices.\")\n",
        "    p.add_argument(\"--date_col\", default=None, help=\"Column name containing date/timestamp. If not set, auto-detects.\")\n",
        "    p.add_argument(\"--epoch_unit\", choices=[\"s\", \"ms\"], default=\"s\", help=\"If date column is numeric epoch, unit (seconds 's' or milliseconds 'ms').\")\n",
        "    p.add_argument(\"--model_name\", default=\"sentence-transformers/all-MiniLM-L6-v2\", help=\"SBERT model name\")\n",
        "    p.add_argument(\"--batch_size\", type=int, default=128)\n",
        "    p.add_argument(\"--device\", default=\"cpu\")\n",
        "    p.add_argument(\"--text_cols_try\", nargs=\"*\", default=[\"title\", \"headline\", \"body\", \"content\", \"text\"], help=\"Text columns to try if --text_col not provided\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    path = Path(args.news_csv)\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"{path} does not exist\")\n",
        "\n",
        "    # Read a small sample first to inspect columns\n",
        "    sample = pd.read_csv(path, nrows=10)\n",
        "    print(\"CSV columns:\", sample.columns.tolist())\n",
        "\n",
        "    # decide text column\n",
        "    text_col = args.text_col\n",
        "    if text_col is None:\n",
        "        for c in args.text_cols_try:\n",
        "            if c in sample.columns:\n",
        "                text_col = c\n",
        "                break\n",
        "    if text_col is None:\n",
        "        # fallback: choose first object dtype column that's not the detected date_col\n",
        "        for c in sample.columns:\n",
        "            if sample[c].dtype == object:\n",
        "                text_col = c\n",
        "                break\n",
        "    if text_col is None:\n",
        "        raise ValueError(\"Could not infer text column. Provide --text_col explicitly.\")\n",
        "    print(\"Using text column:\", text_col)\n",
        "\n",
        "    # read full CSV (we'll not parse dates yet)\n",
        "    df = pd.read_csv(path)\n",
        "    print(\"Read full CSV shape:\", df.shape)\n",
        "\n",
        "    # detect date column\n",
        "    date_col = args.date_col or detect_date_col(df)\n",
        "    if date_col is None:\n",
        "        print(\"No date column detected; resulting parquet will have NaT for 'date'. You can pass --date_col to specify.\")\n",
        "        df[\"date_parsed\"] = pd.NaT\n",
        "    else:\n",
        "        print(\"Using date column:\", date_col)\n",
        "        df[\"date_parsed\"] = parse_dates_column(df[date_col], unit=args.epoch_unit)\n",
        "\n",
        "    # normalize to midnight (date-only)\n",
        "    df[\"date_parsed\"] = pd.to_datetime(df[\"date_parsed\"], errors=\"coerce\").dt.normalize()\n",
        "\n",
        "    # build a simple text series (concatenate title+body if both exist)\n",
        "    # If text_col points to a combined field, use it\n",
        "    text_series = df[text_col].astype(str).fillna(\"\").values\n",
        "    # If there's a 'body' column and text_col is only title, try to combine\n",
        "    if \"body\" in df.columns and text_col != \"body\":\n",
        "        # prefer title + body if available\n",
        "        text_series = (df[text_col].fillna(\"\").astype(str) + \". \" + df[\"body\"].fillna(\"\").astype(str)).values\n",
        "\n",
        "    # initialize SBERT model\n",
        "    print(\"Loading SBERT model:\", args.model_name)\n",
        "    model = SentenceTransformer(args.model_name, device=args.device)\n",
        "\n",
        "    # compute embeddings in batches\n",
        "    n = len(text_series)\n",
        "    batch_size = max(1, args.batch_size)\n",
        "    embeddings: List[np.ndarray] = []\n",
        "    print(f\"Computing embeddings for {n} rows with batch_size={batch_size} on device={args.device}\")\n",
        "    for i in tqdm(range(0, n, batch_size)):\n",
        "        batch_texts = text_series[i : i + batch_size].tolist()\n",
        "        emb = model.encode(batch_texts, show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=False)\n",
        "        embeddings.append(emb)\n",
        "    embeddings = np.vstack(embeddings)  # shape (n, emb_dim)\n",
        "    print(\"Embeddings shape:\", embeddings.shape)\n",
        "\n",
        "    # attach embeddings (as python lists) to dataframe\n",
        "    df_out = pd.DataFrame(\n",
        "        {\n",
        "            \"date\": pd.to_datetime(df[\"date_parsed\"]).dt.normalize(),\n",
        "            \"text_emb\": [e.tolist() for e in embeddings],\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # if you want to keep other metadata (e.g., title, url), add them:\n",
        "    # df_out[\"title\"] = df.get(\"title\", pd.Series([\"\"]*len(df)))\n",
        "    # df_out[\"url\"] = df.get(\"url\", pd.Series([\"\"]*len(df)))\n",
        "\n",
        "    out_path = Path(args.out)\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    df_out.to_parquet(out_path, index=False)\n",
        "    print(\"Wrote embeddings to\", out_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKxcUskSk3mV",
        "outputId": "a5340c77-eb05-4fbd-aaf2-27e3716f576d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/build_text_embeddings.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch tqdm pandas pyarrow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11CkYMT8k5Ux",
        "outputId": "2508cce4-a971-43ab-944b-2f1bc201f12c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (18.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python src/build_text_embeddings.py \\\n",
        "  --news_csv data/raw/RedditNews.csv \\\n",
        "  --out data/processed/text_embeddings.parquet \\\n",
        "  --batch_size 128 \\\n",
        "  --device cpu \\\n",
        "  --text_col News \\\n",
        "  --date_col Date \\\n",
        "  --epoch_unit s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzmxEM9RC8FZ",
        "outputId": "ef85fd8c-48bb-45db-a047-844cfbd32946"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV columns: ['Date', 'News']\n",
            "Using text column: News\n",
            "Read full CSV shape: (49647, 2)\n",
            "Using date column: Date\n",
            "Loading SBERT model: sentence-transformers/all-MiniLM-L6-v2\n",
            "Computing embeddings for 49647 rows with batch_size=128 on device=cpu\n",
            "Embeddings shape: (49647, 384)\n",
            "Wrote embeddings to data/processed/text_embeddings.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-09 04:40:30.894023: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762663230.935230    7228 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762663230.948767    7228 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762663230.987267    7228 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762663230.987324    7228 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762663230.987328    7228 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762663230.987330    7228 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-09 04:40:30.997836: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\r  0%|          | 0/388 [00:00<?, ?it/s]\r  0%|          | 1/388 [00:02<16:27,  2.55s/it]\r  1%|          | 2/388 [00:04<14:10,  2.20s/it]\r  1%|          | 3/388 [00:07<15:55,  2.48s/it]\r  1%|          | 4/388 [00:10<16:29,  2.58s/it]\r  1%|         | 5/388 [00:12<15:15,  2.39s/it]\r  2%|         | 6/388 [00:13<14:07,  2.22s/it]\r  2%|         | 7/388 [00:15<13:32,  2.13s/it]\r  2%|         | 8/388 [00:18<13:22,  2.11s/it]\r  2%|         | 9/388 [00:20<13:30,  2.14s/it]\r  3%|         | 10/388 [00:23<15:53,  2.52s/it]\r  3%|         | 11/388 [00:25<15:28,  2.46s/it]\r  3%|         | 12/388 [00:27<14:19,  2.29s/it]\r  3%|         | 13/388 [00:29<13:36,  2.18s/it]\r  4%|         | 14/388 [00:31<12:59,  2.08s/it]\r  4%|         | 15/388 [00:33<12:31,  2.02s/it]\r  4%|         | 16/388 [00:35<13:17,  2.14s/it]\r  4%|         | 17/388 [00:38<14:38,  2.37s/it]\r  5%|         | 18/388 [00:40<13:55,  2.26s/it]\r  5%|         | 19/388 [00:42<13:06,  2.13s/it]\r  5%|         | 20/388 [00:44<12:44,  2.08s/it]\r  5%|         | 21/388 [00:46<12:17,  2.01s/it]\r  6%|         | 22/388 [00:48<11:49,  1.94s/it]\r  6%|         | 23/388 [00:50<13:16,  2.18s/it]\r  6%|         | 24/388 [00:53<14:09,  2.33s/it]\r  6%|         | 25/388 [00:55<13:26,  2.22s/it]\r  7%|         | 26/388 [00:57<12:49,  2.12s/it]\r  7%|         | 27/388 [00:59<12:17,  2.04s/it]\r  7%|         | 28/388 [01:01<12:00,  2.00s/it]\r  7%|         | 29/388 [01:03<11:36,  1.94s/it]\r  8%|         | 30/388 [01:06<13:35,  2.28s/it]\r  8%|         | 31/388 [01:08<13:56,  2.34s/it]\r  8%|         | 32/388 [01:10<13:07,  2.21s/it]\r  9%|         | 33/388 [01:12<12:11,  2.06s/it]\r  9%|         | 34/388 [01:13<11:18,  1.92s/it]\r  9%|         | 35/388 [01:15<11:30,  1.96s/it]\r  9%|         | 36/388 [01:17<11:03,  1.89s/it]\r 10%|         | 37/388 [01:20<12:16,  2.10s/it]\r 10%|         | 38/388 [01:22<13:12,  2.27s/it]\r 10%|         | 39/388 [01:24<12:26,  2.14s/it]\r 10%|         | 40/388 [01:26<12:21,  2.13s/it]\r 11%|         | 41/388 [01:28<11:43,  2.03s/it]\r 11%|         | 42/388 [01:30<11:16,  1.96s/it]\r 11%|         | 43/388 [01:32<11:03,  1.92s/it]\r 11%|        | 44/388 [01:34<12:00,  2.09s/it]\r 12%|        | 45/388 [01:37<13:03,  2.28s/it]\r 12%|        | 46/388 [01:39<12:40,  2.23s/it]\r 12%|        | 47/388 [01:41<12:18,  2.16s/it]\r 12%|        | 48/388 [01:43<11:47,  2.08s/it]\r 13%|        | 49/388 [01:45<11:18,  2.00s/it]\r 13%|        | 50/388 [01:47<11:57,  2.12s/it]\r 13%|        | 51/388 [01:51<14:06,  2.51s/it]\r 13%|        | 52/388 [01:53<13:30,  2.41s/it]\r 14%|        | 53/388 [01:54<12:19,  2.21s/it]\r 14%|        | 54/388 [01:56<11:53,  2.14s/it]\r 14%|        | 55/388 [01:58<11:13,  2.02s/it]\r 14%|        | 56/388 [02:00<11:02,  1.99s/it]\r 15%|        | 57/388 [02:03<12:14,  2.22s/it]\r 15%|        | 58/388 [02:06<13:16,  2.41s/it]\r 15%|        | 59/388 [02:08<12:09,  2.22s/it]\r 15%|        | 60/388 [02:10<11:54,  2.18s/it]\r 16%|        | 61/388 [02:12<11:48,  2.17s/it]\r 16%|        | 62/388 [02:14<11:23,  2.10s/it]\r 16%|        | 63/388 [02:16<11:26,  2.11s/it]\r 16%|        | 64/388 [02:19<13:14,  2.45s/it]\r 17%|        | 65/388 [02:21<13:02,  2.42s/it]\r 17%|        | 66/388 [02:23<12:18,  2.29s/it]\r 17%|        | 67/388 [02:26<12:03,  2.25s/it]\r 18%|        | 68/388 [02:28<11:40,  2.19s/it]\r 18%|        | 69/388 [02:30<11:11,  2.10s/it]\r 18%|        | 70/388 [02:33<12:34,  2.37s/it]\r 18%|        | 71/388 [02:35<12:49,  2.43s/it]\r 19%|        | 72/388 [02:37<12:08,  2.31s/it]\r 19%|        | 73/388 [02:39<11:16,  2.15s/it]\r 19%|        | 74/388 [02:41<10:59,  2.10s/it]\r 19%|        | 75/388 [02:43<10:54,  2.09s/it]\r 20%|        | 76/388 [02:46<12:02,  2.32s/it]\r 20%|        | 77/388 [02:49<13:08,  2.54s/it]\r 20%|        | 78/388 [02:51<12:07,  2.35s/it]\r 20%|        | 79/388 [02:53<11:24,  2.22s/it]\r 21%|        | 80/388 [02:55<11:02,  2.15s/it]\r 21%|        | 81/388 [02:57<11:01,  2.15s/it]\r 21%|        | 82/388 [02:59<10:30,  2.06s/it]\r 21%|       | 83/388 [03:02<12:06,  2.38s/it]\r 22%|       | 84/388 [03:04<11:58,  2.36s/it]\r 22%|       | 85/388 [03:06<11:09,  2.21s/it]\r 22%|       | 86/388 [03:08<10:51,  2.16s/it]\r 22%|       | 87/388 [03:10<10:16,  2.05s/it]\r 23%|       | 88/388 [03:12<10:05,  2.02s/it]\r 23%|       | 89/388 [03:14<10:16,  2.06s/it]\r 23%|       | 90/388 [03:17<12:32,  2.53s/it]\r 23%|       | 91/388 [03:19<11:14,  2.27s/it]\r 24%|       | 92/388 [03:21<10:55,  2.21s/it]\r 24%|       | 93/388 [03:23<10:23,  2.11s/it]\r 24%|       | 94/388 [03:25<10:02,  2.05s/it]\r 24%|       | 95/388 [03:27<09:54,  2.03s/it]\r 25%|       | 96/388 [03:30<10:55,  2.24s/it]\r 25%|       | 97/388 [03:32<11:35,  2.39s/it]\r 25%|       | 98/388 [03:34<10:53,  2.25s/it]\r 26%|       | 99/388 [03:36<10:02,  2.09s/it]\r 26%|       | 100/388 [03:38<09:43,  2.02s/it]\r 26%|       | 101/388 [03:40<09:31,  1.99s/it]\r 26%|       | 102/388 [03:42<09:04,  1.91s/it]\r 27%|       | 103/388 [03:45<10:39,  2.24s/it]\r 27%|       | 104/388 [03:47<10:54,  2.30s/it]\r 27%|       | 105/388 [03:49<10:18,  2.19s/it]\r 27%|       | 106/388 [03:51<09:52,  2.10s/it]\r 28%|       | 107/388 [03:53<09:27,  2.02s/it]\r 28%|       | 108/388 [03:55<09:09,  1.96s/it]\r 28%|       | 109/388 [03:57<09:16,  1.99s/it]\r 28%|       | 110/388 [04:00<10:47,  2.33s/it]\r 29%|       | 111/388 [04:02<10:36,  2.30s/it]\r 29%|       | 112/388 [04:04<10:04,  2.19s/it]\r 29%|       | 113/388 [04:06<09:40,  2.11s/it]\r 29%|       | 114/388 [04:08<09:13,  2.02s/it]\r 30%|       | 115/388 [04:09<08:52,  1.95s/it]\r 30%|       | 116/388 [04:12<09:02,  1.99s/it]\r 30%|       | 117/388 [04:15<10:25,  2.31s/it]\r 30%|       | 118/388 [04:17<10:12,  2.27s/it]\r 31%|       | 119/388 [04:18<09:24,  2.10s/it]\r 31%|       | 120/388 [04:20<09:02,  2.02s/it]\r 31%|       | 121/388 [04:22<08:58,  2.02s/it]\r 31%|      | 122/388 [04:24<08:51,  2.00s/it]\r 32%|      | 123/388 [04:27<09:41,  2.20s/it]\r 32%|      | 124/388 [04:30<10:41,  2.43s/it]\r 32%|      | 125/388 [04:32<10:00,  2.28s/it]\r 32%|      | 126/388 [04:34<09:17,  2.13s/it]\r 33%|      | 127/388 [04:35<08:44,  2.01s/it]\r 33%|      | 128/388 [04:37<08:20,  1.93s/it]\r 33%|      | 129/388 [04:39<08:14,  1.91s/it]\r 34%|      | 130/388 [04:42<09:07,  2.12s/it]\r 34%|      | 131/388 [04:44<09:40,  2.26s/it]\r 34%|      | 132/388 [04:46<09:17,  2.18s/it]\r 34%|      | 133/388 [04:48<08:59,  2.11s/it]\r 35%|      | 134/388 [04:50<08:34,  2.03s/it]\r 35%|      | 135/388 [04:52<08:14,  1.95s/it]\r 35%|      | 136/388 [04:54<08:06,  1.93s/it]\r 35%|      | 137/388 [04:57<09:27,  2.26s/it]\r 36%|      | 138/388 [04:59<09:31,  2.29s/it]\r 36%|      | 139/388 [05:01<08:47,  2.12s/it]\r 36%|      | 140/388 [05:02<08:18,  2.01s/it]\r 36%|      | 141/388 [05:04<08:01,  1.95s/it]\r 37%|      | 142/388 [05:06<07:48,  1.90s/it]\r 37%|      | 143/388 [05:08<07:31,  1.84s/it]\r 37%|      | 144/388 [05:11<08:53,  2.19s/it]\r 37%|      | 145/388 [05:13<09:21,  2.31s/it]\r 38%|      | 146/388 [05:15<08:47,  2.18s/it]\r 38%|      | 147/388 [05:17<08:19,  2.07s/it]\r 38%|      | 148/388 [05:19<08:04,  2.02s/it]\r 38%|      | 149/388 [05:21<07:42,  1.94s/it]\r 39%|      | 150/388 [05:23<07:51,  1.98s/it]\r 39%|      | 151/388 [05:26<09:01,  2.28s/it]\r 39%|      | 152/388 [05:28<09:04,  2.31s/it]\r 39%|      | 153/388 [05:30<08:28,  2.16s/it]\r 40%|      | 154/388 [05:32<08:10,  2.10s/it]\r 40%|      | 155/388 [05:34<07:38,  1.97s/it]\r 40%|      | 156/388 [05:35<07:21,  1.90s/it]\r 40%|      | 157/388 [05:38<07:44,  2.01s/it]\r 41%|      | 158/388 [05:41<09:01,  2.35s/it]\r 41%|      | 159/388 [05:43<08:44,  2.29s/it]\r 41%|      | 160/388 [05:45<08:12,  2.16s/it]\r 41%|     | 161/388 [05:47<08:04,  2.13s/it]\r 42%|     | 162/388 [05:49<07:47,  2.07s/it]\r 42%|     | 163/388 [05:51<07:58,  2.13s/it]\r 42%|     | 164/388 [05:54<09:08,  2.45s/it]\r 43%|     | 165/388 [05:57<09:01,  2.43s/it]\r 43%|     | 166/388 [05:58<08:23,  2.27s/it]\r 43%|     | 167/388 [06:00<07:59,  2.17s/it]\r 43%|     | 168/388 [06:02<07:35,  2.07s/it]\r 44%|     | 169/388 [06:04<07:41,  2.11s/it]\r 44%|     | 170/388 [06:07<08:00,  2.21s/it]\r 44%|     | 171/388 [06:10<09:07,  2.52s/it]\r 44%|     | 172/388 [06:12<08:32,  2.37s/it]\r 45%|     | 173/388 [06:14<08:07,  2.27s/it]\r 45%|     | 174/388 [06:16<07:42,  2.16s/it]\r 45%|     | 175/388 [06:18<07:28,  2.11s/it]\r 45%|     | 176/388 [06:20<07:09,  2.02s/it]\r 46%|     | 177/388 [06:23<08:31,  2.43s/it]\r 46%|     | 178/388 [06:25<08:15,  2.36s/it]\r 46%|     | 179/388 [06:27<07:34,  2.18s/it]\r 46%|     | 180/388 [06:29<07:31,  2.17s/it]\r 47%|     | 181/388 [06:31<07:12,  2.09s/it]\r 47%|     | 182/388 [06:33<06:59,  2.04s/it]\r 47%|     | 183/388 [06:35<07:04,  2.07s/it]\r 47%|     | 184/388 [06:39<08:24,  2.47s/it]\r 48%|     | 185/388 [06:41<08:07,  2.40s/it]\r 48%|     | 186/388 [06:43<07:40,  2.28s/it]\r 48%|     | 187/388 [06:45<07:37,  2.27s/it]\r 48%|     | 188/388 [06:47<07:18,  2.19s/it]\r 49%|     | 189/388 [06:50<07:30,  2.26s/it]\r 49%|     | 190/388 [06:53<08:41,  2.64s/it]\r 49%|     | 191/388 [06:55<08:05,  2.46s/it]\r 49%|     | 192/388 [06:57<07:30,  2.30s/it]\r 50%|     | 193/388 [06:59<07:26,  2.29s/it]\r 50%|     | 194/388 [07:01<07:12,  2.23s/it]\r 50%|     | 195/388 [07:04<07:20,  2.28s/it]\r 51%|     | 196/388 [07:07<08:21,  2.61s/it]\r 51%|     | 197/388 [07:10<08:02,  2.52s/it]\r 51%|     | 198/388 [07:12<07:30,  2.37s/it]\r 51%|    | 199/388 [07:14<07:05,  2.25s/it]\r 52%|    | 200/388 [07:15<06:41,  2.14s/it]\r 52%|    | 201/388 [07:17<06:32,  2.10s/it]\r 52%|    | 202/388 [07:21<07:32,  2.44s/it]\r 52%|    | 203/388 [07:23<07:35,  2.46s/it]\r 53%|    | 204/388 [07:26<07:29,  2.44s/it]\r 53%|    | 205/388 [07:28<07:07,  2.34s/it]\r 53%|    | 206/388 [07:30<06:49,  2.25s/it]\r 53%|    | 207/388 [07:32<06:56,  2.30s/it]\r 54%|    | 208/388 [07:36<08:01,  2.68s/it]\r 54%|    | 209/388 [07:38<07:35,  2.54s/it]\r 54%|    | 210/388 [07:40<07:06,  2.40s/it]\r 54%|    | 211/388 [07:42<06:48,  2.31s/it]\r 55%|    | 212/388 [07:44<06:33,  2.24s/it]\r 55%|    | 213/388 [07:46<06:13,  2.13s/it]\r 55%|    | 214/388 [07:49<06:57,  2.40s/it]\r 55%|    | 215/388 [07:52<07:11,  2.50s/it]\r 56%|    | 216/388 [07:54<06:47,  2.37s/it]\r 56%|    | 217/388 [07:56<06:26,  2.26s/it]\r 56%|    | 218/388 [07:58<06:10,  2.18s/it]\r 56%|    | 219/388 [08:00<05:58,  2.12s/it]\r 57%|    | 220/388 [08:02<06:14,  2.23s/it]\r 57%|    | 221/388 [08:05<06:58,  2.51s/it]\r 57%|    | 222/388 [08:07<06:21,  2.30s/it]\r 57%|    | 223/388 [08:09<05:48,  2.11s/it]\r 58%|    | 224/388 [08:11<05:40,  2.07s/it]\r 58%|    | 225/388 [08:13<05:29,  2.02s/it]\r 58%|    | 226/388 [08:15<05:18,  1.97s/it]\r 59%|    | 227/388 [08:17<05:56,  2.22s/it]\r 59%|    | 228/388 [08:20<06:16,  2.35s/it]\r 59%|    | 229/388 [08:22<05:49,  2.20s/it]\r 59%|    | 230/388 [08:24<05:39,  2.15s/it]\r 60%|    | 231/388 [08:26<05:23,  2.06s/it]\r 60%|    | 232/388 [08:28<05:08,  1.98s/it]\r 60%|    | 233/388 [08:30<05:09,  2.00s/it]\r 60%|    | 234/388 [08:33<06:00,  2.34s/it]\r 61%|    | 235/388 [08:35<05:51,  2.29s/it]\r 61%|    | 236/388 [08:37<05:40,  2.24s/it]\r 61%|    | 237/388 [08:39<05:24,  2.15s/it]\r 61%|   | 238/388 [08:41<05:11,  2.07s/it]\r 62%|   | 239/388 [08:43<05:05,  2.05s/it]\r 62%|   | 240/388 [08:45<05:22,  2.18s/it]\r 62%|   | 241/388 [08:48<05:51,  2.39s/it]\r 62%|   | 242/388 [08:50<05:26,  2.24s/it]\r 63%|   | 243/388 [08:52<05:12,  2.15s/it]\r 63%|   | 244/388 [08:54<05:04,  2.11s/it]\r 63%|   | 245/388 [08:56<04:58,  2.09s/it]\r 63%|   | 246/388 [08:58<05:01,  2.12s/it]\r 64%|   | 247/388 [09:02<05:41,  2.42s/it]\r 64%|   | 248/388 [09:04<05:33,  2.38s/it]\r 64%|   | 249/388 [09:06<05:10,  2.23s/it]\r 64%|   | 250/388 [09:08<04:59,  2.17s/it]\r 65%|   | 251/388 [09:10<04:57,  2.18s/it]\r 65%|   | 252/388 [09:12<04:51,  2.14s/it]\r 65%|   | 253/388 [09:15<05:23,  2.40s/it]\r 65%|   | 254/388 [09:17<05:26,  2.44s/it]\r 66%|   | 255/388 [09:19<05:03,  2.28s/it]\r 66%|   | 256/388 [09:21<04:48,  2.19s/it]\r 66%|   | 257/388 [09:23<04:42,  2.16s/it]\r 66%|   | 258/388 [09:26<04:37,  2.14s/it]\r 67%|   | 259/388 [09:28<04:52,  2.26s/it]\r 67%|   | 260/388 [09:31<05:26,  2.55s/it]\r 67%|   | 261/388 [09:33<05:05,  2.41s/it]\r 68%|   | 262/388 [09:36<04:53,  2.33s/it]\r 68%|   | 263/388 [09:38<04:40,  2.25s/it]\r 68%|   | 264/388 [09:40<04:27,  2.15s/it]\r 68%|   | 265/388 [09:42<04:45,  2.32s/it]\r 69%|   | 266/388 [09:46<05:17,  2.60s/it]\r 69%|   | 267/388 [09:47<04:50,  2.40s/it]\r 69%|   | 268/388 [09:50<04:39,  2.33s/it]\r 69%|   | 269/388 [09:52<04:29,  2.27s/it]\r 70%|   | 270/388 [09:54<04:12,  2.14s/it]\r 70%|   | 271/388 [09:55<03:54,  2.00s/it]\r 70%|   | 272/388 [09:59<04:38,  2.40s/it]\r 70%|   | 273/388 [10:01<04:32,  2.37s/it]\r 71%|   | 274/388 [10:03<04:18,  2.26s/it]\r 71%|   | 275/388 [10:05<04:10,  2.22s/it]\r 71%|   | 276/388 [10:07<04:05,  2.19s/it]\r 71%|  | 277/388 [10:09<03:54,  2.11s/it]\r 72%|  | 278/388 [10:12<04:21,  2.38s/it]\r 72%|  | 279/388 [10:15<04:31,  2.49s/it]\r 72%|  | 280/388 [10:17<04:15,  2.37s/it]\r 72%|  | 281/388 [10:19<04:04,  2.29s/it]\r 73%|  | 282/388 [10:21<03:45,  2.13s/it]\r 73%|  | 283/388 [10:23<03:42,  2.12s/it]\r 73%|  | 284/388 [10:25<03:43,  2.15s/it]\r 73%|  | 285/388 [10:28<04:14,  2.47s/it]\r 74%|  | 286/388 [10:30<03:56,  2.31s/it]\r 74%|  | 287/388 [10:32<03:45,  2.23s/it]\r 74%|  | 288/388 [10:35<03:41,  2.22s/it]\r 74%|  | 289/388 [10:36<03:28,  2.11s/it]\r 75%|  | 290/388 [10:38<03:19,  2.04s/it]\r 75%|  | 291/388 [10:41<03:53,  2.41s/it]\r 75%|  | 292/388 [10:44<03:50,  2.40s/it]\r 76%|  | 293/388 [10:46<03:32,  2.24s/it]\r 76%|  | 294/388 [10:48<03:22,  2.16s/it]\r 76%|  | 295/388 [10:50<03:21,  2.17s/it]\r 76%|  | 296/388 [10:52<03:16,  2.14s/it]\r 77%|  | 297/388 [10:55<03:30,  2.31s/it]\r 77%|  | 298/388 [10:58<03:45,  2.51s/it]\r 77%|  | 299/388 [11:00<03:29,  2.35s/it]\r 77%|  | 300/388 [11:02<03:17,  2.24s/it]\r 78%|  | 301/388 [11:04<03:12,  2.21s/it]\r 78%|  | 302/388 [11:06<03:03,  2.14s/it]\r 78%|  | 303/388 [11:08<03:03,  2.16s/it]\r 78%|  | 304/388 [11:12<03:40,  2.62s/it]\r 79%|  | 305/388 [11:14<03:20,  2.41s/it]\r 79%|  | 306/388 [11:16<03:10,  2.32s/it]\r 79%|  | 307/388 [11:18<03:02,  2.25s/it]\r 79%|  | 308/388 [11:20<02:52,  2.16s/it]\r 80%|  | 309/388 [11:22<02:45,  2.09s/it]\r 80%|  | 310/388 [11:25<03:21,  2.58s/it]\r 80%|  | 311/388 [11:27<03:06,  2.42s/it]\r 80%|  | 312/388 [11:29<02:52,  2.27s/it]\r 81%|  | 313/388 [11:31<02:42,  2.17s/it]\r 81%|  | 314/388 [11:33<02:37,  2.13s/it]\r 81%|  | 315/388 [11:35<02:28,  2.03s/it]\r 81%| | 316/388 [11:38<02:35,  2.16s/it]\r 82%| | 317/388 [11:41<02:56,  2.48s/it]\r 82%| | 318/388 [11:43<02:45,  2.37s/it]\r 82%| | 319/388 [11:45<02:36,  2.27s/it]\r 82%| | 320/388 [11:47<02:28,  2.18s/it]\r 83%| | 321/388 [11:49<02:28,  2.21s/it]\r 83%| | 322/388 [11:51<02:27,  2.24s/it]\r 83%| | 323/388 [11:55<02:50,  2.62s/it]\r 84%| | 324/388 [11:57<02:38,  2.47s/it]\r 84%| | 325/388 [11:59<02:26,  2.33s/it]\r 84%| | 326/388 [12:01<02:18,  2.24s/it]\r 84%| | 327/388 [12:03<02:11,  2.16s/it]\r 85%| | 328/388 [12:05<02:07,  2.13s/it]\r 85%| | 329/388 [12:09<02:30,  2.55s/it]\r 85%| | 330/388 [12:11<02:23,  2.48s/it]\r 85%| | 331/388 [12:13<02:18,  2.43s/it]\r 86%| | 332/388 [12:16<02:14,  2.41s/it]\r 86%| | 333/388 [12:18<02:04,  2.26s/it]\r 86%| | 334/388 [12:20<02:05,  2.32s/it]\r 86%| | 335/388 [12:23<02:20,  2.65s/it]\r 87%| | 336/388 [12:26<02:11,  2.54s/it]\r 87%| | 337/388 [12:28<01:59,  2.34s/it]\r 87%| | 338/388 [12:30<01:52,  2.24s/it]\r 87%| | 339/388 [12:32<01:46,  2.18s/it]\r 88%| | 340/388 [12:34<01:43,  2.16s/it]\r 88%| | 341/388 [12:37<01:56,  2.47s/it]\r 88%| | 342/388 [12:40<01:55,  2.51s/it]\r 88%| | 343/388 [12:41<01:42,  2.28s/it]\r 89%| | 344/388 [12:43<01:35,  2.17s/it]\r 89%| | 345/388 [12:45<01:30,  2.10s/it]\r 89%| | 346/388 [12:47<01:26,  2.05s/it]\r 89%| | 347/388 [12:50<01:29,  2.18s/it]\r 90%| | 348/388 [12:52<01:35,  2.38s/it]\r 90%| | 349/388 [12:55<01:31,  2.35s/it]\r 90%| | 350/388 [12:57<01:24,  2.23s/it]\r 90%| | 351/388 [12:59<01:19,  2.14s/it]\r 91%| | 352/388 [13:01<01:14,  2.07s/it]\r 91%| | 353/388 [13:03<01:13,  2.11s/it]\r 91%| | 354/388 [13:06<01:21,  2.39s/it]\r 91%|| 355/388 [13:08<01:19,  2.42s/it]\r 92%|| 356/388 [13:10<01:12,  2.27s/it]\r 92%|| 357/388 [13:12<01:06,  2.14s/it]\r 92%|| 358/388 [13:14<01:02,  2.07s/it]\r 93%|| 359/388 [13:16<00:59,  2.04s/it]\r 93%|| 360/388 [13:19<01:03,  2.27s/it]\r 93%|| 361/388 [13:21<01:05,  2.41s/it]\r 93%|| 362/388 [13:23<00:58,  2.25s/it]\r 94%|| 363/388 [13:25<00:54,  2.19s/it]\r 94%|| 364/388 [13:27<00:51,  2.14s/it]\r 94%|| 365/388 [13:29<00:47,  2.05s/it]\r 94%|| 366/388 [13:32<00:46,  2.13s/it]\r 95%|| 367/388 [13:35<00:52,  2.48s/it]\r 95%|| 368/388 [13:37<00:47,  2.39s/it]\r 95%|| 369/388 [13:39<00:42,  2.23s/it]\r 95%|| 370/388 [13:41<00:39,  2.18s/it]\r 96%|| 371/388 [13:43<00:34,  2.04s/it]\r 96%|| 372/388 [13:44<00:31,  1.97s/it]\r 96%|| 373/388 [13:47<00:31,  2.09s/it]\r 96%|| 374/388 [13:50<00:33,  2.37s/it]\r 97%|| 375/388 [13:52<00:29,  2.30s/it]\r 97%|| 376/388 [13:54<00:26,  2.18s/it]\r 97%|| 377/388 [13:56<00:23,  2.09s/it]\r 97%|| 378/388 [13:58<00:21,  2.10s/it]\r 98%|| 379/388 [14:00<00:18,  2.07s/it]\r 98%|| 380/388 [14:03<00:18,  2.36s/it]\r 98%|| 381/388 [14:05<00:16,  2.34s/it]\r 98%|| 382/388 [14:07<00:13,  2.29s/it]\r 99%|| 383/388 [14:09<00:10,  2.16s/it]\r 99%|| 384/388 [14:11<00:08,  2.06s/it]\r 99%|| 385/388 [14:13<00:06,  2.06s/it]\r 99%|| 386/388 [14:16<00:04,  2.43s/it]\r100%|| 387/388 [14:19<00:02,  2.57s/it]\r100%|| 388/388 [14:21<00:00,  2.25s/it]\r100%|| 388/388 [14:21<00:00,  2.22s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python - <<'PY'\n",
        "import pandas as pd, numpy as np, json, os\n",
        "pd.options.display.width = 160\n",
        "\n",
        "emb_path = \"data/processed/text_embeddings.parquet\"\n",
        "feat_path = \"data/processed/aapl_features.parquet\"\n",
        "\n",
        "def show(msg): print(\"\\n=== \" + msg + \" ===\")\n",
        "\n",
        "# 1) File existence\n",
        "show(\"Files present\")\n",
        "print(\"embeddings exists:\", os.path.exists(emb_path))\n",
        "print(\"features exists:\", os.path.exists(feat_path))\n",
        "\n",
        "if not os.path.exists(emb_path):\n",
        "    raise SystemExit(\"text_embeddings.parquet not found. Run build_text_embeddings.py first.\")\n",
        "if not os.path.exists(feat_path):\n",
        "    raise SystemExit(\"aapl_features.parquet not found. Run features.py first.\")\n",
        "\n",
        "# 2) Load small samples\n",
        "show(\"Load embedding file (head)\")\n",
        "df_emb = pd.read_parquet(emb_path)\n",
        "print(\"rows (dates) in embeddings:\", len(df_emb))\n",
        "print(\"columns:\", list(df_emb.columns))\n",
        "print(df_emb.head(5).to_dict('records')[:5])\n",
        "\n",
        "# 3) Embedding shape & dtype checks\n",
        "show(\"Embedding shape & dtype checks\")\n",
        "# ensure text_emb column exists\n",
        "if 'text_emb' not in df_emb.columns:\n",
        "    print(\"ERROR: no 'text_emb' column found.\")\n",
        "else:\n",
        "    # check element types and length distribution\n",
        "    lens = df_emb['text_emb'].apply(lambda x: len(x) if isinstance(x,(list,tuple,np.ndarray)) else -1)\n",
        "    print(\"embedding vector lengths (unique counts):\")\n",
        "    print(lens.value_counts().to_dict())\n",
        "    # sample one vector numeric stats\n",
        "    sample_idx = lens.idxmax()\n",
        "    vec = df_emb.loc[sample_idx,'text_emb']\n",
        "    if isinstance(vec,(list,tuple,np.ndarray)):\n",
        "        arr = np.array(vec, dtype=float)\n",
        "        print(\"sample vector dtype:\", arr.dtype, \"shape:\", arr.shape)\n",
        "        print(\"sample vector stats: mean {:.6f}, std {:.6f}, min {:.6f}, max {:.6f}\".format(arr.mean(), arr.std(), arr.min(), arr.max()))\n",
        "    else:\n",
        "        print(\"sample vector is not list-like:\", type(vec))\n",
        "\n",
        "# 4) NaN / null checks\n",
        "show(\"Null / NaN checks\")\n",
        "null_text_emb = df_emb['text_emb'].isna().sum()\n",
        "print(\"text_emb null count:\", null_text_emb)\n",
        "# if text_emb present but some entries not lists\n",
        "bad_entries = df_emb['text_emb'].apply(lambda x: not isinstance(x,(list,tuple,np.ndarray)))\n",
        "print(\"non-list text_emb entries:\", int(bad_entries.sum()))\n",
        "\n",
        "# 5) Date coverage vs features\n",
        "show(\"Date coverage alignment with aapl_features\")\n",
        "df_feat = pd.read_parquet(feat_path)\n",
        "df_feat['date'] = pd.to_datetime(df_feat['date']).dt.normalize()\n",
        "df_emb['date'] = pd.to_datetime(df_emb['date']).dt.normalize()\n",
        "dates_feat = set(df_feat['date'].unique())\n",
        "dates_emb = set(df_emb['date'].unique())\n",
        "common = sorted(list(dates_feat & dates_emb))\n",
        "print(\"unique stock dates:\", len(dates_feat))\n",
        "print(\"unique embedding dates:\", len(dates_emb))\n",
        "print(\"overlap dates:\", len(common), \"({:.1%})\".format(len(common)/len(dates_feat)))\n",
        "print(\"first 5 matched dates:\", common[:5])\n",
        "print(\"first 5 stock-only dates (example):\", list(sorted(dates_feat - dates_emb))[:5])\n",
        "print(\"first 5 emb-only dates (example):\", list(sorted(dates_emb - dates_feat))[:5])\n",
        "\n",
        "# 6) Quick sanity: check if embeddings were averaged (i.e., not identical across days)\n",
        "show(\"Sanity: variety across embeddings\")\n",
        "if len(df_emb) >= 2:\n",
        "    # compute pairwise distance sample\n",
        "    n = min(200, len(df_emb))\n",
        "    sample = df_emb['text_emb'].apply(lambda x: np.array(x,dtype=float)).values[:n]\n",
        "    means = [v.mean() for v in sample]\n",
        "    print(\"sample mean of vectors (first 10):\", [float(m) for m in means[:10]])\n",
        "    # compute std of means\n",
        "    print(\"std of vector means across sample:\", float(np.std(means)))\n",
        "else:\n",
        "    print(\"Not enough embedding dates to check variety.\")\n",
        "\n",
        "# 7) If everything looks OK, print one example of embeddings merged with features for a date\n",
        "show(\"Sample merged row (if overlap exists)\")\n",
        "if len(common) > 0:\n",
        "    d = common[0]\n",
        "    emb_row = df_emb[df_emb['date']==d].iloc[0]\n",
        "    feat_row = df_feat[df_feat['date']==d].iloc[0]\n",
        "    print(\"date:\", d)\n",
        "    print(\"feature columns sample:\", {c: float(feat_row[c]) for c in ['ret'] if 'ret' in feat_row})\n",
        "    print(\"embedding sample (first 8 dims):\", np.array(emb_row['text_emb'],dtype=float)[:8].tolist())\n",
        "else:\n",
        "    print(\"No overlapping dates to display merged example.\")\n",
        "\n",
        "print(\"\\\\nDone.\")\n",
        "PY\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmrFHt5cy4p2",
        "outputId": "ec28c070-1e86-4a5a-ce23-29a6cfb02087"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Files present ===\n",
            "embeddings exists: True\n",
            "features exists: True\n",
            "\n",
            "=== Load embedding file (head) ===\n",
            "rows (dates) in embeddings: 49647\n",
            "columns: ['date', 'text_emb']\n",
            "[{'date': Timestamp('2016-07-01 00:00:00'), 'text_emb': array([-3.27281952e-02,  6.92857727e-02, -7.82452598e-02,  8.92044380e-02,\n",
            "        6.16298765e-02,  1.90350413e-02, -8.91246349e-02,  6.94398955e-03,\n",
            "       -3.76664028e-02,  2.63277236e-02,  6.53022453e-02, -3.22593227e-02,\n",
            "        3.60599421e-02, -1.94225721e-02, -5.39694056e-02,  3.59263346e-02,\n",
            "       -1.24027148e-01, -1.39124002e-02, -1.76910777e-03,  2.78770085e-02,\n",
            "       -3.53500955e-02,  1.74325816e-02,  2.61090859e-03,  3.05023380e-02,\n",
            "        1.50437346e-02, -3.11678220e-02, -3.13687325e-02, -4.34636995e-02,\n",
            "        3.49768177e-02, -3.33533529e-03,  1.76618006e-02,  5.56184128e-02,\n",
            "       -6.74656481e-02, -1.82258561e-02, -1.83785427e-02,  8.04618653e-03,\n",
            "        7.20669329e-02,  6.66547045e-02,  1.88012384e-02, -4.78569837e-03,\n",
            "       -1.39132980e-02, -9.37933922e-02, -3.49856280e-02,  9.58536193e-02,\n",
            "        4.40516621e-02, -6.26409426e-02, -3.21618989e-02,  5.25336601e-02,\n",
            "        1.65332764e-04, -6.53562695e-03, -1.71005763e-02, -6.12066351e-02,\n",
            "        3.58315813e-03, -2.37587448e-02,  4.84590884e-03, -5.77914715e-02,\n",
            "       -2.89729666e-02,  5.60435141e-03, -2.53099315e-02, -5.36518451e-03,\n",
            "        1.81795601e-02,  8.17672610e-02, -1.36881741e-02, -3.06878723e-02,\n",
            "       -5.64581901e-02,  1.22430073e-02,  1.15449116e-01, -7.69965127e-02,\n",
            "        9.13751405e-03, -1.70822386e-02,  1.02887422e-01,  9.18923318e-03,\n",
            "        3.32713500e-03,  1.11917883e-01, -4.90902960e-02,  8.87434091e-03,\n",
            "        6.89878240e-02,  1.85430273e-02, -9.64900702e-02,  1.46401171e-02,\n",
            "       -6.56879786e-03, -6.30499274e-02,  1.86152607e-02,  2.40845792e-02,\n",
            "        4.62419577e-02,  4.72372118e-03, -3.24771553e-02,  2.42029484e-02,\n",
            "       -7.18413740e-02, -5.14152497e-02,  8.46844818e-03,  1.75100658e-02,\n",
            "        1.54583817e-02,  6.75433427e-02,  1.01157054e-01,  1.81111377e-02,\n",
            "        4.24352549e-02,  3.94804925e-02,  3.00030503e-02,  6.55465573e-03,\n",
            "       -2.96271518e-02,  5.11279376e-03,  4.00839485e-02,  7.25867897e-02,\n",
            "        5.71288876e-02,  8.28923360e-02, -8.52679685e-02,  6.30488573e-03,\n",
            "       -3.86301279e-02,  2.70912535e-02,  1.98008083e-02, -2.86139622e-02,\n",
            "       -4.12840731e-02,  3.98536026e-02,  2.60213781e-02,  8.87378529e-02,\n",
            "       -2.63951961e-02,  4.62167338e-02,  1.26010394e-02, -8.90870318e-02,\n",
            "       -2.77698785e-02,  3.25536132e-02, -1.18004784e-01,  1.68788116e-02,\n",
            "        8.79822671e-03, -3.60197239e-02,  8.07016194e-02,  1.54027449e-33,\n",
            "       -5.64047135e-02, -5.38013838e-02, -4.72047217e-02,  6.37518167e-02,\n",
            "        3.32211368e-02,  6.67956751e-03,  2.74927020e-02, -3.49865854e-02,\n",
            "        7.79714296e-03,  2.35485211e-02, -1.53581379e-02, -7.36749843e-02,\n",
            "       -2.11815536e-02, -1.17475651e-01, -3.12264077e-02,  1.24128148e-01,\n",
            "       -9.26285759e-02, -1.49181010e-02, -2.47159358e-02,  5.98479770e-02,\n",
            "        8.34563747e-02, -4.47756052e-02, -1.50711890e-02, -5.43401353e-02,\n",
            "        2.55745389e-02,  1.19298592e-01,  5.42075559e-02, -3.80021445e-02,\n",
            "       -3.80866267e-02, -3.87747586e-02, -6.62083039e-03, -3.25076841e-02,\n",
            "        9.03698131e-02, -5.17555848e-02,  3.14592645e-02,  5.71805518e-03,\n",
            "        7.31707141e-02, -2.25682966e-02, -3.55327912e-02,  4.27188687e-02,\n",
            "       -3.62003222e-02,  3.28042731e-02,  5.60419001e-02,  1.84336249e-02,\n",
            "       -3.75982746e-02, -2.83909775e-02,  5.89651912e-02,  1.91175472e-02,\n",
            "        9.22119170e-02,  1.48129677e-02, -4.50587608e-02, -2.23587360e-02,\n",
            "       -7.17387199e-02, -3.92071269e-02,  5.95069444e-03,  6.38456792e-02,\n",
            "       -4.62051593e-02, -3.39625706e-03,  3.59736907e-04, -6.25518151e-03,\n",
            "        3.45467851e-02, -6.30530268e-02, -2.76988149e-02,  3.78423557e-02,\n",
            "       -2.84100100e-02, -4.98048998e-02,  1.58017501e-02, -3.31313536e-02,\n",
            "        7.93953836e-02, -4.99554351e-02,  3.07907746e-03, -1.31643647e-02,\n",
            "       -6.86460286e-02, -7.72471055e-02,  3.73515580e-03,  1.34239830e-02,\n",
            "        6.35408014e-02, -2.60318033e-02, -2.86093983e-03,  2.99866032e-02,\n",
            "        1.20500274e-01,  6.86491430e-02,  3.13491449e-02,  3.74370553e-02,\n",
            "        7.13704005e-02,  6.64242506e-02,  2.88509158e-03,  3.73695977e-02,\n",
            "       -6.88656196e-02,  9.06304047e-02, -2.95707565e-02,  8.36330503e-02,\n",
            "        6.12998083e-02, -5.21667674e-02,  1.74557362e-02, -3.21332548e-33,\n",
            "       -3.38273384e-02, -8.05348903e-02,  3.55331488e-02,  8.54420885e-02,\n",
            "        2.83797886e-02, -1.29161090e-01, -4.95025925e-02,  4.69749719e-02,\n",
            "       -3.01505588e-02, -6.80494308e-02,  1.20363735e-01, -4.66952138e-02,\n",
            "        3.45015004e-02,  1.32580884e-02, -4.37389640e-03,  2.88783330e-02,\n",
            "       -2.58403886e-02,  7.61748329e-02, -4.99635153e-02,  5.73543794e-02,\n",
            "       -5.32970130e-02,  3.86585705e-02, -1.01340175e-01, -1.73649471e-02,\n",
            "        7.05654398e-02, -1.09260157e-02,  2.67018694e-02, -1.93326976e-02,\n",
            "       -1.88710764e-02, -3.04551311e-02, -3.85628790e-02,  9.83256008e-03,\n",
            "       -9.56109725e-03, -9.34611168e-03, -2.59209238e-02,  2.72570085e-02,\n",
            "       -1.34597840e-02,  4.12226804e-02,  4.69756760e-02,  2.96583120e-03,\n",
            "       -4.56515849e-02,  6.80891052e-02, -6.11980148e-02,  6.11306122e-03,\n",
            "       -2.52894592e-02,  1.18689714e-02,  8.12494829e-02,  4.20512445e-02,\n",
            "        1.04026631e-01, -6.28135949e-02,  1.33231999e-02, -3.07159927e-02,\n",
            "       -1.25024142e-02,  5.63742816e-02,  4.54774797e-02, -3.25214528e-02,\n",
            "       -6.78015314e-03,  2.27181017e-02,  7.32464492e-02, -4.77039814e-02,\n",
            "       -5.34314290e-02,  2.24716123e-02, -3.72819267e-02, -5.19384556e-02,\n",
            "        6.24561450e-03, -1.80159733e-02, -3.91233340e-03, -1.73514839e-02,\n",
            "       -4.82952595e-02,  2.37226207e-02,  1.47237852e-01, -2.20273560e-05,\n",
            "       -1.43778831e-01,  1.04745720e-02, -2.62042023e-02, -3.16374935e-02,\n",
            "       -8.87856930e-02,  7.92135298e-02, -3.00843846e-02, -7.83336721e-03,\n",
            "        5.88886254e-02,  1.80231314e-02, -5.27996384e-02,  1.54876513e-02,\n",
            "        1.03925094e-02,  4.11262037e-03,  1.81181971e-02, -3.92179862e-02,\n",
            "        6.38311133e-02, -8.98541361e-02,  8.86632595e-03, -2.12713163e-02,\n",
            "       -8.21982622e-02, -1.17884338e-01, -1.14609063e-01, -3.19143076e-08,\n",
            "       -3.12563293e-02,  9.55474153e-02, -1.04736656e-01, -6.58422932e-02,\n",
            "       -4.81011383e-02,  7.56862992e-03,  6.46149293e-02,  4.45210822e-02,\n",
            "        2.64730118e-03,  2.93080881e-02, -6.82016015e-02,  6.98585063e-02,\n",
            "        4.41618152e-02, -2.35832445e-02, -2.24761497e-02, -4.80242781e-02,\n",
            "       -1.21965585e-02,  5.88250086e-02,  3.11920960e-02,  8.29439983e-03,\n",
            "        4.90426123e-02, -1.19509939e-02,  3.45100053e-02, -1.43613655e-03,\n",
            "       -2.77369693e-02, -9.39910207e-03,  3.43322270e-02,  7.39931175e-03,\n",
            "        5.88265620e-02, -1.09495319e-01, -7.58035779e-02,  3.85131314e-03,\n",
            "       -2.06849296e-02, -5.90627752e-02,  3.73082832e-02,  5.55631192e-03,\n",
            "        4.38501127e-02, -2.51608342e-03, -8.96049291e-02, -5.49707152e-02,\n",
            "        1.25347143e-02,  5.12639657e-02, -1.98058300e-02, -2.16680602e-03,\n",
            "       -9.42573696e-03, -1.07280567e-01, -3.49309593e-02, -4.82618697e-02,\n",
            "       -4.39002141e-02,  3.75368148e-02,  9.20933858e-02, -5.73458150e-02,\n",
            "        2.59017684e-02, -3.82202491e-02, -9.71433055e-03, -5.23938946e-02,\n",
            "        2.26046541e-03,  6.87464653e-03, -2.54466701e-02,  6.19779713e-02,\n",
            "       -3.80622898e-03, -1.15310811e-02,  1.00598872e-01, -3.02381199e-02])}, {'date': Timestamp('2016-07-01 00:00:00'), 'text_emb': array([-3.16988006e-02,  1.04987040e-01,  5.40172048e-02, -3.19456644e-02,\n",
            "        2.52371673e-02, -7.12645501e-02,  1.10958330e-01, -5.40968776e-02,\n",
            "        1.08832559e-02,  1.12837933e-01, -1.22989304e-01, -4.63768020e-02,\n",
            "       -2.02897005e-02,  9.50480774e-02, -4.57851812e-02,  9.37173516e-03,\n",
            "        1.14867128e-02,  8.66074488e-02, -2.50897072e-02, -6.80652782e-02,\n",
            "       -3.80303413e-02, -6.93790242e-02,  2.80160311e-04,  5.60446419e-02,\n",
            "       -3.12702656e-02, -1.88142620e-02, -4.60375808e-02,  7.64983594e-02,\n",
            "       -2.72808038e-02, -1.88077055e-02, -3.51989977e-02, -8.70092735e-02,\n",
            "        4.24594954e-02,  3.93304601e-02,  8.84758309e-03,  1.15768574e-01,\n",
            "       -1.62223000e-02, -4.31088246e-02, -1.32770389e-01, -1.96838062e-02,\n",
            "        5.39364405e-02,  2.56105457e-02,  4.46418785e-02, -1.02133332e-02,\n",
            "        2.51402240e-02,  1.13422178e-01,  3.99602726e-02, -2.26962045e-02,\n",
            "       -2.69989427e-02,  4.00963165e-02,  4.56905812e-02, -2.07434669e-02,\n",
            "        4.08821777e-02, -7.21174330e-02, -2.07249019e-02,  1.00919336e-01,\n",
            "       -3.17439474e-02, -8.11671838e-03, -4.53870818e-02,  1.08758323e-02,\n",
            "       -1.47740729e-02, -2.86021736e-03, -4.71915752e-02,  4.40533236e-02,\n",
            "       -1.18869450e-02, -7.05729984e-03, -5.59788011e-02,  5.36091700e-02,\n",
            "       -2.38376502e-02, -5.21445200e-02,  7.70677030e-02, -3.70699428e-02,\n",
            "       -3.54308859e-02, -8.13984200e-02,  6.93830177e-02,  5.23133092e-02,\n",
            "        4.99870442e-03, -1.24842236e-02,  2.67450493e-02,  5.83457723e-02,\n",
            "        8.97062048e-02, -9.38741341e-02,  2.54647192e-02, -3.17529552e-02,\n",
            "        6.70004562e-02, -2.11333763e-02,  2.84097288e-02, -4.34868038e-02,\n",
            "       -1.15667889e-02,  4.69540060e-02,  1.76566187e-02, -2.01576296e-03,\n",
            "        1.13108993e-01,  1.45901563e-02,  9.35930014e-02,  4.89669144e-02,\n",
            "       -8.71372372e-02,  5.10585643e-02, -7.08451793e-02,  9.90619361e-02,\n",
            "       -2.09102519e-02,  2.91148890e-02,  7.22298026e-02,  7.52044991e-02,\n",
            "        4.82504629e-03,  2.38130465e-02,  9.75546613e-02, -5.79959489e-02,\n",
            "       -4.33968306e-02,  8.16111490e-02, -6.05594404e-02,  2.70263292e-02,\n",
            "        6.97194710e-02,  5.66024985e-03,  6.30018488e-02,  7.91138336e-02,\n",
            "       -5.39311543e-02,  8.46098438e-02,  6.66569769e-02, -7.57207349e-02,\n",
            "       -5.33650704e-02, -1.56421587e-02, -3.59734483e-02,  5.45774065e-02,\n",
            "       -5.28439507e-02,  1.44202933e-01,  1.86456963e-02,  3.39462932e-34,\n",
            "       -4.48567420e-02,  4.70834188e-02,  9.71365720e-02, -9.54278335e-02,\n",
            "       -4.93677855e-02,  6.41837642e-02,  5.26276939e-02, -5.16051762e-02,\n",
            "        3.54255624e-02, -5.27256392e-02, -5.78800067e-02,  1.95179470e-02,\n",
            "        7.02856556e-02, -5.35403229e-02, -2.99213156e-02, -2.32812688e-02,\n",
            "        7.17070699e-02,  1.89798176e-02,  1.46093313e-02,  7.02031925e-02,\n",
            "        7.60638118e-02,  6.04001991e-03, -7.25543872e-02, -2.87992209e-02,\n",
            "        4.04697768e-02,  5.85356690e-02, -1.84065793e-02, -5.83724864e-03,\n",
            "        4.89441343e-02,  2.98366304e-02, -6.74458966e-02, -3.33622023e-02,\n",
            "       -4.27334644e-02, -1.82107147e-02, -1.51739167e-02,  1.85979158e-02,\n",
            "       -2.04473138e-02, -7.36823445e-03, -4.01619561e-02, -7.04430044e-03,\n",
            "       -4.09141444e-02, -2.64248415e-03, -4.44268510e-02,  1.39733115e-02,\n",
            "       -3.32480762e-03, -3.52017321e-02,  3.17740589e-02,  4.34067845e-02,\n",
            "        1.05148576e-01, -2.22821273e-02, -2.98568327e-02, -1.58394743e-02,\n",
            "       -5.30386902e-02, -1.16782576e-01,  7.40540475e-02,  1.49147222e-02,\n",
            "        2.29622778e-02,  2.33744867e-02,  9.30633489e-03,  1.90364383e-02,\n",
            "       -6.06761836e-02, -1.02117155e-02, -5.52100874e-02,  8.85962546e-02,\n",
            "        5.39266467e-02,  4.22315337e-02, -3.40159461e-02, -4.76741567e-02,\n",
            "       -2.27971990e-02, -4.40327041e-02,  4.53937389e-02, -1.47244502e-02,\n",
            "       -2.17639264e-02, -7.86042213e-03, -7.43332878e-02,  2.61651427e-02,\n",
            "        1.99742652e-02, -5.23276180e-02, -8.58504772e-02,  4.47703972e-02,\n",
            "       -6.11126423e-02, -1.62542760e-02,  1.19695840e-02,  2.76631285e-02,\n",
            "       -8.54865834e-03, -2.63618715e-02,  6.05770871e-02,  4.10284474e-02,\n",
            "        1.07738689e-01,  5.98709099e-02,  2.19536759e-02, -8.62998217e-02,\n",
            "        1.85466111e-02,  7.29306489e-02, -1.37704924e-01, -9.87878748e-34,\n",
            "       -4.94584395e-03,  1.98009685e-02, -8.00134093e-02, -1.61377322e-02,\n",
            "       -1.52881839e-03, -3.05880298e-04,  4.86572832e-02,  5.84060922e-02,\n",
            "        8.32179911e-04, -1.09736277e-02,  2.64489949e-02, -3.27505320e-02,\n",
            "        5.17992228e-02, -6.10421486e-02, -8.19085911e-02,  5.08989207e-02,\n",
            "        3.51717360e-02,  4.45525115e-03, -2.11529415e-02,  1.62726697e-02,\n",
            "       -1.97020378e-02,  4.68762740e-02,  3.32913548e-02, -1.82624080e-03,\n",
            "       -3.48016284e-02, -2.74811164e-02,  2.51334794e-02,  1.68136731e-02,\n",
            "       -8.56948271e-02, -2.97549125e-02, -3.52194011e-02, -3.98785882e-02,\n",
            "       -4.16477509e-02,  1.46285584e-02,  5.32718189e-02,  3.18091698e-02,\n",
            "       -3.45680974e-02, -1.91796012e-02, -1.80464685e-02,  5.35519123e-02,\n",
            "       -3.04370113e-02,  2.11380087e-02, -6.76178513e-03,  1.09447055e-01,\n",
            "        2.80571822e-02,  8.82675871e-02, -8.92895907e-02,  5.67872934e-02,\n",
            "       -3.21153901e-03, -4.94134687e-02, -1.32684678e-01, -5.03459945e-02,\n",
            "       -4.22096364e-02, -1.85044203e-02,  4.81303874e-03,  2.58424375e-02,\n",
            "       -4.61171977e-02,  5.07823788e-02, -5.35953157e-02,  4.43296181e-03,\n",
            "        7.00149313e-02, -1.05882823e-01,  2.15788130e-02, -1.30928140e-02,\n",
            "        3.41654904e-02, -5.41277900e-02, -5.04426844e-03,  2.45476514e-02,\n",
            "       -9.08058733e-02,  6.74326941e-02, -6.12862855e-02,  1.81254081e-03,\n",
            "       -9.11145657e-02,  3.78346555e-02, -3.35317152e-03,  3.01951263e-02,\n",
            "       -1.97922457e-02,  9.85017791e-02,  6.09320439e-02,  8.87656491e-03,\n",
            "       -6.13484047e-02,  3.90255228e-02, -4.85913754e-02, -5.61698079e-02,\n",
            "        6.14618063e-02, -3.38841323e-03,  3.66061889e-02, -4.77611758e-02,\n",
            "        5.96874915e-02,  2.71276627e-02,  6.84627220e-02,  1.19226650e-04,\n",
            "        1.12350024e-02,  1.17068244e-02, -2.59907432e-02, -1.73073236e-08,\n",
            "        5.57189323e-02,  6.42400282e-03, -3.67570631e-02,  5.59596252e-03,\n",
            "       -8.73917043e-02,  7.27054151e-03, -4.51912265e-03, -1.22443236e-01,\n",
            "        1.38790356e-02, -8.27848539e-03, -6.36947677e-02,  3.75267141e-03,\n",
            "        2.55802851e-02,  1.06673250e-02,  3.83397471e-03,  2.37195846e-02,\n",
            "       -6.58590766e-03, -1.02875773e-02, -5.73544856e-03,  1.34756844e-02,\n",
            "       -3.15035470e-02, -1.27545362e-02, -4.59097959e-02, -6.42687753e-02,\n",
            "       -8.57103709e-03, -6.36652391e-03,  3.17610465e-02, -4.07274365e-02,\n",
            "       -2.85473373e-02,  3.54797617e-02, -4.65935804e-02, -3.18898670e-02,\n",
            "       -1.94488391e-02, -4.10310440e-02,  3.25964019e-02,  8.40540826e-02,\n",
            "        1.47649869e-02, -3.49527337e-02, -4.66037281e-02, -6.27248585e-02,\n",
            "       -5.83860055e-02,  5.67674413e-02,  3.65665443e-02, -2.85405200e-02,\n",
            "        7.03493059e-02,  1.91855766e-02,  5.17610162e-02, -9.09909308e-02,\n",
            "        2.34855246e-02, -1.36827743e-02, -5.70319965e-02, -1.23827420e-02,\n",
            "        1.00939041e-02,  2.18471345e-02,  8.55854973e-02, -1.12650543e-02,\n",
            "       -2.50171423e-02, -3.44895720e-02,  2.68011950e-02,  1.76261291e-02,\n",
            "       -1.64883733e-02, -1.85347609e-02, -5.09026572e-02, -3.25117558e-02])}, {'date': Timestamp('2016-07-01 00:00:00'), 'text_emb': array([ 3.36340293e-02, -1.63894277e-02,  6.81505725e-02, -1.41071677e-01,\n",
            "        4.51107398e-02, -3.46402079e-02,  3.75193246e-02, -2.14408766e-02,\n",
            "        1.04883481e-02, -3.19648162e-02, -5.48500381e-02,  2.46245656e-02,\n",
            "        4.18224707e-02,  3.77596309e-03, -3.03883366e-02,  1.48779415e-02,\n",
            "       -2.22983547e-02,  1.98727325e-02, -4.81989905e-02,  4.34991121e-02,\n",
            "        9.94973406e-02,  1.14322519e-02,  1.06318211e-02,  1.74418353e-02,\n",
            "        2.10506804e-02, -3.84669602e-02, -1.09932227e-02,  3.72754447e-02,\n",
            "       -1.22526847e-02,  1.74129475e-02,  7.18097910e-02, -1.68694891e-02,\n",
            "       -9.05192941e-02, -3.37923393e-02, -3.69195789e-02, -7.37239942e-02,\n",
            "       -1.22422976e-02, -1.04289930e-02,  4.45432663e-02, -2.87937932e-02,\n",
            "       -2.77112033e-02, -9.74771008e-02, -3.26511413e-02,  1.17908604e-02,\n",
            "        2.75718961e-02,  8.31499770e-02, -3.06982417e-02,  3.27667184e-02,\n",
            "       -2.04520449e-02,  1.19862985e-02,  1.69429816e-02,  6.74762279e-02,\n",
            "        4.21595387e-02, -9.96126533e-02, -2.90044826e-02,  1.20108705e-02,\n",
            "        2.14341898e-02, -1.61285643e-02,  8.76876432e-03,  3.57111283e-02,\n",
            "        3.08743375e-03, -4.37225588e-03,  3.42677440e-03,  3.80628519e-02,\n",
            "        8.38198289e-02,  9.54875164e-03, -6.29397109e-02, -2.40567047e-02,\n",
            "       -1.23167470e-01,  7.28710443e-02,  1.17740827e-02, -5.56241395e-03,\n",
            "        4.00882736e-02, -9.21587052e-05,  7.53664970e-02,  4.30275165e-02,\n",
            "       -3.70213464e-02,  8.20021629e-02, -1.77370161e-02,  3.49618420e-02,\n",
            "       -9.58630070e-02,  1.48519559e-03,  3.25461663e-02, -7.79216364e-02,\n",
            "        9.79277771e-03, -7.46065611e-03,  7.86467195e-02, -1.26492465e-02,\n",
            "       -4.64069247e-02,  5.45935780e-02, -9.94261280e-02, -9.28288475e-02,\n",
            "       -1.28887249e-02,  1.00353763e-01, -1.36575028e-01, -1.75554473e-02,\n",
            "        3.87312099e-02,  4.96963523e-02, -9.60362852e-02,  2.73067001e-02,\n",
            "       -1.06284976e-01,  8.85579810e-02,  5.64346462e-03,  2.27130409e-02,\n",
            "       -6.95075616e-02,  1.10566253e-02, -2.08955668e-02,  1.14876563e-02,\n",
            "        6.51123151e-02, -4.37598489e-02, -4.16310132e-02, -5.21982312e-02,\n",
            "        6.92002699e-02, -3.80911566e-02,  2.78770421e-02, -3.33530083e-02,\n",
            "       -3.77677158e-02, -9.19970572e-02, -6.73280805e-02, -4.90237996e-02,\n",
            "        2.31184550e-02,  3.42814513e-02, -1.58997737e-02,  5.10312803e-02,\n",
            "       -3.31948809e-02, -2.84447856e-02,  3.45112272e-02, -1.67676775e-33,\n",
            "        1.30647523e-02, -1.17147854e-02,  6.76214546e-02,  2.93136239e-02,\n",
            "       -6.16391525e-02,  4.40974924e-04, -9.52436328e-02, -6.21250411e-03,\n",
            "        5.86303324e-02, -9.35679823e-02, -6.36056289e-02,  1.92735158e-02,\n",
            "        1.34702204e-02,  9.29586217e-03,  2.92368103e-02,  6.07550368e-02,\n",
            "        5.89337908e-02,  5.95645746e-03,  1.93861928e-02, -7.64017180e-02,\n",
            "        6.92470744e-02, -7.91784823e-02,  7.69762546e-02, -2.11238433e-02,\n",
            "        8.89100581e-02, -3.07324175e-02,  3.63755673e-02,  3.23990472e-02,\n",
            "       -1.09109869e-02,  7.09627056e-03, -1.18325718e-01, -3.59197669e-02,\n",
            "       -9.23649780e-03,  5.19401208e-02, -3.29203717e-02,  3.87671366e-02,\n",
            "        1.17941275e-02,  2.97649708e-02,  1.87693425e-02,  3.51867899e-02,\n",
            "        6.34389371e-02, -3.26666757e-02, -7.32019171e-02,  2.13169884e-02,\n",
            "       -2.41675191e-02, -6.45322800e-02, -8.58583115e-03, -4.15132418e-02,\n",
            "        4.50034579e-03, -6.88858889e-03,  5.00686951e-02, -6.90811267e-03,\n",
            "       -9.64487530e-03, -4.83795144e-02,  1.05960913e-01,  8.46513081e-03,\n",
            "       -4.26191017e-02,  4.97322828e-02,  5.27490750e-02, -8.45933929e-02,\n",
            "       -4.85782698e-02,  3.96825783e-02, -4.43793759e-02,  9.91232097e-02,\n",
            "       -7.52552599e-02,  1.11874416e-02, -1.20304041e-02, -2.13877577e-02,\n",
            "       -6.85241669e-02, -3.06709409e-02, -4.02562879e-02,  6.29104152e-02,\n",
            "        4.92326804e-02, -5.87711744e-02, -1.10970093e-02,  2.50708871e-02,\n",
            "       -2.52281372e-02,  1.55612826e-02,  1.09219663e-01,  1.75876811e-03,\n",
            "       -1.86346974e-02, -2.75885817e-02,  3.16053331e-02, -2.47051157e-02,\n",
            "       -8.20340496e-03,  7.34255016e-02,  4.08415776e-03, -2.04417780e-02,\n",
            "        4.48129326e-02, -4.48684432e-02, -9.57629383e-02, -8.58073160e-02,\n",
            "        5.58198430e-02, -3.43651809e-02, -4.91428636e-02, -1.42343963e-33,\n",
            "       -4.26384062e-02,  3.91658582e-02, -1.54996598e-02,  1.97859462e-02,\n",
            "        3.33368890e-02,  7.76718184e-02,  6.31728321e-02,  3.16009484e-03,\n",
            "       -1.33337462e-02, -5.12309838e-03,  8.05429649e-03, -3.62066217e-02,\n",
            "        7.87565336e-02, -1.03979232e-02,  1.36685995e-02, -2.12858599e-02,\n",
            "        9.06927884e-02, -6.88399076e-02,  7.44621037e-04,  8.57532620e-02,\n",
            "       -4.33768407e-02,  2.02047285e-02, -9.86830220e-02,  3.02337594e-02,\n",
            "       -6.97063804e-02, -5.32623241e-03,  5.34976423e-02, -1.91146620e-02,\n",
            "       -6.59375191e-02, -1.25599742e-01, -2.75689666e-03, -7.51445303e-03,\n",
            "       -2.12445669e-02,  5.13959825e-02,  1.01403818e-01,  6.50557643e-03,\n",
            "       -2.21886318e-02, -2.17233859e-02,  1.36803545e-04,  8.12712908e-02,\n",
            "        1.06091285e-02, -6.55813217e-02,  2.07164884e-02, -4.37556161e-03,\n",
            "        4.92306128e-02,  6.29173731e-03,  9.76568670e-04, -7.57206529e-02,\n",
            "        3.85062546e-02,  7.53031373e-02,  5.18537723e-02,  1.32901236e-01,\n",
            "       -5.04352227e-02, -9.96934064e-03, -2.38441192e-02, -2.54623797e-02,\n",
            "       -3.06313951e-02,  7.25298515e-03, -2.83526685e-02,  5.44800311e-02,\n",
            "       -4.86808158e-02, -4.90045501e-03,  1.70198381e-02, -7.37201236e-03,\n",
            "        3.79311554e-02, -2.71357857e-02, -1.86045155e-01,  1.04396299e-01,\n",
            "        9.83963758e-02,  4.79009226e-02,  1.12597972e-01, -4.48861346e-02,\n",
            "       -4.45247516e-02, -1.52609069e-02,  7.16047287e-02, -7.54748425e-03,\n",
            "        1.51671087e-02,  7.56907230e-03, -1.33694559e-02,  2.75545642e-02,\n",
            "       -1.63901355e-02, -2.03809068e-02,  4.27254150e-03, -2.90326457e-02,\n",
            "       -2.30507143e-02, -7.39188045e-02, -6.34647384e-02, -1.28514826e-01,\n",
            "        3.62461396e-02, -6.38388917e-02,  5.30223809e-02, -5.35994675e-03,\n",
            "        3.14712338e-03, -7.10107908e-02,  1.99542567e-02, -1.98308392e-08,\n",
            "        5.98974787e-02,  1.97894871e-02, -5.11933351e-03,  2.09334996e-02,\n",
            "       -2.37429719e-02, -2.10152306e-02, -3.07454318e-02, -9.49396491e-02,\n",
            "        3.22023071e-02, -2.22739577e-02,  2.36535445e-02, -8.30631331e-03,\n",
            "       -8.46433267e-02, -8.82348120e-02, -4.46943678e-02,  1.15297036e-02,\n",
            "        4.14495915e-02, -4.88612503e-02,  2.38216016e-02,  6.07338697e-02,\n",
            "       -2.55346968e-04,  1.89692155e-02,  1.93085875e-02,  2.60100570e-02,\n",
            "        2.93951165e-02, -5.24834506e-02,  1.01837879e-02, -2.32158694e-02,\n",
            "       -2.20384412e-02,  5.40314987e-02,  1.03645050e-03, -4.91625257e-02,\n",
            "       -5.15567437e-02,  2.97502242e-03,  6.99015036e-02, -6.66595623e-02,\n",
            "        1.80170666e-02,  1.03881240e-01,  4.87864539e-02,  2.60320045e-02,\n",
            "        1.93625465e-02,  4.31605503e-02,  3.47635546e-03, -1.24886008e-02,\n",
            "       -5.15960231e-02,  2.52744332e-02,  1.67415887e-02, -6.35448471e-02,\n",
            "        5.64598478e-02,  2.31863223e-02, -7.39060938e-02,  7.39695653e-02,\n",
            "        5.31058796e-02, -2.90825088e-02,  4.89137918e-02,  5.64801879e-02,\n",
            "        5.63552193e-02, -2.65580080e-02, -3.38130891e-02, -4.77772765e-02,\n",
            "       -1.63155864e-03,  2.33569322e-03,  1.38152868e-01,  5.29725887e-02])}, {'date': Timestamp('2016-07-01 00:00:00'), 'text_emb': array([ 4.89868596e-02,  9.35921818e-02, -4.04195413e-02,  5.17741591e-02,\n",
            "        3.15695745e-03,  9.16799158e-02,  8.93122479e-02, -7.95463324e-02,\n",
            "       -5.38692400e-02,  2.31650695e-02, -8.36596452e-03, -3.99840325e-02,\n",
            "       -8.21468383e-02,  3.92640829e-02,  2.92389449e-02, -7.50221089e-02,\n",
            "        6.40600696e-02, -5.24384715e-02,  1.60565004e-02,  8.20919201e-02,\n",
            "        1.18577220e-01,  4.13747765e-02,  8.37754086e-02, -2.47297753e-02,\n",
            "       -5.73364347e-02, -5.49148396e-02,  4.30571772e-02,  3.86516079e-02,\n",
            "       -1.59655735e-02,  5.16017489e-02,  3.50604132e-02, -4.74151969e-03,\n",
            "        3.55329402e-02,  3.14527750e-02, -3.84144150e-02, -4.64448631e-02,\n",
            "        1.17902197e-02, -3.94474678e-02,  2.91829351e-02, -3.66321281e-02,\n",
            "        5.53015023e-02, -1.28297433e-01,  2.70803478e-02,  7.90668055e-02,\n",
            "       -2.70122401e-02,  7.06088915e-02,  2.65393406e-02, -5.19662015e-02,\n",
            "       -5.63188605e-02, -6.84999377e-02,  3.32568921e-02, -8.92127752e-02,\n",
            "        8.06828663e-02,  2.93184314e-02,  1.91988889e-02, -1.04489945e-01,\n",
            "        1.83885302e-02, -3.94026656e-03, -2.47482751e-02,  5.14169224e-03,\n",
            "       -4.28176634e-02,  1.83674041e-02,  9.18846484e-03, -4.38802177e-03,\n",
            "        4.55404818e-02,  2.77874339e-02, -6.56637326e-02,  3.95731553e-02,\n",
            "        8.13973173e-02,  1.00584157e-01,  6.89766854e-02, -1.01301512e-02,\n",
            "       -3.37243862e-02, -4.63055121e-03, -5.06406240e-02,  3.00587062e-03,\n",
            "       -7.27331964e-03,  3.34169120e-02,  1.73544586e-02, -1.10296637e-01,\n",
            "       -3.50172110e-02, -6.87207002e-03, -2.20776722e-03,  4.17020842e-02,\n",
            "       -1.68860592e-02, -6.78556785e-02,  3.70549262e-02, -2.21124403e-02,\n",
            "        4.77577932e-02, -3.75969112e-02, -3.48253697e-02, -6.66223047e-03,\n",
            "       -1.09130843e-02, -3.33887786e-02,  1.47635220e-02, -7.65170949e-03,\n",
            "       -4.62729633e-02, -5.37495278e-02, -2.55062878e-01,  4.89890575e-02,\n",
            "       -6.91378638e-02,  7.05993269e-03, -3.98809426e-02,  4.34048623e-02,\n",
            "        6.65394589e-02, -3.81083935e-02, -5.91115542e-02, -2.69373078e-02,\n",
            "       -5.85879479e-03, -1.82874396e-03,  2.64676437e-02,  5.48996776e-02,\n",
            "        4.98229712e-02,  1.49088437e-02,  3.64066176e-02, -1.71772651e-02,\n",
            "       -6.64450675e-02, -4.82509611e-03,  5.49524464e-02, -1.13312704e-02,\n",
            "        3.69708403e-03, -2.59226318e-02, -9.46837515e-02,  2.56320387e-02,\n",
            "       -2.44054180e-02,  1.42569738e-02,  4.83649075e-02, -1.49911428e-33,\n",
            "        2.14934871e-02, -5.23462296e-02,  2.61662696e-02, -5.32620773e-03,\n",
            "        4.40584384e-02,  6.88187182e-02,  5.79077117e-02, -2.89146584e-02,\n",
            "        4.12657037e-02,  1.35838455e-02,  7.30177760e-02, -1.21929318e-01,\n",
            "        4.13736217e-02,  1.54891880e-02, -4.10974473e-02,  2.38092691e-02,\n",
            "        7.99671039e-02,  2.41798349e-02,  1.93432700e-02,  5.92821091e-03,\n",
            "       -2.03589462e-02,  2.80040689e-03,  4.22393763e-03,  1.97909516e-03,\n",
            "       -4.62913997e-02, -1.22997127e-02, -2.58985674e-03, -4.46793362e-02,\n",
            "        3.90749499e-02,  1.03638265e-02, -3.25577741e-04,  1.18111141e-01,\n",
            "        7.02051222e-02, -5.14373854e-02,  9.51122306e-03,  2.90593673e-02,\n",
            "        1.12327456e-03,  8.19746312e-03, -4.49254997e-02, -8.80058110e-03,\n",
            "        2.39456017e-02,  1.83782894e-02,  1.85358524e-02,  3.62709500e-02,\n",
            "        9.52589326e-03, -2.72635054e-02, -1.47308214e-02, -3.68512757e-02,\n",
            "        1.90274138e-03,  6.32766262e-02,  2.21926421e-02,  2.51837745e-02,\n",
            "       -5.84710110e-03, -4.89037856e-02, -5.21039777e-02, -1.41088376e-02,\n",
            "        1.68530061e-03,  2.29828805e-02, -3.19629945e-02, -4.08856198e-02,\n",
            "        3.04071680e-02, -3.69228423e-02,  9.79127828e-03, -2.41148360e-02,\n",
            "       -4.07090737e-03, -7.68806413e-02,  3.04411333e-02, -9.38079059e-02,\n",
            "       -1.36737689e-01, -2.14475440e-03, -4.69749719e-02,  6.79233670e-02,\n",
            "        1.93101130e-02, -6.09610826e-02, -5.57313412e-02, -3.41559127e-02,\n",
            "       -6.04461739e-03, -1.47407930e-02,  1.76541489e-02, -5.81286587e-02,\n",
            "        5.16704991e-02, -5.46676889e-02,  9.41489115e-02, -4.77205254e-02,\n",
            "       -4.71487083e-02,  6.89530512e-03,  4.51659858e-02,  3.99222560e-02,\n",
            "        4.87107486e-02,  2.27239393e-02, -2.46941485e-02, -1.07333452e-01,\n",
            "        4.77088504e-02,  3.50351743e-02,  2.63387971e-02, -3.08970448e-34,\n",
            "       -9.30713713e-02, -6.27170131e-02, -3.83054502e-02,  2.77221613e-02,\n",
            "        1.29950270e-01, -3.95506471e-02, -3.70268635e-02, -4.77341563e-02,\n",
            "        3.22810188e-03,  9.05147865e-02, -3.58430594e-02, -4.88176532e-02,\n",
            "        7.52372220e-02,  4.11163792e-02,  8.64837393e-02,  4.16761711e-02,\n",
            "        3.35607380e-02, -4.15246561e-02, -4.49511595e-03,  2.84928605e-02,\n",
            "        1.34634553e-02,  2.78782006e-03,  2.77159251e-02, -6.59432542e-03,\n",
            "       -4.10998501e-02,  4.03271951e-02,  1.12772383e-01,  5.57461940e-02,\n",
            "       -9.41749811e-02, -2.81087775e-02, -5.11630662e-02,  7.38589540e-02,\n",
            "       -4.21441980e-02, -6.72004698e-03, -3.38704027e-02, -2.28034239e-02,\n",
            "       -7.14217871e-03,  9.52955708e-03,  3.60246375e-02,  1.11232907e-01,\n",
            "       -1.03739388e-02,  6.54688701e-02, -3.04851606e-02, -1.79471709e-02,\n",
            "       -5.25579080e-02, -1.33281527e-02,  5.55394106e-02,  3.61794718e-02,\n",
            "       -4.36000638e-02,  4.35936600e-02,  1.26256226e-02,  2.38757245e-02,\n",
            "       -3.16372514e-02, -2.53814757e-02, -3.11830286e-02, -8.26160312e-02,\n",
            "       -7.77922571e-02, -6.80201501e-02, -6.84661046e-02,  4.97686416e-02,\n",
            "        3.80848423e-02,  1.58242024e-02,  2.94894800e-02,  3.84079367e-02,\n",
            "       -1.31374784e-03,  3.75652569e-03, -7.11784661e-02, -2.12754309e-02,\n",
            "        8.96213278e-02,  1.10549880e-02,  3.03854756e-02, -1.12190112e-01,\n",
            "        6.62277192e-02, -2.50443332e-02, -3.39456387e-02, -9.05109495e-02,\n",
            "        5.14021739e-02,  2.38461420e-02, -1.91337764e-02, -1.06012709e-01,\n",
            "        4.76481095e-02, -1.12350218e-01,  6.79807141e-02,  4.50423732e-02,\n",
            "       -3.18382494e-02,  9.53890756e-02,  2.01409124e-02,  2.88060810e-02,\n",
            "       -1.22288743e-03,  4.52214815e-02, -1.56410113e-02, -1.82042494e-02,\n",
            "       -8.41914210e-03,  1.35214478e-02,  7.01465979e-02, -3.13729700e-08,\n",
            "        1.07033104e-02, -4.84220535e-02, -3.78733799e-02, -4.80471104e-02,\n",
            "        9.31963772e-02,  4.55376226e-03, -5.72276972e-02, -1.00896604e-01,\n",
            "        7.34882951e-02,  1.78386047e-02, -3.66020075e-04,  4.53919433e-02,\n",
            "        6.76804781e-02, -7.51115233e-02, -5.83332442e-02, -5.88715822e-02,\n",
            "        2.84416229e-02, -4.50164452e-02, -1.16696969e-01,  3.97176929e-02,\n",
            "       -1.83354709e-02, -6.28498523e-03, -3.02277915e-02, -1.66484434e-02,\n",
            "        2.55884677e-02,  9.02680308e-02,  6.99374974e-02,  2.59406548e-02,\n",
            "        4.62534316e-02,  7.90818706e-02,  3.18215741e-03,  3.04607991e-02,\n",
            "       -2.57411953e-02,  1.59708466e-02, -2.22229566e-02,  1.46807292e-02,\n",
            "        3.90843041e-02,  3.95897627e-02,  5.14730029e-02, -5.63731641e-02,\n",
            "       -1.04940496e-02, -4.96111140e-02,  2.61197928e-02,  4.79800366e-02,\n",
            "       -8.72922037e-03,  1.66336913e-02, -8.71463567e-02,  3.46066132e-02,\n",
            "        2.56784726e-02,  1.31198503e-02, -9.10945311e-02,  3.65708917e-02,\n",
            "        7.04028606e-02,  6.78506345e-02,  3.88657413e-02, -1.82409436e-02,\n",
            "        6.46516262e-03, -1.66273154e-02, -9.08719376e-03, -3.21710408e-02,\n",
            "        1.35757737e-02, -1.01742707e-01, -1.83619838e-02,  8.95372871e-03])}, {'date': Timestamp('2016-07-01 00:00:00'), 'text_emb': array([-1.88603904e-02,  3.34954038e-02,  6.87934458e-02, -7.19748763e-03,\n",
            "        1.24452591e-01,  2.66675018e-02,  1.82418488e-02,  7.71970823e-02,\n",
            "       -2.76356712e-02,  6.58336952e-02, -2.95038112e-02,  5.10199219e-02,\n",
            "       -7.83396065e-02, -1.67189725e-02, -6.81865886e-02,  1.13896199e-01,\n",
            "       -4.73318882e-02, -8.36213876e-05, -1.45647600e-02, -5.09958379e-02,\n",
            "       -9.29054990e-03,  1.20299282e-02,  5.70934489e-02,  4.15554233e-02,\n",
            "       -5.15017211e-02,  3.39983590e-03, -2.19377372e-02, -5.40704876e-02,\n",
            "       -3.85763533e-02,  2.76182368e-02, -1.47763109e-02,  6.13287743e-03,\n",
            "        1.56378113e-02,  8.58172681e-03, -2.87853368e-02,  5.39484173e-02,\n",
            "       -8.54114722e-03, -7.84805324e-03, -5.71176223e-02, -3.56048793e-02,\n",
            "        4.54838797e-02, -7.58749470e-02, -4.76507731e-02, -5.00001945e-02,\n",
            "        2.54152231e-02, -2.86276154e-02,  3.89068126e-04, -2.50879186e-03,\n",
            "       -6.01603370e-03, -6.05807379e-02,  2.10598987e-02, -1.01655081e-01,\n",
            "       -5.07371537e-02, -4.66312729e-02,  2.36412138e-02, -3.16142873e-03,\n",
            "        1.62529387e-02, -1.77587066e-02, -5.77953318e-03, -1.50519712e-02,\n",
            "       -1.19354371e-02, -6.90968856e-02, -2.92726178e-02, -1.21219857e-02,\n",
            "        4.68071438e-02,  1.60936430e-01,  2.05778074e-03,  2.04541869e-02,\n",
            "       -5.34671471e-02,  8.05358309e-03,  8.29610303e-02, -2.69620437e-02,\n",
            "        4.56654690e-02, -4.13379353e-03,  3.78512144e-02,  3.61038148e-02,\n",
            "        2.30250545e-02,  7.45895579e-02,  1.29762083e-01, -2.46197991e-02,\n",
            "        6.24177568e-02, -6.48524426e-03, -1.97146628e-02, -6.20904863e-02,\n",
            "       -3.66837345e-02, -2.27001458e-02,  3.13955173e-03, -1.59784425e-02,\n",
            "        9.72420424e-02,  2.06514634e-02, -4.52600941e-02, -4.73985672e-02,\n",
            "        8.39784667e-02,  2.78059244e-02, -4.79476377e-02,  5.14921658e-02,\n",
            "       -2.26683505e-02, -1.00731090e-01, -2.01202389e-02, -2.15860158e-02,\n",
            "        2.32676994e-02,  5.27722612e-02, -3.25716324e-02, -4.02526632e-02,\n",
            "       -3.37061025e-02,  8.13183840e-03, -5.22191674e-02, -7.66580226e-04,\n",
            "        8.93974826e-02,  5.15038073e-02,  9.68789868e-03,  5.73578291e-02,\n",
            "        7.01969787e-02,  5.42264059e-02, -4.26310562e-02,  2.47078836e-02,\n",
            "        7.61139914e-02, -1.30156884e-02,  2.93767415e-02, -9.63057205e-02,\n",
            "       -1.73795708e-02, -2.37582773e-02, -3.89824025e-02, -4.90497015e-02,\n",
            "       -2.30400991e-02,  5.04403301e-02, -9.31667015e-02,  5.37739706e-35,\n",
            "       -6.05042465e-02, -4.31205146e-03,  1.18780985e-01, -3.48031707e-02,\n",
            "        2.72152722e-02,  1.05566448e-02, -2.96389703e-02,  1.69442571e-03,\n",
            "        1.27544738e-02, -5.59067428e-02, -7.16527030e-02, -2.43695620e-02,\n",
            "        1.88380592e-02, -5.45392279e-03,  1.90806063e-03, -3.61540057e-02,\n",
            "       -6.95677325e-02,  2.65904125e-02,  2.54371688e-02, -3.09573989e-02,\n",
            "       -1.35336975e-02, -3.70067768e-02, -3.33547555e-02,  5.96478693e-02,\n",
            "        9.00275633e-02,  3.64443436e-02, -3.28197377e-03, -9.40020836e-04,\n",
            "        6.06838390e-02,  1.88631155e-02,  1.76435392e-02,  1.97577178e-02,\n",
            "       -2.80202944e-02,  2.77259704e-02,  4.63160127e-02, -1.84094757e-02,\n",
            "       -6.32224977e-02, -2.19523273e-02, -5.83741581e-03,  3.45123149e-02,\n",
            "       -2.63590980e-02,  5.83788082e-02, -2.23997161e-02, -4.69959155e-02,\n",
            "        1.15976043e-01,  1.45166386e-02,  5.42706549e-02, -4.58898842e-02,\n",
            "        5.29735815e-04,  2.97350269e-02, -7.30782375e-02,  1.19473845e-01,\n",
            "       -3.45335305e-02, -2.06629913e-02,  8.49452391e-02, -5.20833172e-02,\n",
            "       -3.62238958e-02,  3.86491194e-02, -1.54324034e-02, -1.21736489e-01,\n",
            "       -3.59262899e-02,  1.34787947e-01, -3.97213362e-02, -2.41260161e-03,\n",
            "       -4.79714572e-02,  5.18562570e-02, -1.06669016e-01, -1.22627979e-02,\n",
            "       -8.66634846e-02,  1.12349644e-01,  8.69486779e-02, -6.68355748e-02,\n",
            "       -1.08012795e-01, -5.33438623e-02, -6.43597320e-02, -6.57132193e-02,\n",
            "       -1.91076733e-02,  4.64880876e-02,  1.34238442e-02,  4.11004350e-02,\n",
            "        8.53607729e-02,  4.86975946e-02,  3.36682075e-03, -2.52210088e-02,\n",
            "       -2.27858666e-02, -2.55778655e-02, -6.54311059e-03,  1.10539533e-02,\n",
            "        4.78132069e-02,  9.00036748e-03, -1.28996512e-02,  3.06165451e-03,\n",
            "        2.15545297e-02,  5.66336736e-02, -1.47417605e-01, -1.20415107e-33,\n",
            "       -9.41072125e-03, -4.29431163e-02,  6.59215748e-02,  2.53765583e-02,\n",
            "        5.55286109e-02, -4.13186699e-02, -8.05102736e-02, -7.12357312e-02,\n",
            "        8.53484496e-02,  3.14194076e-02,  8.53667632e-02,  5.27181961e-02,\n",
            "        3.55114490e-02,  1.97289102e-02,  8.91401991e-03, -3.61910127e-02,\n",
            "        7.94209354e-03,  3.58620845e-02, -1.21301219e-01, -4.45142724e-02,\n",
            "        1.25238011e-02,  4.18083072e-02,  4.40939888e-03,  1.11044057e-01,\n",
            "       -3.10898889e-02, -8.18903558e-03, -2.55055521e-02,  6.82645962e-02,\n",
            "        4.90447357e-02, -6.82642236e-02, -2.79516280e-02,  8.60601813e-02,\n",
            "       -1.04359210e-01,  3.04955579e-02,  2.49972567e-02,  3.93609963e-02,\n",
            "       -5.31484336e-02,  1.00018848e-02,  3.00556626e-02,  4.07901295e-02,\n",
            "        4.00287472e-02,  1.69576388e-02, -6.96206018e-02,  3.72145092e-04,\n",
            "       -1.55137144e-02,  2.09383313e-02, -4.24742177e-02, -2.65597980e-02,\n",
            "        1.11650024e-02, -6.33918308e-03, -3.01813092e-02,  8.16029236e-02,\n",
            "       -3.43380943e-02, -1.61972344e-02,  1.09079806e-03, -6.61872700e-02,\n",
            "       -1.25937667e-02,  8.97833891e-03, -1.77671127e-02, -1.67116635e-02,\n",
            "       -8.34767299e-04, -1.43282283e-02,  1.69921778e-02,  3.54275741e-02,\n",
            "        2.20654551e-02, -6.18506484e-02,  4.37348746e-02,  6.36766776e-02,\n",
            "        3.34529430e-02, -5.95821738e-02,  4.51712199e-02,  3.36438827e-02,\n",
            "       -2.65425090e-02, -4.01635654e-02, -3.66717651e-02, -8.50774813e-03,\n",
            "       -7.31126890e-02, -1.42367622e-02, -4.84350249e-02, -5.12189902e-02,\n",
            "        6.77461997e-02, -3.18637304e-02,  7.80648962e-02, -2.71874527e-03,\n",
            "        7.44473329e-03,  1.33485291e-02, -3.45715657e-02,  1.77370086e-02,\n",
            "       -2.13490818e-02,  1.34399846e-01, -1.12467967e-02, -9.56644937e-02,\n",
            "        1.72187686e-02,  8.39420334e-02,  5.80599606e-02, -1.90077571e-08,\n",
            "        5.56968600e-02, -1.39543619e-02, -2.69176927e-03, -1.89401628e-03,\n",
            "       -7.60141388e-03, -1.48373712e-02, -6.11236058e-02,  6.11221641e-02,\n",
            "       -3.48792821e-02,  3.31252515e-02,  2.62938607e-02,  1.23119280e-01,\n",
            "       -2.00961437e-02,  1.49696857e-01, -2.61689536e-02, -1.86074320e-02,\n",
            "       -3.55178304e-02, -9.30219889e-03,  4.80840215e-03,  7.35342829e-03,\n",
            "        1.27054518e-02,  1.30299050e-02, -8.62516388e-02, -3.56863551e-02,\n",
            "        4.32925224e-02, -1.28294500e-02,  7.97777027e-02, -4.19520959e-02,\n",
            "       -6.27557281e-03,  3.00091449e-02, -4.27080616e-02, -1.80754270e-02,\n",
            "       -6.56463057e-02,  5.58347292e-02, -1.31621240e-02, -3.42061967e-02,\n",
            "       -2.40406971e-02,  5.64767160e-02, -1.78408511e-02,  6.55512419e-03,\n",
            "       -2.50057746e-02,  1.13737531e-01, -2.14570947e-02,  5.16956672e-02,\n",
            "       -6.99923411e-02, -7.13278651e-02, -2.71476470e-02,  6.81074932e-02,\n",
            "       -1.08121224e-02,  6.83158189e-02,  5.92454076e-02, -6.86311275e-02,\n",
            "        2.11050902e-02, -5.91949336e-02,  1.15705794e-02, -8.41439962e-02,\n",
            "        5.92256198e-03, -1.59337893e-02, -9.77719873e-02, -4.50903811e-02,\n",
            "        4.24975194e-02, -1.27333879e-01,  4.67155203e-02,  4.54838062e-03])}]\n",
            "\n",
            "=== Embedding shape & dtype checks ===\n",
            "embedding vector lengths (unique counts):\n",
            "{384: 49647}\n",
            "sample vector dtype: float64 shape: (384,)\n",
            "sample vector stats: mean -0.000021, std 0.051031, min -0.143779, max 0.147238\n",
            "\n",
            "=== Null / NaN checks ===\n",
            "text_emb null count: 0\n",
            "non-list text_emb entries: 0\n",
            "\n",
            "=== Date coverage alignment with aapl_features ===\n",
            "unique stock dates: 2202\n",
            "unique embedding dates: 1986\n",
            "overlap dates: 1370 (62.2%)\n",
            "first 5 matched dates: [Timestamp('2011-01-24 00:00:00'), Timestamp('2011-01-25 00:00:00'), Timestamp('2011-01-26 00:00:00'), Timestamp('2011-01-27 00:00:00'), Timestamp('2011-01-28 00:00:00')]\n",
            "first 5 stock-only dates (example): [Timestamp('2008-02-01 00:00:00'), Timestamp('2008-02-04 00:00:00'), Timestamp('2008-02-05 00:00:00'), Timestamp('2008-02-06 00:00:00'), Timestamp('2008-02-07 00:00:00')]\n",
            "first 5 emb-only dates (example): [Timestamp('2011-01-23 00:00:00'), Timestamp('2011-01-29 00:00:00'), Timestamp('2011-01-30 00:00:00'), Timestamp('2011-02-05 00:00:00'), Timestamp('2011-02-06 00:00:00')]\n",
            "\n",
            "=== Sanity: variety across embeddings ===\n",
            "sample mean of vectors (first 10): [-2.093386154947141e-05, 0.0004988277470620908, -0.0006760395642218439, 0.00034575642394224504, 0.0002238198686198937, -0.0005642870531207314, 0.0016400004659128387, -0.00014466060847498352, -0.0010956593548199407, -1.959088009410396e-07]\n",
            "std of vector means across sample: 0.0006014995579984578\n",
            "\n",
            "=== Sample merged row (if overlap exists) ===\n",
            "date: 2011-01-24 00:00:00\n",
            "feature columns sample: {'ret': 0.03231401310161308}\n",
            "embedding sample (first 8 dims): [0.05445598438382149, 0.03521854802966118, -0.003913958091288805, 0.04223310574889183, -0.00795647781342268, 0.014359175227582455, 0.06635284423828125, 0.048984430730342865]\n",
            "\\nDone.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python src/build_text_embeddings.py \\\n",
        "  --news_csv data/raw/RedditNews.csv \\\n",
        "  --out data/processed/text_embeddings.parquet \\\n",
        "  --model_name sentence-transformers/all-MiniLM-L6-v2 \\\n",
        "  --batch_size 128 \\\n",
        "  --device cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdCJknAntgpo",
        "outputId": "f1fd8ded-9f47-45e0-8194-2b58ddad8234"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process is terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# updated\n",
        "\n",
        "%%writefile src/train_multimodal_lstm.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_multimodal_lstm.py\n",
        "\n",
        "Updated version with:\n",
        " - robust embedding alignment + aggregation\n",
        " - overlap statistics printing\n",
        " - emb_fill modes: zero, ffill, hybrid (ffill up to N then zero)\n",
        " - robust target detection / computation\n",
        " - fixed shape handling for sequences (no incorrect reshape)\n",
        " - debug prints\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "import json\n",
        "from typing import Tuple, Optional, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# PyTorch (if not needed, you can change to any other framework)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# -----------------------\n",
        "# Helpers\n",
        "# -----------------------\n",
        "COMMON_TARGET_NAMES = [\"target\", \"y\", \"ret\", \"returns\", \"target_return\", \"next_return\"]\n",
        "COMMON_DATE_COLS = [\"date\", \"created_utc\", \"timestamp\", \"created\", \"publish_date\", \"published_at\", \"time\"]\n",
        "\n",
        "\n",
        "def detect_date_col(df: pd.DataFrame, prefer: Optional[str] = None) -> Optional[str]:\n",
        "    if prefer and prefer in df.columns:\n",
        "        return prefer\n",
        "    for c in COMMON_DATE_COLS:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    for c in df.columns:\n",
        "        if pd.api.types.is_datetime64_any_dtype(df[c]):\n",
        "            return c\n",
        "    for c in df.columns:\n",
        "        if pd.api.types.is_integer_dtype(df[c]) or pd.api.types.is_float_dtype(df[c]):\n",
        "            series = df[c].dropna()\n",
        "            if len(series) == 0:\n",
        "                continue\n",
        "            v = series.iloc[0]\n",
        "            if isinstance(v, (int, float)) and (1e9 < abs(v) < 1e12):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "\n",
        "def parse_dates_column(series: pd.Series, unit: str = \"s\") -> pd.Series:\n",
        "    if pd.api.types.is_datetime64_any_dtype(series):\n",
        "        return pd.to_datetime(series).dt.normalize()\n",
        "    if pd.api.types.is_integer_dtype(series) or pd.api.types.is_float_dtype(series):\n",
        "        return pd.to_datetime(series, unit=unit, errors=\"coerce\").dt.normalize()\n",
        "    return pd.to_datetime(series, errors=\"coerce\").dt.normalize()\n",
        "\n",
        "\n",
        "def align_embeddings_to_trade_dates(\n",
        "    df_num: pd.DataFrame,\n",
        "    df_text: pd.DataFrame,\n",
        "    emb_agg: str = \"mean\",\n",
        "    emb_fill: str = \"zero\",\n",
        "    emb_ffill_limit: int = 0,\n",
        "    debug: bool = False,\n",
        ") -> Tuple[pd.DataFrame, int]:\n",
        "    \"\"\"\n",
        "    Align text embeddings to the dates present in df_num (trading dates).\n",
        "    Returns (df_out, emb_dim) where df_out has columns ['date','text_emb'] (text_emb is list of floats).\n",
        "    emb_fill: 'zero' | 'ffill' | 'hybrid' (hybrid == ffill up to emb_ffill_limit then zeros)\n",
        "    emb_agg: aggregation over multiple text items per date: mean|sum|max\n",
        "    \"\"\"\n",
        "    df_num = df_num.copy()\n",
        "    df_num[\"date\"] = pd.to_datetime(df_num[\"date\"]).dt.normalize()\n",
        "    df_text = df_text.copy()\n",
        "    if \"date\" in df_text.columns:\n",
        "        df_text[\"date\"] = pd.to_datetime(df_text[\"date\"]).dt.normalize()\n",
        "    else:\n",
        "        # ensure there's at least a date column (maybe named differently)\n",
        "        date_col = detect_date_col(df_text)\n",
        "        if date_col:\n",
        "            df_text[\"date\"] = pd.to_datetime(df_text[date_col]).dt.normalize()\n",
        "        else:\n",
        "            df_text[\"date\"] = pd.NaT\n",
        "\n",
        "    if \"text_emb\" not in df_text.columns:\n",
        "        raise ValueError(\"text embeddings dataframe must contain 'text_emb' column\")\n",
        "\n",
        "    # Convert list/array to numpy arrays and drop invalids\n",
        "    def to_arr(x):\n",
        "        try:\n",
        "            if isinstance(x, (list, tuple, np.ndarray)):\n",
        "                arr = np.asarray(x, dtype=float)\n",
        "                return arr\n",
        "            # sometimes stored as string representation\n",
        "            if isinstance(x, str):\n",
        "                # try to eval safely: fallback - not ideal but often necessary\n",
        "                import ast\n",
        "\n",
        "                v = ast.literal_eval(x)\n",
        "                arr = np.asarray(v, dtype=float)\n",
        "                return arr\n",
        "        except Exception:\n",
        "            return None\n",
        "        return None\n",
        "\n",
        "    df_text[\"_emb_arr\"] = df_text[\"text_emb\"].apply(to_arr)\n",
        "    df_text = df_text[~df_text[\"_emb_arr\"].isna()].copy()\n",
        "    if df_text.shape[0] == 0:\n",
        "        raise ValueError(\"No valid embeddings found in text embeddings dataframe (column 'text_emb')\")\n",
        "\n",
        "    emb_dim = df_text[\"_emb_arr\"].iloc[0].shape[0]\n",
        "    # keep only rows with matching embedding dim\n",
        "    df_text = df_text[df_text[\"_emb_arr\"].apply(lambda x: x.shape[0] == emb_dim)].copy()\n",
        "\n",
        "    # Build DataFrame with one column per embedding dimension indexed by date\n",
        "    emb_cols = [f\"emb_{i}\" for i in range(emb_dim)]\n",
        "    emb_df = pd.DataFrame(df_text[\"_emb_arr\"].tolist(), columns=emb_cols, index=df_text[\"date\"])\n",
        "    emb_df.index.name = \"date\"\n",
        "\n",
        "    # aggregate multiple items per date\n",
        "    if emb_agg == \"mean\":\n",
        "        emb_daily = emb_df.groupby(emb_df.index).mean()\n",
        "    elif emb_agg == \"sum\":\n",
        "        emb_daily = emb_df.groupby(emb_df.index).sum()\n",
        "    elif emb_agg == \"max\":\n",
        "        emb_daily = emb_df.groupby(emb_df.index).max()\n",
        "    else:\n",
        "        emb_daily = emb_df.groupby(emb_df.index).mean()\n",
        "\n",
        "    # alignment index (we align to stock feature dates)\n",
        "    stock_dates = pd.to_datetime(df_num[\"date\"].unique()).normalize()\n",
        "    emb_index = pd.DatetimeIndex(sorted(stock_dates))\n",
        "    emb_daily = emb_daily.reindex(emb_index)  # missing dates -> NaNs\n",
        "\n",
        "    # Overlap stats\n",
        "    unique_stock_dates = len(stock_dates)\n",
        "    unique_emb_dates = emb_daily.dropna(how=\"all\").shape[0]\n",
        "    overlap = len(set(stock_dates).intersection(set(emb_daily.dropna(how=\"all\").index)))\n",
        "    if debug:\n",
        "        print(\"=== Embedding/Feature date overlap statistics ===\")\n",
        "        print(\"Unique stock feature dates:\", unique_stock_dates)\n",
        "        print(\"Unique embedding dates:\", unique_emb_dates)\n",
        "        print(f\"Overlap (stock  emb): {overlap} ({overlap/unique_stock_dates*100:.2f}% of stock dates)\")\n",
        "        stock_dt_sorted = sorted(stock_dates)\n",
        "        emb_dt_sorted = sorted(emb_daily.dropna(how=\"all\").index)\n",
        "        print(\"Example overlap dates (first 10):\", list(sorted(set(stock_dt_sorted).intersection(set(emb_dt_sorted))))[:10])\n",
        "        print(\"Example stock-only dates (first 10):\", stock_dt_sorted[:10])\n",
        "        print(\"Example emb-only dates (first 10):\", emb_dt_sorted[:10])\n",
        "        print(\"emb_fill:\", emb_fill, \"emb_ffill_limit:\", emb_ffill_limit)\n",
        "        print(\"===============================================\")\n",
        "\n",
        "    # Apply filling strategy\n",
        "    if emb_fill == \"zero\":\n",
        "        emb_filled = emb_daily.fillna(0.0)\n",
        "    elif emb_fill == \"ffill\":\n",
        "        emb_filled = emb_daily.ffill(limit=emb_ffill_limit)\n",
        "    elif emb_fill in (\"hybrid\", \"ffill_then_zero\"):\n",
        "        emb_filled = emb_daily.ffill(limit=emb_ffill_limit).fillna(0.0)\n",
        "    else:\n",
        "        emb_filled = emb_daily.fillna(0.0)\n",
        "\n",
        "    # Convert back to list column in the same order as df_num's dates\n",
        "    df_dates_order = pd.DataFrame({\"date\": pd.to_datetime(df_num[\"date\"]).dt.normalize()})\n",
        "    # Use emb_filled rows by df_dates_order order (for index alignment)\n",
        "    # If some dates are outside emb index, reindex will produce NaNs -> fill as per above logic\n",
        "    emb_for_dates = emb_filled.reindex(df_dates_order[\"date\"].values).fillna(0.0)\n",
        "    emb_list = emb_for_dates.values.tolist()\n",
        "    df_out = df_dates_order.copy()\n",
        "    df_out[\"text_emb\"] = emb_list\n",
        "    return df_out, emb_dim\n",
        "\n",
        "\n",
        "def detect_or_compute_target(df: pd.DataFrame, target_col_arg: Optional[str], debug: bool = False) -> Tuple[pd.DataFrame, str]:\n",
        "    \"\"\"\n",
        "    Ensure df has a 'target' column. If none present and price column exists, compute next-day pct change.\n",
        "    Returns (df, target_col_name)\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    if target_col_arg and target_col_arg in df.columns:\n",
        "        if debug:\n",
        "            print(f\"Using provided target column: {target_col_arg}\")\n",
        "        return df, target_col_arg\n",
        "\n",
        "    for c in COMMON_TARGET_NAMES:\n",
        "        if c in df.columns:\n",
        "            if debug:\n",
        "                print(f\"Found existing target column: {c}\")\n",
        "            return df, c\n",
        "\n",
        "    # try to compute from price columns\n",
        "    price_col = None\n",
        "    if \"adj_close\" in df.columns:\n",
        "        price_col = \"adj_close\"\n",
        "    elif \"close\" in df.columns:\n",
        "        price_col = \"close\"\n",
        "    elif \"Close\" in df.columns:\n",
        "        price_col = \"Close\"\n",
        "\n",
        "    if price_col:\n",
        "        if debug:\n",
        "            print(f\"Computing target as next-day pct change of price col '{price_col}'\")\n",
        "        df = df.sort_values(\"date\").copy()\n",
        "        df[\"target\"] = df[price_col].pct_change().shift(-1)  # next-day return\n",
        "        return df, \"target\"\n",
        "\n",
        "    raise ValueError(\"No 'target' column found in features parquet. Provide --target_col or include a price column like 'close' or 'adj_close' to compute one.\")\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Simple LSTM model\n",
        "# -----------------------\n",
        "class SimpleLSTM(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int = 64, num_layers: int = 1, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, input_size)\n",
        "        out, (hn, cn) = self.lstm(x)  # out: (batch, seq_len, hidden_size)\n",
        "        # use last timestep\n",
        "        last = out[:, -1, :]\n",
        "        y = self.head(last)\n",
        "        return y.squeeze(-1)\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Sequence creation\n",
        "# -----------------------\n",
        "def create_sequences(features_df: pd.DataFrame, seq_len: int, feature_cols: List[str], target_col: str) -> Tuple[np.ndarray, np.ndarray, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Build sequences from a dataframe sorted by date.\n",
        "    Returns (X, y, df_sorted) where:\n",
        "     - X shape = (n_samples, seq_len, n_features)\n",
        "     - y shape = (n_samples,)\n",
        "    \"\"\"\n",
        "    df = features_df.sort_values(\"date\").reset_index(drop=True).copy()\n",
        "    vals = df[feature_cols].values.astype(float)\n",
        "    targets = df[target_col].values.astype(float)\n",
        "\n",
        "    n = len(df)\n",
        "    seqs = []\n",
        "    ys = []\n",
        "    indices = []\n",
        "    for i in range(n - seq_len):\n",
        "        seq = vals[i : i + seq_len]\n",
        "        tgt = targets[i + seq_len]  # predict next day's target\n",
        "        if np.isnan(tgt):\n",
        "            continue\n",
        "        seqs.append(seq)\n",
        "        ys.append(tgt)\n",
        "        indices.append(i)\n",
        "    if len(seqs) == 0:\n",
        "        return np.zeros((0, seq_len, len(feature_cols))), np.zeros((0,)), df\n",
        "    X = np.stack(seqs, axis=0)\n",
        "    y = np.array(ys, dtype=float)\n",
        "    return X, y, df\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Main\n",
        "# -----------------------\n",
        "def main():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--features\", required=True, help=\"Features parquet with 'date' and feature columns\")\n",
        "    p.add_argument(\"--text_emb\", required=True, help=\"Text embeddings parquet with 'date' and 'text_emb' (list) columns\")\n",
        "    p.add_argument(\"--seq_len\", type=int, default=10)\n",
        "    p.add_argument(\"--emb_agg\", default=\"mean\", choices=[\"mean\", \"sum\", \"max\"])\n",
        "    p.add_argument(\"--emb_fill\", default=\"zero\", choices=[\"zero\", \"ffill\", \"hybrid\"])\n",
        "    p.add_argument(\"--emb_ffill_limit\", type=int, default=0, help=\"Days to forward-fill embeddings for ffill/hybrid\")\n",
        "    p.add_argument(\"--target_col\", default=None, help=\"Name of target column if present\")\n",
        "    p.add_argument(\"--epochs\", type=int, default=10)\n",
        "    p.add_argument(\"--batch_size\", type=int, default=64)\n",
        "    p.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    p.add_argument(\"--device\", default=\"cpu\", help=\"cpu or cuda\")\n",
        "    p.add_argument(\"--out\", default=\"outputs/multimodal_lstm_preds.json\")\n",
        "    p.add_argument(\"--debug\", action=\"store_true\")\n",
        "    p.add_argument(\"--flatten_for_classical\", action=\"store_true\", help=\"If set, flatten sequences into single vectors (seq_len * features) for classical ML models. If not set, keep (batch,seq_len,features) for LSTM.\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    device = torch.device(args.device if torch.cuda.is_available() and args.device != \"cpu\" else \"cpu\")\n",
        "    if args.debug:\n",
        "        print(\"Using device:\", device)\n",
        "\n",
        "    # Load features & embeddings\n",
        "    features_path = Path(args.features)\n",
        "    text_emb_path = Path(args.text_emb)\n",
        "    if not features_path.exists():\n",
        "        raise FileNotFoundError(args.features)\n",
        "    if not text_emb_path.exists():\n",
        "        raise FileNotFoundError(args.text_emb)\n",
        "\n",
        "    if args.debug:\n",
        "        print(\"Loading features from:\", features_path)\n",
        "    df_features = pd.read_parquet(features_path)\n",
        "    if args.debug:\n",
        "        print(\"Loading embeddings from:\", text_emb_path)\n",
        "    df_text = pd.read_parquet(text_emb_path)\n",
        "\n",
        "    # Ensure date exists in features\n",
        "    if \"date\" not in df_features.columns:\n",
        "        date_col = detect_date_col(df_features)\n",
        "        if date_col:\n",
        "            df_features[\"date\"] = pd.to_datetime(df_features[date_col]).dt.normalize()\n",
        "        else:\n",
        "            raise ValueError(\"Features parquet must contain a 'date' column or a detectable datetime column.\")\n",
        "\n",
        "    # Ensure text date exists\n",
        "    if \"date\" not in df_text.columns:\n",
        "        date_col = detect_date_col(df_text)\n",
        "        if date_col:\n",
        "            df_text[\"date\"] = pd.to_datetime(df_text[date_col]).dt.normalize()\n",
        "        else:\n",
        "            df_text[\"date\"] = pd.NaT\n",
        "\n",
        "    # Align embeddings -> df_aligned has text_emb for every stock date in df_features order\n",
        "    if args.debug:\n",
        "        print(\"Aligning/aggregating embeddings to trading dates...\")\n",
        "    df_text_aligned, emb_dim = align_embeddings_to_trade_dates(\n",
        "        df_num=df_features,\n",
        "        df_text=df_text,\n",
        "        emb_agg=args.emb_agg,\n",
        "        emb_fill=args.emb_fill,\n",
        "        emb_ffill_limit=args.emb_ffill_limit,\n",
        "        debug=args.debug,\n",
        "    )\n",
        "    if args.debug:\n",
        "        print(f\"Detected embedding dimension: {emb_dim}\")\n",
        "\n",
        "    # Merge aligned embeddings into df_features (keep df_features order)\n",
        "    df_features = df_features.sort_values(\"date\").reset_index(drop=True)\n",
        "    df_text_aligned = df_text_aligned.reset_index(drop=True)\n",
        "    if len(df_text_aligned) != len(df_features):\n",
        "        # safer align by date merge on 'date' preserving df_features order\n",
        "        df_merged = pd.merge(df_features, df_text_aligned, on=\"date\", how=\"left\", sort=False)\n",
        "    else:\n",
        "        df_merged = pd.concat([df_features.reset_index(drop=True), df_text_aligned[\"text_emb\"].reset_index(drop=True)], axis=1)\n",
        "\n",
        "    # If some text_emb are still NaN (shouldn't after filling), convert to zeros\n",
        "    if df_merged[\"text_emb\"].isna().any():\n",
        "        if args.debug:\n",
        "            print(\"Warning: some text_emb are NaN after alignment; filling with zeros.\")\n",
        "        df_merged[\"text_emb\"] = df_merged[\"text_emb\"].apply(lambda x: [0.0] * emb_dim if (pd.isna(x) or x is None) else x)\n",
        "\n",
        "    # Detect or compute target\n",
        "    df_merged, target_col = detect_or_compute_target(df_merged, args.target_col, debug=args.debug)\n",
        "\n",
        "    # Build feature columns: numeric columns in features + embedding dimensions expanded as separate columns\n",
        "    non_feature_cols = set([\"date\", target_col, \"text_emb\"])\n",
        "    candidate_feature_cols = [c for c in df_merged.columns if c not in non_feature_cols and np.issubdtype(df_merged[c].dtype, np.number)]\n",
        "    # Expand embedding dims into separate columns emb_0, emb_1, ...\n",
        "    emb_cols = [f\"emb_{i}\" for i in range(emb_dim)]\n",
        "    emb_vals = np.vstack(df_merged[\"text_emb\"].apply(lambda x: np.asarray(x, dtype=float)).values)\n",
        "    emb_df = pd.DataFrame(emb_vals, columns=emb_cols, index=df_merged.index)\n",
        "    df_features_expanded = pd.concat([df_merged.reset_index(drop=True), emb_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    feature_cols = candidate_feature_cols + emb_cols\n",
        "    if args.debug:\n",
        "        print(\"Feature columns used (sample):\", feature_cols[:10], \"... total:\", len(feature_cols))\n",
        "\n",
        "    # Create sequences\n",
        "    X, y, df_sorted = create_sequences(df_features_expanded[[\"date\"] + feature_cols + [target_col]], seq_len=args.seq_len, feature_cols=feature_cols, target_col=target_col)\n",
        "    if args.debug:\n",
        "        print(\"Created sequences:\")\n",
        "        print(\"X shape:\", X.shape)\n",
        "        print(\"y shape:\", y.shape)\n",
        "\n",
        "    if X.shape[0] == 0:\n",
        "        raise ValueError(\"No sequences created (maybe too short dataset relative to seq_len or all NaN targets).\")\n",
        "\n",
        "    # Split train/test simple chronological split (80/20)\n",
        "    n_samples = X.shape[0]\n",
        "    train_n = int(n_samples * 0.8)\n",
        "    X_train = X[:train_n]\n",
        "    y_train = y[:train_n]\n",
        "    X_test = X[train_n:]\n",
        "    y_test = y[train_n:]\n",
        "\n",
        "    # Optionally flatten for classical models (user requested earlier)\n",
        "    if args.flatten_for_classical:\n",
        "        # each sample -> vector of length seq_len * n_features\n",
        "        n_samples_train, seq_len_local, n_feats_ts = X_train.shape\n",
        "        X_train_flat = X_train.reshape(n_samples_train, seq_len_local * n_feats_ts)\n",
        "        X_test_flat = X_test.reshape(X_test.shape[0], seq_len_local * n_feats_ts)\n",
        "        # create data loaders from flat arrays\n",
        "        train_dataset = TensorDataset(torch.from_numpy(X_train_flat).float(), torch.from_numpy(y_train).float())\n",
        "        test_dataset = TensorDataset(torch.from_numpy(X_test_flat).float(), torch.from_numpy(y_test).float())\n",
        "        # For simplicity we won't train an LSTM if flatten_for_classical is True;\n",
        "        # user should plug in classical model. We'll run a trivial baseline (ridge-like) here if desired.\n",
        "        if args.debug:\n",
        "            print(\"flatten_for_classical requested: returning flattened arrays for classical model training.\")\n",
        "        # Save predictions placeholder: just predict mean of train targets on test\n",
        "        preds = [float(np.mean(y_train))] * len(y_test)\n",
        "        out_dict = {\"preds\": preds, \"y_test\": y_test.tolist(), \"method\": \"baseline_mean_due_to_flatten_flag\"}\n",
        "        Path(args.out).parent.mkdir(parents=True, exist_ok=True)\n",
        "        with open(args.out, \"w\") as f:\n",
        "            json.dump(out_dict, f)\n",
        "        print(\"Wrote fallback predictions to\", args.out)\n",
        "        return\n",
        "\n",
        "    # Training with PyTorch LSTM\n",
        "    input_size = X_train.shape[2]\n",
        "    model = SimpleLSTM(input_size=input_size, hidden_size=64, num_layers=1, dropout=0.0).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    train_ds = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())\n",
        "    test_ds = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float())\n",
        "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "    if args.debug:\n",
        "        print(\"Starting training: epochs:\", args.epochs, \"train_samples:\", len(train_ds), \"test_samples:\", len(test_ds))\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(args.epochs):\n",
        "        total_loss = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(xb)\n",
        "            loss = criterion(out, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * xb.size(0)\n",
        "        avg_loss = total_loss / len(train_ds)\n",
        "        if args.debug:\n",
        "            print(f\"[Epoch {epoch+1}/{args.epochs}] train loss: {avg_loss:.6f}\")\n",
        "\n",
        "    # Evaluation: predict on test set\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    trues = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            xb = xb.to(device)\n",
        "            out = model(xb)\n",
        "            preds.extend(out.cpu().numpy().tolist())\n",
        "            trues.extend(yb.numpy().tolist())\n",
        "\n",
        "    preds = np.array(preds)\n",
        "    trues = np.array(trues)\n",
        "\n",
        "    # helper to compute metrics for a pair of arrays\n",
        "    def compute_metrics(a_true: np.ndarray, a_pred: np.ndarray):\n",
        "        mae_v = np.mean(np.abs(a_pred - a_true))\n",
        "        rmse_v = np.sqrt(np.mean((a_pred - a_true) ** 2))\n",
        "        if len(a_true) >= 2:\n",
        "            da_v = 100.0 * np.mean(np.sign(a_pred) == np.sign(a_true))\n",
        "        else:\n",
        "            da_v = 0.0\n",
        "        return mae_v, rmse_v, da_v\n",
        "\n",
        "    # point estimates\n",
        "    mae, rmse, da = compute_metrics(trues, preds)\n",
        "\n",
        "    # bootstrap confidence intervals (95%) for MAE, RMSE, DA\n",
        "    n_test_pts = len(trues)\n",
        "    n_boot = 1000\n",
        "    seed = 42\n",
        "    if n_test_pts >= 2:\n",
        "        rng = np.random.default_rng(seed)\n",
        "        mae_bs = np.empty(n_boot, dtype=float)\n",
        "        rmse_bs = np.empty(n_boot, dtype=float)\n",
        "        da_bs = np.empty(n_boot, dtype=float)\n",
        "        for i in range(n_boot):\n",
        "            idx = rng.integers(0, n_test_pts, size=n_test_pts)  # with-replacement indices\n",
        "            t_sample = trues[idx]\n",
        "            p_sample = preds[idx]\n",
        "            m, r, d = compute_metrics(t_sample, p_sample)\n",
        "            mae_bs[i] = m\n",
        "            rmse_bs[i] = r\n",
        "            da_bs[i] = d\n",
        "        # 2.5th and 97.5th percentiles\n",
        "        mae_lo, mae_hi = np.percentile(mae_bs, [2.5, 97.5]).tolist()\n",
        "        rmse_lo, rmse_hi = np.percentile(rmse_bs, [2.5, 97.5]).tolist()\n",
        "        da_lo, da_hi = np.percentile(da_bs, [2.5, 97.5]).tolist()\n",
        "    else:\n",
        "        # not enough points to bootstrap reliably\n",
        "        mae_lo = mae_hi = float(mae)\n",
        "        rmse_lo = rmse_hi = float(rmse)\n",
        "        da_lo = da_hi = float(da)\n",
        "\n",
        "    out_metrics = {\n",
        "        \"MAE_mean\": float(mae),\n",
        "        \"MAE_lo\": float(mae_lo),\n",
        "        \"MAE_hi\": float(mae_hi),\n",
        "        \"RMSE_mean\": float(rmse),\n",
        "        \"RMSE_lo\": float(rmse_lo),\n",
        "        \"RMSE_hi\": float(rmse_hi),\n",
        "        \"DA_mean\": float(da),\n",
        "        \"DA_lo\": float(da_lo),\n",
        "        \"DA_hi\": float(da_hi),\n",
        "        \"n_test\": int(n_test_pts),\n",
        "        \"bootstrap_samples\": int(n_boot) if n_test_pts >= 2 else 0,\n",
        "        \"bootstrap_seed\": int(seed) if n_test_pts >= 2 else None,\n",
        "    }\n",
        "\n",
        "    Path(args.out).parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(args.out, \"w\") as f:\n",
        "        json.dump({\"preds\": preds.tolist(), \"y_test\": trues.tolist(), \"metrics\": out_metrics}, f)\n",
        "\n",
        "    print(\"Saved predictions+metrics to\", args.out)\n",
        "    if args.debug:\n",
        "        print(\"Metrics:\", json.dumps(out_metrics, indent=2))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaVy_li_QEK7",
        "outputId": "b2043b34-971f-4ac1-c687-da3cd8771664"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/train_multimodal_lstm.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#updated\n",
        "\n",
        "%%bash\n",
        "python src/train_multimodal_lstm.py \\\n",
        "  --features data/processed/aapl_features.parquet \\\n",
        "  --text_emb data/processed/text_embeddings.parquet \\\n",
        "  --seq_len 10 --emb_agg mean --emb_fill zero \\\n",
        "  --epochs 15 --batch_size 64 --device cpu --debug \\\n",
        "  --out outputs/multimodal_lstm_preds.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ly7CeegtxEhF",
        "outputId": "592a7984-2f56-4ff3-e945-24d93f237e9b"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Loading features from: data/processed/aapl_features.parquet\n",
            "Loading embeddings from: data/processed/text_embeddings.parquet\n",
            "Aligning/aggregating embeddings to trading dates...\n",
            "=== Embedding/Feature date overlap statistics ===\n",
            "Unique stock feature dates: 2202\n",
            "Unique embedding dates: 1370\n",
            "Overlap (stock  emb): 1370 (62.22% of stock dates)\n",
            "Example overlap dates (first 10): [Timestamp('2011-01-24 00:00:00'), Timestamp('2011-01-25 00:00:00'), Timestamp('2011-01-26 00:00:00'), Timestamp('2011-01-27 00:00:00'), Timestamp('2011-01-28 00:00:00'), Timestamp('2011-01-31 00:00:00'), Timestamp('2011-02-01 00:00:00'), Timestamp('2011-02-02 00:00:00'), Timestamp('2011-02-03 00:00:00'), Timestamp('2011-02-04 00:00:00')]\n",
            "Example stock-only dates (first 10): [Timestamp('2008-02-01 00:00:00'), Timestamp('2008-02-04 00:00:00'), Timestamp('2008-02-05 00:00:00'), Timestamp('2008-02-06 00:00:00'), Timestamp('2008-02-07 00:00:00'), Timestamp('2008-02-08 00:00:00'), Timestamp('2008-02-11 00:00:00'), Timestamp('2008-02-12 00:00:00'), Timestamp('2008-02-13 00:00:00'), Timestamp('2008-02-14 00:00:00')]\n",
            "Example emb-only dates (first 10): [Timestamp('2011-01-24 00:00:00'), Timestamp('2011-01-25 00:00:00'), Timestamp('2011-01-26 00:00:00'), Timestamp('2011-01-27 00:00:00'), Timestamp('2011-01-28 00:00:00'), Timestamp('2011-01-31 00:00:00'), Timestamp('2011-02-01 00:00:00'), Timestamp('2011-02-02 00:00:00'), Timestamp('2011-02-03 00:00:00'), Timestamp('2011-02-04 00:00:00')]\n",
            "emb_fill: zero emb_ffill_limit: 0\n",
            "===============================================\n",
            "Detected embedding dimension: 384\n",
            "Found existing target column: ret\n",
            "Feature columns used (sample): ['Close', 'High', 'Low', 'Open', 'Volume', 'close_next', 'ret_next', 'ret_lag_1', 'ret_lag_2', 'ret_lag_3'] ... total: 399\n",
            "Created sequences:\n",
            "X shape: (2192, 10, 399)\n",
            "y shape: (2192,)\n",
            "Starting training: epochs: 15 train_samples: 1753 test_samples: 439\n",
            "[Epoch 1/15] train loss: 0.004669\n",
            "[Epoch 2/15] train loss: 0.000653\n",
            "[Epoch 3/15] train loss: 0.000461\n",
            "[Epoch 4/15] train loss: 0.000452\n",
            "[Epoch 5/15] train loss: 0.000448\n",
            "[Epoch 6/15] train loss: 0.000450\n",
            "[Epoch 7/15] train loss: 0.000456\n",
            "[Epoch 8/15] train loss: 0.000452\n",
            "[Epoch 9/15] train loss: 0.000449\n",
            "[Epoch 10/15] train loss: 0.000449\n",
            "[Epoch 11/15] train loss: 0.000452\n",
            "[Epoch 12/15] train loss: 0.000449\n",
            "[Epoch 13/15] train loss: 0.000450\n",
            "[Epoch 14/15] train loss: 0.000452\n",
            "[Epoch 15/15] train loss: 0.000456\n",
            "Saved predictions+metrics to outputs/multimodal_lstm_preds.json\n",
            "Metrics: {\n",
            "  \"MAE_mean\": 0.011513171055826735,\n",
            "  \"MAE_lo\": 0.010507830485886377,\n",
            "  \"MAE_hi\": 0.012518637186095435,\n",
            "  \"RMSE_mean\": 0.015863879287977684,\n",
            "  \"RMSE_lo\": 0.014312979126536427,\n",
            "  \"RMSE_hi\": 0.01742773336132664,\n",
            "  \"DA_mean\": 50.34168564920274,\n",
            "  \"DA_lo\": 45.78587699316628,\n",
            "  \"DA_hi\": 55.130979498861045,\n",
            "  \"n_test\": 439,\n",
            "  \"bootstrap_samples\": 1000,\n",
            "  \"bootstrap_seed\": 42\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BWRrqpEp4m-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/eval2.py\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "eval.py\n",
        "\n",
        "Flexible evaluator for prediction JSON outputs. Supports multiple input formats:\n",
        "\n",
        "1) New training script format:\n",
        "   {\n",
        "     \"preds\": [..],\n",
        "     \"y_test\": [..],\n",
        "     \"metrics\": { optional precomputed metrics }\n",
        "   }\n",
        "\n",
        "2) Older \"per_fold\" format:\n",
        "   { \"per_fold\": [ { \"preds\": [...], \"y_test\": [...] }, ... ] }\n",
        "\n",
        "3) A plain list of fold records:\n",
        "   [ { \"preds\": [...], \"y_test\": [...] }, ... ]\n",
        "\n",
        "This script computes:\n",
        " - MAE, RMSE, Directional Accuracy (DA)\n",
        " - 95% CI via bootstrap (default 1000 samples)\n",
        " - For multiple folds, returns mean + 2.5/97.5 percentiles across folds.\n",
        "\n",
        "Writes JSON to --out path.\n",
        "\"\"\"\n",
        "import argparse\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------\n",
        "# Metrics / utils\n",
        "# ---------------------\n",
        "def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
        "    mae = float(np.mean(np.abs(y_pred - y_true)))\n",
        "    rmse = float(np.sqrt(np.mean((y_pred - y_true) ** 2)))\n",
        "    # directional accuracy: equality of sign (zero treated as sign 0)\n",
        "    sign_equal = (np.sign(y_pred) == np.sign(y_true))\n",
        "    da = 100.0 * float(np.mean(sign_equal))\n",
        "    return {\"MAE\": mae, \"RMSE\": rmse, \"DA\": da}\n",
        "\n",
        "\n",
        "def bootstrap_sample_metrics(y_true: np.ndarray, y_pred: np.ndarray, n_boot: int = 1000, seed: int = 42) -> Dict[str, Tuple[float, float, float]]:\n",
        "    \"\"\"\n",
        "    For a single fold: sample indices with replacement and compute bootstrap distribution\n",
        "    Returns dict: key -> (mean, lo, hi)\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(seed)\n",
        "    n = len(y_true)\n",
        "    if n == 0:\n",
        "        raise ValueError(\"Empty test set for bootstrap.\")\n",
        "    stats = {\"MAE\": [], \"RMSE\": [], \"DA\": []}\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.randint(0, n, size=n)\n",
        "        yt = y_true[idx]\n",
        "        yp = y_pred[idx]\n",
        "        m = compute_metrics(yt, yp)\n",
        "        stats[\"MAE\"].append(m[\"MAE\"])\n",
        "        stats[\"RMSE\"].append(m[\"RMSE\"])\n",
        "        stats[\"DA\"].append(m[\"DA\"])\n",
        "    out = {}\n",
        "    for k, arr in stats.items():\n",
        "        arr = np.array(arr)\n",
        "        out[k] = (float(arr.mean()), float(np.percentile(arr, 2.5)), float(np.percentile(arr, 97.5)))\n",
        "    return out\n",
        "\n",
        "\n",
        "def aggregate_fold_metrics(fold_metrics: List[Dict[str, float]]) -> Dict[str, Tuple[float, float, float]]:\n",
        "    \"\"\"\n",
        "    Given a list of per-fold metrics (dicts with keys MAE, RMSE, DA),\n",
        "    return (mean, lo, hi) where lo/hi are 2.5/97.5 percentiles across folds.\n",
        "    \"\"\"\n",
        "    arrs = {\"MAE\": [], \"RMSE\": [], \"DA\": []}\n",
        "    for fm in fold_metrics:\n",
        "        arrs[\"MAE\"].append(fm[\"MAE\"])\n",
        "        arrs[\"RMSE\"].append(fm[\"RMSE\"])\n",
        "        arrs[\"DA\"].append(fm[\"DA\"])\n",
        "    out = {}\n",
        "    for k, vals in arrs.items():\n",
        "        vals = np.array(vals)\n",
        "        out[k] = (float(vals.mean()), float(np.percentile(vals, 2.5)), float(np.percentile(vals, 97.5)))\n",
        "    return out\n",
        "\n",
        "\n",
        "# ---------------------\n",
        "# Loader helpers\n",
        "# ---------------------\n",
        "def load_json(path: Path) -> Any:\n",
        "    with open(path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def ensure_array(x) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Convert list-like to numpy array. Handles nested lists etc.\n",
        "    \"\"\"\n",
        "    return np.asarray(x, dtype=float)\n",
        "\n",
        "\n",
        "def detect_preds_structure(data: Any) -> Tuple[str, Any]:\n",
        "    \"\"\"\n",
        "    Return a tuple (kind, payload)\n",
        "    kind: \"single\", \"folds\", \"legacy_per_fold\"\n",
        "    payload: for \"single\" -> dict with 'preds' and 'y_test'\n",
        "             for \"folds\" -> list of per-fold dicts (each with preds,y_test)\n",
        "    \"\"\"\n",
        "    if isinstance(data, dict):\n",
        "        if \"preds\" in data and \"y_test\" in data:\n",
        "            return \"single\", {\"preds\": data[\"preds\"], \"y_test\": data[\"y_test\"], \"metrics\": data.get(\"metrics\", None)}\n",
        "        if \"per_fold\" in data and isinstance(data[\"per_fold\"], list):\n",
        "            return \"folds\", data[\"per_fold\"]\n",
        "        # sometimes authors put folds under 'folds' or 'results'\n",
        "        for k in (\"folds\", \"results\", \"perFold\"):\n",
        "            if k in data and isinstance(data[k], list):\n",
        "                return \"folds\", data[k]\n",
        "        # If dict of many entries that look like fold keys, attempt to coerce\n",
        "        # Otherwise fail and try list detection below\n",
        "    if isinstance(data, list):\n",
        "        # list of fold-like objects? ensure each has preds & y_test\n",
        "        if len(data) > 0 and all(isinstance(x, dict) and \"preds\" in x and \"y_test\" in x for x in data):\n",
        "            return \"folds\", data\n",
        "    raise ValueError(\"Unrecognized preds_json structure. Expect single dict with 'preds' and 'y_test', or a list/dict containing per-fold entries.\")\n",
        "\n",
        "\n",
        "# ---------------------\n",
        "# Main\n",
        "# ---------------------\n",
        "def main():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--preds_json\", required=True, help=\"Predictions JSON created by training script\")\n",
        "    p.add_argument(\"--out\", required=True, help=\"Output metrics JSON path\")\n",
        "    p.add_argument(\"--n_boot\", type=int, default=1000, help=\"Bootstrap samples when computing CIs for a single-fold preds file\")\n",
        "    p.add_argument(\"--seed\", type=int, default=42, help=\"Bootstrap RNG seed\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    preds_path = Path(args.preds_json)\n",
        "    if not preds_path.exists():\n",
        "        raise FileNotFoundError(args.preds_json)\n",
        "\n",
        "    data = load_json(preds_path)\n",
        "\n",
        "    kind, payload = detect_preds_structure(data)\n",
        "\n",
        "    out_metrics: Dict[str, Any] = {}\n",
        "    out_payload = {\"source\": str(preds_path), \"input_kind\": kind}\n",
        "\n",
        "    if kind == \"single\":\n",
        "        preds = ensure_array(payload[\"preds\"])\n",
        "        y_test = ensure_array(payload[\"y_test\"])\n",
        "        if preds.shape != y_test.shape:\n",
        "            # try to flatten if preds nested\n",
        "            preds = preds.flatten()\n",
        "            y_test = y_test.flatten()\n",
        "            if preds.shape != y_test.shape:\n",
        "                raise ValueError(f\"preds and y_test shapes mismatch after flatten: {preds.shape} vs {y_test.shape}\")\n",
        "        # compute point metrics\n",
        "        m = compute_metrics(y_test, preds)\n",
        "        # bootstrap per-sample for CI\n",
        "        boot = bootstrap_sample_metrics(y_test, preds, n_boot=args.n_boot, seed=args.seed)\n",
        "        out_metrics[\"MAE_mean\"], out_metrics[\"MAE_lo\"], out_metrics[\"MAE_hi\"] = boot[\"MAE\"]\n",
        "        out_metrics[\"RMSE_mean\"], out_metrics[\"RMSE_lo\"], out_metrics[\"RMSE_hi\"] = boot[\"RMSE\"]\n",
        "        out_metrics[\"DA_mean\"], out_metrics[\"DA_lo\"], out_metrics[\"DA_hi\"] = boot[\"DA\"]\n",
        "        out_metrics[\"n_test\"] = int(len(y_test))\n",
        "        out_metrics[\"bootstrap_samples\"] = int(args.n_boot)\n",
        "        out_metrics[\"bootstrap_seed\"] = int(args.seed)\n",
        "        # include training-provided metrics if present\n",
        "        if payload.get(\"metrics\") is not None:\n",
        "            out_payload[\"provided_metrics\"] = payload[\"metrics\"]\n",
        "    elif kind == \"folds\":\n",
        "        folds = payload  # list of dicts\n",
        "        fold_metrics = []\n",
        "        n_total = 0\n",
        "        for f in folds:\n",
        "            if not (\"preds\" in f and \"y_test\" in f):\n",
        "                raise ValueError(\"Each fold entry must contain 'preds' and 'y_test'.\")\n",
        "            preds = ensure_array(f[\"preds\"]).flatten()\n",
        "            y_test = ensure_array(f[\"y_test\"]).flatten()\n",
        "            if preds.shape != y_test.shape:\n",
        "                raise ValueError(\"preds and y_test must have same shape within each fold.\")\n",
        "            n_total += len(y_test)\n",
        "            m = compute_metrics(y_test, preds)\n",
        "            fold_metrics.append(m)\n",
        "        # aggregate metrics across folds (mean and percentile)\n",
        "        agg = aggregate_fold_metrics(fold_metrics)\n",
        "        out_metrics[\"MAE_mean\"], out_metrics[\"MAE_lo\"], out_metrics[\"MAE_hi\"] = agg[\"MAE\"]\n",
        "        out_metrics[\"RMSE_mean\"], out_metrics[\"RMSE_lo\"], out_metrics[\"RMSE_hi\"] = agg[\"RMSE\"]\n",
        "        out_metrics[\"DA_mean\"], out_metrics[\"DA_lo\"], out_metrics[\"DA_hi\"] = agg[\"DA\"]\n",
        "        out_metrics[\"n_folds\"] = len(fold_metrics)\n",
        "        out_metrics[\"n_test_total\"] = int(n_total)\n",
        "    else:\n",
        "        raise RuntimeError(\"unexpected structure type\")\n",
        "\n",
        "    # Write out combined JSON\n",
        "    out_path = Path(args.out)\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(out_path, \"w\") as f:\n",
        "        json.dump({\"metrics\": out_metrics, \"meta\": out_payload}, f, indent=2)\n",
        "\n",
        "    print(f\"Wrote metrics to {out_path}\")\n",
        "    print(json.dumps(out_metrics, indent=2))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "oKMpOKicjdsz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ec3b6b1-d809-4f25-fff6-90955895960c"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/eval2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "mGrFRSZ59Tjb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "553349f6-eb58-46dd-c192-d6f85020913c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote metrics to outputs/multimodal_lstm_metrics.json\n",
            "{\n",
            "  \"MAE_mean\": 0.01143525487624622,\n",
            "  \"MAE_lo\": 0.010468048432253423,\n",
            "  \"MAE_hi\": 0.012509532109296844,\n",
            "  \"RMSE_mean\": 0.015763131941407098,\n",
            "  \"RMSE_lo\": 0.014194744226426356,\n",
            "  \"RMSE_hi\": 0.017445346229769915,\n",
            "  \"DA_mean\": 50.341230068337126,\n",
            "  \"DA_lo\": 45.558086560364465,\n",
            "  \"DA_hi\": 54.6753986332574,\n",
            "  \"n_test\": 439,\n",
            "  \"bootstrap_samples\": 1000,\n",
            "  \"bootstrap_seed\": 42\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "python src/eval2.py \\\n",
        "  --preds_json outputs/multimodal_lstm_preds.json \\\n",
        "  --out outputs/multimodal_lstm_metrics.json \\\n",
        "  --n_boot 1000 \\\n",
        "  --seed 42\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j2rFR71ZZHS5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}